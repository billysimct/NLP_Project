{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/FOMC Labelled Sentences.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# read file from data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m labelled_sentences \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39m'\u001b[39;49m\u001b[39mdata/FOMC Labelled Sentences.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m statements \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_excel(\u001b[39m'\u001b[39m\u001b[39mdata/FOMC Statements 1997-2023.xlsx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Import Packages\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    479\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1496\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1497\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m   1498\u001b[0m     )\n\u001b[0;32m   1499\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1501\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1502\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1503\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1369\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1371\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1372\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1373\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1374\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1375\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    869\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    871\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/FOMC Labelled Sentences.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# read file from data\n",
    "labelled_sentences = pd.read_excel('data/FOMC Labelled Sentences.xlsx')\n",
    "statements = pd.read_excel('data/FOMC Statements 1997-2023.xlsx')\n",
    "\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    # Clean Data\n",
    "    clean = []\n",
    "    for i in range(len(data)):\n",
    "        x = data.iloc[i,1]\n",
    "        x = re.sub('_x000D','',x)\n",
    "        x = re.sub('\\n','',x)\n",
    "        x = re.sub('\\t','',x)\n",
    "        x = re.sub('_','',x)\n",
    "        x = word_tokenize(x)\n",
    "        x = [i.lower() for i in x if i not in string.punctuation]\n",
    "        x = [i for i in x if i not in set(stopwords.words('english'))]\n",
    "        x = ' '.join(x)\n",
    "        clean.append(x)\n",
    "\n",
    "    # Relabel\n",
    "    test = []\n",
    "    for i in range(len(data)):\n",
    "        if data.iloc[i,3] == 1:\n",
    "            test.append(\"1\")\n",
    "        elif data.iloc[i,3] == 0.5:\n",
    "            test.append(\"0.5\")\n",
    "        elif data.iloc[i,3] == 0:\n",
    "            test.append(\"0\")\n",
    "        elif data.iloc[i,3] == -0.5:\n",
    "            test.append(\"-0.5\")\n",
    "        elif data.iloc[i,3] == -1:\n",
    "            test.append(\"-1\")\n",
    "        else:\n",
    "            test.append(\"Remove\")\n",
    "\n",
    "    # New table\n",
    "    data['Sentences'] = clean\n",
    "    data['Category'] = test\n",
    "    df = data[['Sentences','Meeting Date','Category']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cleaning(data, colnum):\n",
    "    sentencelist = []\n",
    "    for i in range(len(data)):\n",
    "        x = re.sub('_x000D','',data.iloc[i,colnum])\n",
    "        x = re.sub('\\n','',x)\n",
    "        x = re.sub('\\t','',x)\n",
    "        x = re.sub('_','',x)\n",
    "        x = [i.lower() for i in word_tokenize(x) if i not in stopWords]\n",
    "        x = [i for i in x if i not in string.punctuation]\n",
    "        sentencelist.append(' '.join(x))\n",
    "\n",
    "    return sentencelist\n",
    "\n",
    "def predict(data):\n",
    "    X_new_counts = count_vector.transform(data)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "    # Execute prediction(classification).\n",
    "    predicted = clf.predict(X_new_tfidf)\n",
    "    return predicted\n",
    "\n",
    "\n",
    "# cleandata = clean_data(labelled_sentences)\n",
    "# train = cleaning(cleandata,0)\n",
    "\n",
    "cleandata = clean_data(labelled_sentences)\n",
    "train, test = train_test_split(cleandata, test_size=0.2, random_state=2020, stratify=cleandata['Category'])\n",
    "clean_train = cleaning(train,0)\n",
    "\n",
    "# Create dictionary and transform to feature vectors.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "X_train_counts = count_vector.fit_transform(clean_train)\n",
    "\n",
    "# TF-IDF vectorize.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Create model(naive bayes) and training. \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train['Category'])\n",
    "\n",
    "clean_test = cleaning(test,0)\n",
    "\n",
    "abc = predict(clean_test)\n",
    "abc\n",
    "\n",
    "result = []\n",
    "for i in range(len(test)):\n",
    "    result.append(test.iloc[i,2])\n",
    "\n",
    "print(f\"Confusion Matrix \\n{confusion_matrix(result, abc)}\")\n",
    "\n",
    "# Results / Accuracy\n",
    "\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(result, abc)*100))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(result, abc)*100))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(result, abc, average='micro')*100))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(result, abc, average='micro')*100))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(result, abc, average='micro')*100))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(result, abc, average='macro')*100))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(result, abc, average='macro')*100))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(result, abc, average='macro')*100))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(result, abc, average='weighted')*100))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(result, abc, average='weighted')*100))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(result, abc, average='weighted')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read file from data\n",
    "labelled_sentences = pd.read_excel('data/FOMC Labelled Sentences.xlsx')\n",
    "statements = pd.read_excel('data/FOMC Statements 1997-2023.xlsx')\n",
    "\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    # Clean Data\n",
    "    clean = []\n",
    "    for i in range(len(data)):\n",
    "        x = data.iloc[i,1]\n",
    "        x = re.sub('_x000D','',x)\n",
    "        x = re.sub('\\n','',x)\n",
    "        x = re.sub('\\t','',x)\n",
    "        x = re.sub('_','',x)\n",
    "        x = word_tokenize(x)\n",
    "        x = [i.lower() for i in x if i not in string.punctuation]\n",
    "        x = [i for i in x if i not in set(stopwords.words('english'))]\n",
    "        x = ' '.join(x)\n",
    "        clean.append(x)\n",
    "\n",
    "    # Relabel\n",
    "    test = []\n",
    "    for i in range(len(data)):\n",
    "        if data.iloc[i,3] == 1:\n",
    "            test.append(\"1\")\n",
    "        elif data.iloc[i,3] == 0.5:\n",
    "            test.append(\"0.5\")\n",
    "        elif data.iloc[i,3] == 0:\n",
    "            test.append(\"0\")\n",
    "        elif data.iloc[i,3] == -0.5:\n",
    "            test.append(\"-0.5\")\n",
    "        elif data.iloc[i,3] == -1:\n",
    "            test.append(\"-1\")\n",
    "        else:\n",
    "            test.append(\"Remove\")\n",
    "\n",
    "    # New table\n",
    "    data['Sentences'] = clean\n",
    "    data['Category'] = test\n",
    "    df = data[['Sentences','Meeting Date','Category']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cleaning(data, colnum):\n",
    "    sentencelist = []\n",
    "    for i in range(len(data)):\n",
    "        x = re.sub('_x000D','',data.iloc[i,colnum])\n",
    "        x = re.sub('\\n','',x)\n",
    "        x = re.sub('\\t','',x)\n",
    "        x = re.sub('_','',x)\n",
    "        x = [i.lower() for i in word_tokenize(x) if i not in stopWords]\n",
    "        x = [i for i in x if i not in string.punctuation]\n",
    "        sentencelist.append(' '.join(x))\n",
    "\n",
    "    return sentencelist\n",
    "\n",
    "def predict(data):\n",
    "    X_new_counts = count_vector.transform(data)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "    # Execute prediction(classification).\n",
    "    predicted = clf.predict(X_new_tfidf)\n",
    "    return predicted\n",
    "\n",
    "\n",
    "cleandata = clean_data(labelled_sentences)\n",
    "train = cleaning(cleandata,0)\n",
    "\n",
    "# Create dictionary and transform to feature vectors.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "X_train_counts = count_vector.fit_transform(train)\n",
    "\n",
    "# TF-IDF vectorize.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Create model(naive bayes) and training. \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, cleandata['Category'])\n",
    "\n",
    "statementlist = cleaning(statements,1)\n",
    "\n",
    "list1 = []\n",
    "for i in statementlist:\n",
    "    list1.append(i.split('.'))\n",
    "\n",
    "list1\n",
    "\n",
    "prediction = []\n",
    "for i in list1:\n",
    "    prediction.append(predict(i))\n",
    "\n",
    "prediction\n",
    "\n",
    "predict_statements = []\n",
    "for i in prediction:\n",
    "    value = 0\n",
    "    count = 0\n",
    "    list1 = []\n",
    "    for j in i:\n",
    "        if j == \"Remove\":\n",
    "            pass\n",
    "        else:\n",
    "            value += float(j)\n",
    "            count += 1\n",
    "            list1.append(float(j))\n",
    "    # print(value, value/count)\n",
    "\n",
    "    if value/count < -0.5:\n",
    "        predict_statements.append(\"Very Hawkish\")\n",
    "    elif value/count < 0:\n",
    "        predict_statements.append(\"Hawkish\")\n",
    "    elif value/count == 0:\n",
    "        predict_statements.append(\"Neutral\")\n",
    "    elif value/count > 0.5:\n",
    "        predict_statements.append(\"Very Dovish\")\n",
    "    elif value/count > 0:\n",
    "        predict_statements.append(\"Dovish\")\n",
    "    \n",
    "statements['predicted'] = predict_statements\n",
    "statements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
