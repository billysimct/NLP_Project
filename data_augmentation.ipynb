{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labelled_sentences = pd.read_excel('training_data.xlsx')\n",
    "labelled_sentences = labelled_sentences[['Sentence', 'Score']]\n",
    "\n",
    "# align data type for Score column to string\n",
    "score_map = {-1:'1', -0.5:'2', 0:'3', 0.5:'4', 1:'5', 'Remove':'Remove'}\n",
    "labelled_sentences['Score'] = labelled_sentences['Score'].apply(lambda x: score_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lohsi\\Desktop\\NLP\\project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# functions to perform data augmentation\n",
    "\n",
    "import copy\n",
    "import nlpaug.augmenter.word as naw\n",
    "aug = naw.SynonymAug()\n",
    "\n",
    "def word_substitution(sentence, augmenter):\n",
    "    augmented_text = augmenter.augment(sentence)\n",
    "    return augmented_text[0]\n",
    "\n",
    "def data_augmentation(train_data):\n",
    "    train_temp = copy.deepcopy(train_data)\n",
    "    train_temp['Sentence'] = train_temp['Sentence'].apply(lambda x: word_substitution(x, aug))\n",
    "    train_data = pd.concat([train_data, train_temp], ignore_index=True, axis=0)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add prompt to sentences\n",
    "\n",
    "def preprocess(df):\n",
    "    new_df = pd.DataFrame(columns=['prompt', 'completion'])\n",
    "    # new_df['prompt'] = df['Sentence'].apply(lambda x: 'Give a score to the following sentence; give a score of 9 when the sentence is not relevant for financial sentiment analysis; give a score of 1 to 5, in step of 1, where 1 represents the most dovish sentiment, and 5 representing the most hawkish sentiment. ' + x + ' \\n\\n###\\n\\n')\n",
    "    new_df['prompt'] = df['Sentence'].apply(lambda x: x + ' \\n\\n###\\n\\n')\n",
    "    new_df['completion'] = df['Score']\n",
    "    new_df['completion'] = new_df['completion'].apply(lambda x: ' ' + str(x) + '###')  \n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada - No data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train, test = train_test_split(labelled_sentences, test_size=0.2, random_state=23, stratify=labelled_sentences['Score'])\n",
    "train_new = preprocess(train)\n",
    "\n",
    "# split train data for train and validate\n",
    "train_gpt, test_gpt = train_test_split(train_new, test_size=0.2, random_state=23)\n",
    "\n",
    "# save training and validation data to jsonl files\n",
    "train_gpt.to_json('train_gpt_no_aug.jsonl', orient='records', lines=True)\n",
    "test_gpt.to_json('val_gpt_no_aug.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from train_gpt_no_aug.jsonl: file-mmwJWcOkSEQ8qMfUdjmdj7rE\n",
      "Uploaded file from val_gpt_no_aug.jsonl: file-X7G0h06fFMmA67dpoWLed6Zl\n",
      "Created fine-tune: ft-weBlhPNmrqDGI86s9muuJYiL\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-06-16 13:02:06] Created fine-tune: ft-weBlhPNmrqDGI86s9muuJYiL\n",
      "[2023-06-16 13:03:00] Fine-tune costs $0.01\n",
      "[2023-06-16 13:03:00] Fine-tune enqueued. Queue number: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload progress:   0%|          | 0.00/30.6k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.6k/30.6k [00:00<00:00, 30.7Mit/s]\n",
      "\n",
      "Upload progress:   0%|          | 0.00/8.31k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.31k/8.31k [00:00<00:00, 9.04Mit/s]\n"
     ]
    }
   ],
   "source": [
    "# create fine tunes model\n",
    "!openai api fine_tunes.create -t \"train_gpt_no_aug.jsonl\" -v \"val_gpt_no_aug.jsonl\" --compute_classification_metrics --classification_n_classes 6 -m ada --n_epochs 5 --suffix \"nlp_ada_no_aug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-16 13:02:06] Created fine-tune: ft-weBlhPNmrqDGI86s9muuJYiL\n",
      "[2023-06-16 13:03:00] Fine-tune costs $0.01\n",
      "[2023-06-16 13:03:00] Fine-tune enqueued. Queue number: 1\n",
      "[2023-06-16 13:03:17] Fine-tune is in the queue. Queue number: 0\n",
      "[2023-06-16 13:03:22] Fine-tune started\n",
      "[2023-06-16 13:03:57] Completed epoch 1/5\n",
      "[2023-06-16 13:04:19] Completed epoch 2/5\n",
      "[2023-06-16 13:04:39] Completed epoch 3/5\n",
      "[2023-06-16 13:04:58] Completed epoch 4/5\n",
      "[2023-06-16 13:05:18] Completed epoch 5/5\n",
      "[2023-06-16 13:05:36] Uploaded model: ada:ft-personal:nlp-ada-no-aug-2023-06-16-05-05-35\n",
      "[2023-06-16 13:05:37] Uploaded result file: file-YsjQiZsArgdY4g2iLrUSeVNu\n",
      "[2023-06-16 13:05:37] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded ðŸŽ‰\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-personal:nlp-ada-no-aug-2023-06-16-05-05-35 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i ft-weBlhPNmrqDGI86s9muuJYiL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.results -i ft-weBlhPNmrqDGI86s9muuJYiL > result_no_augment.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>elapsed_tokens</th>\n",
       "      <th>elapsed_examples</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_sequence_accuracy</th>\n",
       "      <th>training_token_accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_sequence_accuracy</th>\n",
       "      <th>validation_token_accuracy</th>\n",
       "      <th>classification/accuracy</th>\n",
       "      <th>classification/weighted_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>129</td>\n",
       "      <td>6217</td>\n",
       "      <td>129</td>\n",
       "      <td>0.033701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.089744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>259</td>\n",
       "      <td>12371</td>\n",
       "      <td>259</td>\n",
       "      <td>0.046682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.371429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>386</td>\n",
       "      <td>18570</td>\n",
       "      <td>386</td>\n",
       "      <td>0.022546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.404474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>517</td>\n",
       "      <td>24709</td>\n",
       "      <td>517</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.358681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>642</td>\n",
       "      <td>30706</td>\n",
       "      <td>642</td>\n",
       "      <td>0.009186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.447868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     step  elapsed_tokens  elapsed_examples  training_loss  \\\n",
       "128   129            6217               129       0.033701   \n",
       "258   259           12371               259       0.046682   \n",
       "385   386           18570               386       0.022546   \n",
       "516   517           24709               517       0.022717   \n",
       "641   642           30706               642       0.009186   \n",
       "\n",
       "     training_sequence_accuracy  training_token_accuracy  validation_loss  \\\n",
       "128                         1.0                      1.0         0.061983   \n",
       "258                         0.0                      0.5              NaN   \n",
       "385                         1.0                      1.0              NaN   \n",
       "516                         1.0                      1.0              NaN   \n",
       "641                         1.0                      1.0              NaN   \n",
       "\n",
       "     validation_sequence_accuracy  validation_token_accuracy  \\\n",
       "128                           0.0                        0.5   \n",
       "258                           NaN                        NaN   \n",
       "385                           NaN                        NaN   \n",
       "516                           NaN                        NaN   \n",
       "641                           NaN                        NaN   \n",
       "\n",
       "     classification/accuracy  classification/weighted_f1_score  \n",
       "128                  0.21875                          0.089744  \n",
       "258                  0.40625                          0.371429  \n",
       "385                  0.43750                          0.404474  \n",
       "516                  0.34375                          0.358681  \n",
       "641                  0.40625                          0.447868  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('result_no_augment.csv')\n",
    "results[results['classification/accuracy'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = 'ada:ft-personal:nlp-prod-ada-2023-06-14-03-38-06'\n",
    "test_sentences = test_new['Sentence'].tolist()\n",
    "y_pred = []\n",
    "\n",
    "for s in test_sentences:\n",
    "    res = openai.Completion.create(model=ft_model, prompt=s, max_tokens=1, temperature=0, logprobs=2)\n",
    "    classify = res['choices'][0]['text'].strip()\n",
    "    y_pred.append(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada + Data augmentation: Paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lohsi\\Desktop\\NLP\\project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import pandas as pd\n",
    "import openai\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_sentences = pd.read_excel('training_data.xlsx')\n",
    "labelled_sentences = labelled_sentences[['Sentence', 'Score']]\n",
    "\n",
    "# align data type for Score column to string\n",
    "score_map = {-1:'1', -0.5:'2', 0:'3', 0.5:'4', 1:'5', 'Remove':'Remove'}\n",
    "labelled_sentences['Score'] = labelled_sentences['Score'].apply(lambda x: score_map[x])\n",
    "\n",
    "# split data\n",
    "train, test = train_test_split(labelled_sentences, test_size=0.2, random_state=23, stratify=labelled_sentences['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Sentence'].str.split().str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "def paraphrase(\n",
    "    sentence,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=128\n",
    "):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {sentence}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp = copy.deepcopy(train)\n",
    "train_temp['Sentence'] = train_temp['Sentence'].apply(lambda x: paraphrase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.concat([train, train_temp], ignore_index=True, axis=0)\n",
    "train.to_csv('train_paraphrase.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = preprocess(train)\n",
    "\n",
    "# split train data for train and validate\n",
    "train_gpt, test_gpt = train_test_split(train_new, test_size=0.2, random_state=23)\n",
    "\n",
    "# save training and validation data to jsonl files\n",
    "train_gpt.to_json('train_gpt_paraphrase.jsonl', orient='records', lines=True)\n",
    "test_gpt.to_json('val_gpt_paraphrase.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from train_gpt_paraphrase.jsonl: file-fYMjZv8tiw9A5z8Vdg2X9yg7\n",
      "Uploaded file from val_gpt_paraphrase.jsonl: file-rrjOKshDbZXoAtoXfzfwPnI5\n",
      "Created fine-tune: ft-JLwO9bVsAC5GzeE8ZgOldJxt\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-06-16 14:31:49] Created fine-tune: ft-JLwO9bVsAC5GzeE8ZgOldJxt\n",
      "[2023-06-16 14:32:35] Fine-tune costs $0.02\n",
      "[2023-06-16 14:32:35] Fine-tune enqueued. Queue number: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload progress:   0%|          | 0.00/57.9k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57.9k/57.9k [00:00<?, ?it/s]\n",
      "\n",
      "Upload progress:   0%|          | 0.00/15.1k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.1k/15.1k [00:00<00:00, 15.3Mit/s]\n"
     ]
    }
   ],
   "source": [
    "# create fine tunes model\n",
    "!openai api fine_tunes.create -t \"train_gpt_paraphrase.jsonl\" -v \"val_gpt_paraphrase.jsonl\" --compute_classification_metrics --classification_n_classes 6 -m ada --n_epochs 5 --suffix \"nlp_ada_para\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-16 14:31:49] Created fine-tune: ft-JLwO9bVsAC5GzeE8ZgOldJxt\n",
      "[2023-06-16 14:32:35] Fine-tune costs $0.02\n",
      "[2023-06-16 14:32:35] Fine-tune enqueued. Queue number: 0\n",
      "[2023-06-16 14:42:37] Fine-tune started\n",
      "[2023-06-16 14:43:33] Completed epoch 1/5\n",
      "[2023-06-16 14:44:14] Completed epoch 2/5\n",
      "[2023-06-16 14:44:55] Completed epoch 3/5\n",
      "[2023-06-16 14:45:36] Completed epoch 4/5\n",
      "[2023-06-16 14:46:16] Completed epoch 5/5\n",
      "[2023-06-16 14:46:41] Uploaded model: ada:ft-personal:nlp-ada-para-2023-06-16-06-46-41\n",
      "[2023-06-16 14:46:42] Uploaded result file: file-g6RlFS3uJ4vjttH8nwz15XpV\n",
      "[2023-06-16 14:46:42] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded ðŸŽ‰\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-personal:nlp-ada-para-2023-06-16-06-46-41 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i ft-JLwO9bVsAC5GzeE8ZgOldJxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.results -i ft-JLwO9bVsAC5GzeE8ZgOldJxt > result_paraphrase.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>elapsed_tokens</th>\n",
       "      <th>elapsed_examples</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_sequence_accuracy</th>\n",
       "      <th>training_token_accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_sequence_accuracy</th>\n",
       "      <th>validation_token_accuracy</th>\n",
       "      <th>classification/accuracy</th>\n",
       "      <th>classification/weighted_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>258</td>\n",
       "      <td>11378</td>\n",
       "      <td>258</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.262419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>514</td>\n",
       "      <td>22770</td>\n",
       "      <td>514</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.435863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>772</td>\n",
       "      <td>34148</td>\n",
       "      <td>772</td>\n",
       "      <td>0.041208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.579313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>1030</td>\n",
       "      <td>45534</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.710382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>1281</td>\n",
       "      <td>56601</td>\n",
       "      <td>1281</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.083623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.663262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      step  elapsed_tokens  elapsed_examples  training_loss  \\\n",
       "257    258           11378               258       0.045614   \n",
       "513    514           22770               514       0.026385   \n",
       "771    772           34148               772       0.041208   \n",
       "1029  1030           45534              1030       0.009556   \n",
       "1280  1281           56601              1281       0.015705   \n",
       "\n",
       "      training_sequence_accuracy  training_token_accuracy  validation_loss  \\\n",
       "257                          1.0                      1.0              NaN   \n",
       "513                          1.0                      1.0              NaN   \n",
       "771                          0.0                      0.5              NaN   \n",
       "1029                         1.0                      1.0              NaN   \n",
       "1280                         1.0                      1.0         0.083623   \n",
       "\n",
       "      validation_sequence_accuracy  validation_token_accuracy  \\\n",
       "257                            NaN                        NaN   \n",
       "513                            NaN                        NaN   \n",
       "771                            NaN                        NaN   \n",
       "1029                           NaN                        NaN   \n",
       "1280                           0.0                        0.5   \n",
       "\n",
       "      classification/accuracy  classification/weighted_f1_score  \n",
       "257                  0.343750                          0.262419  \n",
       "513                  0.515625                          0.435863  \n",
       "771                  0.593750                          0.579313  \n",
       "1029                 0.718750                          0.710382  \n",
       "1280                 0.687500                          0.663262  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('result_paraphrase.csv')\n",
    "results[results['classification/accuracy'].notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada + Data augmentation: Random word substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_sentences = pd.read_excel('training_data.xlsx')\n",
    "labelled_sentences = labelled_sentences[['Sentence', 'Score']]\n",
    "\n",
    "# align data type for Score column to string\n",
    "score_map = {-1:'1', -0.5:'2', 0:'3', 0.5:'4', 1:'5', 'Remove':'Remove'}\n",
    "labelled_sentences['Score'] = labelled_sentences['Score'].apply(lambda x: score_map[x])\n",
    "\n",
    "# split data\n",
    "train, test = train_test_split(labelled_sentences, test_size=0.2, random_state=23, stratify=labelled_sentences['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Word Augmenter\n",
    "# swaps words randomly in sentence\n",
    "import nlpaug.augmenter.word as naw\n",
    "aug = naw.RandomWordAug(action=\"swap\")\n",
    "\n",
    "train_temp = copy.deepcopy(train)\n",
    "train_temp['Sentence'] = train_temp['Sentence'].apply(lambda x: aug.augment(x)[0])\n",
    "train= pd.concat([train, train_temp], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = preprocess(train)\n",
    "\n",
    "# split train data for train and validate\n",
    "train_gpt, test_gpt = train_test_split(train_new, test_size=0.2, random_state=23)\n",
    "\n",
    "# save training and validation data to jsonl files\n",
    "train_gpt.to_json('train_gpt_random_swap.jsonl', orient='records', lines=True)\n",
    "test_gpt.to_json('val_gpt_random_swap.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from train_gpt_random_swap.jsonl: file-XXY97Qadf8qr8AGeL18PTtDg\n",
      "Uploaded file from val_gpt_random_swap.jsonl: file-wLxbe8dI43l2mBs2ozBgvKn9\n",
      "Created fine-tune: ft-33DwIEilkbEeYis4grqFv0mr\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-06-16 14:51:41] Created fine-tune: ft-33DwIEilkbEeYis4grqFv0mr\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload progress:   0%|          | 0.00/61.8k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61.8k/61.8k [00:00<00:00, 61.8Mit/s]\n",
      "\n",
      "Upload progress:   0%|          | 0.00/16.0k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.0k/16.0k [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# create fine tunes model\n",
    "!openai api fine_tunes.create -t \"train_gpt_random_swap.jsonl\" -v \"val_gpt_random_swap.jsonl\" --compute_classification_metrics --classification_n_classes 6 -m ada --n_epochs 5 --suffix \"nlp_ada_random_swap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-16 14:51:41] Created fine-tune: ft-33DwIEilkbEeYis4grqFv0mr\n",
      "[2023-06-16 14:53:03] Fine-tune costs $0.02\n",
      "[2023-06-16 14:53:04] Fine-tune enqueued. Queue number: 0\n",
      "[2023-06-16 15:03:08] Fine-tune started\n",
      "[2023-06-16 15:04:03] Completed epoch 1/5\n",
      "[2023-06-16 15:04:44] Completed epoch 2/5\n",
      "[2023-06-16 15:05:24] Completed epoch 3/5\n",
      "[2023-06-16 15:06:05] Completed epoch 4/5\n",
      "[2023-06-16 15:06:45] Completed epoch 5/5\n",
      "[2023-06-16 15:07:12] Uploaded model: ada:ft-personal:nlp-ada-random-swap-2023-06-16-07-07-12\n",
      "[2023-06-16 15:07:13] Uploaded result file: file-XYhukLHDQniO6rxenn0LIWau\n",
      "[2023-06-16 15:07:13] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded ðŸŽ‰\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-personal:nlp-ada-random-swap-2023-06-16-07-07-12 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i ft-33DwIEilkbEeYis4grqFv0mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.results -i ft-33DwIEilkbEeYis4grqFv0mr > result_random_swap.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>elapsed_tokens</th>\n",
       "      <th>elapsed_examples</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_sequence_accuracy</th>\n",
       "      <th>training_token_accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_sequence_accuracy</th>\n",
       "      <th>validation_token_accuracy</th>\n",
       "      <th>classification/accuracy</th>\n",
       "      <th>classification/weighted_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>257</td>\n",
       "      <td>12369</td>\n",
       "      <td>257</td>\n",
       "      <td>0.059696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.062358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.092780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>514</td>\n",
       "      <td>24698</td>\n",
       "      <td>514</td>\n",
       "      <td>0.044197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.620056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>770</td>\n",
       "      <td>37074</td>\n",
       "      <td>770</td>\n",
       "      <td>0.025310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.635237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>1027</td>\n",
       "      <td>49387</td>\n",
       "      <td>1027</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.818468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>1282</td>\n",
       "      <td>61634</td>\n",
       "      <td>1282</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.857551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      step  elapsed_tokens  elapsed_examples  training_loss  \\\n",
       "256    257           12369               257       0.059696   \n",
       "513    514           24698               514       0.044197   \n",
       "769    770           37074               770       0.025310   \n",
       "1026  1027           49387              1027       0.005591   \n",
       "1281  1282           61634              1282       0.022434   \n",
       "\n",
       "      training_sequence_accuracy  training_token_accuracy  validation_loss  \\\n",
       "256                          0.0                      0.5         0.062358   \n",
       "513                          1.0                      1.0              NaN   \n",
       "769                          1.0                      1.0              NaN   \n",
       "1026                         1.0                      1.0              NaN   \n",
       "1281                         1.0                      1.0              NaN   \n",
       "\n",
       "      validation_sequence_accuracy  validation_token_accuracy  \\\n",
       "256                            0.0                        0.5   \n",
       "513                            NaN                        NaN   \n",
       "769                            NaN                        NaN   \n",
       "1026                           NaN                        NaN   \n",
       "1281                           NaN                        NaN   \n",
       "\n",
       "      classification/accuracy  classification/weighted_f1_score  \n",
       "256                  0.203125                          0.092780  \n",
       "513                  0.671875                          0.620056  \n",
       "769                  0.671875                          0.635237  \n",
       "1026                 0.828125                          0.818468  \n",
       "1281                 0.859375                          0.857551  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('result_random_swap.csv')\n",
    "results[results['classification/accuracy'].notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada + Data augmentation: Wordnet word substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_sentences = pd.read_excel('training_data.xlsx')\n",
    "labelled_sentences = labelled_sentences[['Sentence', 'Score']]\n",
    "\n",
    "# align data type for Score column to string\n",
    "score_map = {-1:'1', -0.5:'2', 0:'3', 0.5:'4', 1:'5', 'Remove':'Remove'}\n",
    "labelled_sentences['Score'] = labelled_sentences['Score'].apply(lambda x: score_map[x])\n",
    "\n",
    "# split data\n",
    "train, test = train_test_split(labelled_sentences, test_size=0.2, random_state=23, stratify=labelled_sentences['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym Augmenter\n",
    "# substitution by WordNet's synonym\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "train_temp = copy.deepcopy(train)\n",
    "train_temp['Sentence'] = train_temp['Sentence'].apply(lambda x: aug.augment(x)[0])\n",
    "train= pd.concat([train, train_temp], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = preprocess(train)\n",
    "\n",
    "# split train data for train and validate\n",
    "train_gpt, test_gpt = train_test_split(train_new, test_size=0.2, random_state=23)\n",
    "\n",
    "# save training and validation data to jsonl files\n",
    "train_gpt.to_json('train_gpt_wordnet_swap.jsonl', orient='records', lines=True)\n",
    "test_gpt.to_json('val_gpt_wordnet_swap.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from train_gpt_wordnet_swap.jsonl: file-UKlKBKQLMMsqZqtaJXZ0rfhf\n",
      "Uploaded file from val_gpt_wordnet_swap.jsonl: file-9qFuTpmBWmB4fXVOSbsHlFDF\n",
      "Created fine-tune: ft-X1H6TIUiv20CjEBEs7uI1sPH\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-06-16 15:11:36] Created fine-tune: ft-X1H6TIUiv20CjEBEs7uI1sPH\n",
      "[2023-06-16 15:12:19] Fine-tune costs $0.02\n",
      "[2023-06-16 15:12:19] Fine-tune enqueued. Queue number: 0\n",
      "[2023-06-16 15:12:20] Fine-tune started\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload progress:   0%|          | 0.00/63.1k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63.1k/63.1k [00:00<00:00, 63.2Mit/s]\n",
      "\n",
      "Upload progress:   0%|          | 0.00/16.4k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.4k/16.4k [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# create fine tunes model\n",
    "!openai api fine_tunes.create -t \"train_gpt_wordnet_swap.jsonl\" -v \"val_gpt_wordnet_swap.jsonl\" --compute_classification_metrics --classification_n_classes 6 -m ada --n_epochs 5 --suffix \"nlp_ada_wordnet_swap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-16 15:11:36] Created fine-tune: ft-X1H6TIUiv20CjEBEs7uI1sPH\n",
      "[2023-06-16 15:12:19] Fine-tune costs $0.02\n",
      "[2023-06-16 15:12:19] Fine-tune enqueued. Queue number: 0\n",
      "[2023-06-16 15:12:20] Fine-tune started\n",
      "[2023-06-16 15:13:16] Completed epoch 1/5\n",
      "[2023-06-16 15:13:58] Completed epoch 2/5\n",
      "[2023-06-16 15:14:38] Completed epoch 3/5\n",
      "[2023-06-16 15:15:18] Completed epoch 4/5\n",
      "[2023-06-16 15:15:58] Completed epoch 5/5\n",
      "[2023-06-16 15:16:25] Uploaded model: ada:ft-personal:nlp-ada-wordnet-swap-2023-06-16-07-16-24\n",
      "[2023-06-16 15:16:26] Uploaded result file: file-TxLaU4z4RgPTHX9xnM2mvvKd\n",
      "[2023-06-16 15:16:26] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded ðŸŽ‰\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-personal:nlp-ada-wordnet-swap-2023-06-16-07-16-24 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i ft-X1H6TIUiv20CjEBEs7uI1sPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.results -i ft-X1H6TIUiv20CjEBEs7uI1sPH > result_wordnet_swap.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>elapsed_tokens</th>\n",
       "      <th>elapsed_examples</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_sequence_accuracy</th>\n",
       "      <th>training_token_accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_sequence_accuracy</th>\n",
       "      <th>validation_token_accuracy</th>\n",
       "      <th>classification/accuracy</th>\n",
       "      <th>classification/weighted_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>259</td>\n",
       "      <td>12795</td>\n",
       "      <td>259</td>\n",
       "      <td>0.052101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.310119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>517</td>\n",
       "      <td>25445</td>\n",
       "      <td>517</td>\n",
       "      <td>0.066325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.542972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>772</td>\n",
       "      <td>38140</td>\n",
       "      <td>772</td>\n",
       "      <td>0.021665</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.582568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>1029</td>\n",
       "      <td>50781</td>\n",
       "      <td>1029</td>\n",
       "      <td>0.019316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.811083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>1283</td>\n",
       "      <td>63363</td>\n",
       "      <td>1283</td>\n",
       "      <td>0.022246</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.825057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      step  elapsed_tokens  elapsed_examples  training_loss  \\\n",
       "258    259           12795               259       0.052101   \n",
       "516    517           25445               517       0.066325   \n",
       "771    772           38140               772       0.021665   \n",
       "1028  1029           50781              1029       0.019316   \n",
       "1282  1283           63363              1283       0.022246   \n",
       "\n",
       "      training_sequence_accuracy  training_token_accuracy  validation_loss  \\\n",
       "258                          0.0                      0.5              NaN   \n",
       "516                          0.0                      0.5              NaN   \n",
       "771                          1.0                      1.0              NaN   \n",
       "1028                         1.0                      1.0              NaN   \n",
       "1282                         1.0                      1.0              NaN   \n",
       "\n",
       "      validation_sequence_accuracy  validation_token_accuracy  \\\n",
       "258                            NaN                        NaN   \n",
       "516                            NaN                        NaN   \n",
       "771                            NaN                        NaN   \n",
       "1028                           NaN                        NaN   \n",
       "1282                           NaN                        NaN   \n",
       "\n",
       "      classification/accuracy  classification/weighted_f1_score  \n",
       "258                  0.390625                          0.310119  \n",
       "516                  0.609375                          0.542972  \n",
       "771                  0.656250                          0.582568  \n",
       "1028                 0.812500                          0.811083  \n",
       "1282                 0.828125                          0.825057  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('result_wordnet_swap.csv')\n",
    "results[results['classification/accuracy'].notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada + Data augmentation: Google word substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_sentences = pd.read_excel('training_data.xlsx')\n",
    "labelled_sentences = labelled_sentences[['Sentence', 'Score']]\n",
    "\n",
    "# align data type for Score column to string\n",
    "score_map = {-1:'1', -0.5:'2', 0:'3', 0.5:'4', 1:'5', 'Remove':'Remove'}\n",
    "labelled_sentences['Score'] = labelled_sentences['Score'].apply(lambda x: score_map[x])\n",
    "\n",
    "# split data\n",
    "train, test = train_test_split(labelled_sentences, test_size=0.2, random_state=23, stratify=labelled_sentences['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings Augmenter\n",
    "# substitution by word similarity\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path='data/GoogleNews-vectors-negative300.bin',\n",
    "    action=\"substitute\")\n",
    "\n",
    "train_temp = copy.deepcopy(train)\n",
    "train_temp['Sentence'] = train_temp['Sentence'].apply(lambda x: aug.augment(x)[0])\n",
    "train= pd.concat([train, train_temp], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = preprocess(train)\n",
    "\n",
    "# split train data for train and validate\n",
    "train_gpt, test_gpt = train_test_split(train_new, test_size=0.2, random_state=23)\n",
    "\n",
    "# save training and validation data to jsonl files\n",
    "train_gpt.to_json('train_gpt_google_swap.jsonl', orient='records', lines=True)\n",
    "test_gpt.to_json('val_gpt_google_swap.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from train_gpt_google_swap.jsonl: file-12MnQn3zQ41wBz9r0HbZGA9M\n",
      "Uploaded file from val_gpt_google_swap.jsonl: file-iJGa6vzmz8PJOEuFPwyLRgeU\n",
      "Created fine-tune: ft-uzVc3K7S90WgvMgifKU0fbyP\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-06-16 15:21:17] Created fine-tune: ft-uzVc3K7S90WgvMgifKU0fbyP\n",
      "\n",
      "Stream interrupted (client disconnected).\n",
      "To resume the stream, run:\n",
      "\n",
      "  openai api fine_tunes.follow -i ft-uzVc3K7S90WgvMgifKU0fbyP\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload progress:   0%|          | 0.00/67.7k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67.7k/67.7k [00:00<?, ?it/s]\n",
      "\n",
      "Upload progress:   0%|          | 0.00/17.6k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.6k/17.6k [00:00<00:00, 17.0Mit/s]\n"
     ]
    }
   ],
   "source": [
    "# create fine tunes model\n",
    "!openai api fine_tunes.create -t \"train_gpt_google_swap.jsonl\" -v \"val_gpt_google_swap.jsonl\" --compute_classification_metrics --classification_n_classes 6 -m ada --n_epochs 5 --suffix \"nlp_ada_google\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-16 15:21:17] Created fine-tune: ft-uzVc3K7S90WgvMgifKU0fbyP\n",
      "[2023-06-16 15:22:30] Fine-tune costs $0.03\n",
      "[2023-06-16 15:22:30] Fine-tune enqueued. Queue number: 1\n",
      "[2023-06-16 15:22:32] Fine-tune started\n",
      "[2023-06-16 15:23:28] Completed epoch 1/5\n",
      "[2023-06-16 15:24:10] Completed epoch 2/5\n",
      "[2023-06-16 15:24:50] Completed epoch 3/5\n",
      "[2023-06-16 15:25:30] Completed epoch 4/5\n",
      "[2023-06-16 15:26:11] Completed epoch 5/5\n",
      "[2023-06-16 15:26:36] Uploaded model: ada:ft-personal:nlp-ada-google-2023-06-16-07-26-36\n",
      "[2023-06-16 15:26:37] Uploaded result file: file-ltBcgrOrJFSeUHs0XIyE62Qo\n",
      "[2023-06-16 15:26:37] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded ðŸŽ‰\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m ada:ft-personal:nlp-ada-google-2023-06-16-07-26-36 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i ft-uzVc3K7S90WgvMgifKU0fbyP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.results -i ft-uzVc3K7S90WgvMgifKU0fbyP > result_google_swap.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>elapsed_tokens</th>\n",
       "      <th>elapsed_examples</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_sequence_accuracy</th>\n",
       "      <th>training_token_accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_sequence_accuracy</th>\n",
       "      <th>validation_token_accuracy</th>\n",
       "      <th>classification/accuracy</th>\n",
       "      <th>classification/weighted_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>257</td>\n",
       "      <td>14857</td>\n",
       "      <td>257</td>\n",
       "      <td>0.087228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.041054</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.106061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>514</td>\n",
       "      <td>29706</td>\n",
       "      <td>514</td>\n",
       "      <td>0.082291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.186984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>771</td>\n",
       "      <td>44563</td>\n",
       "      <td>771</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.510378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>1027</td>\n",
       "      <td>59379</td>\n",
       "      <td>1027</td>\n",
       "      <td>0.034236</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.637904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>1282</td>\n",
       "      <td>74082</td>\n",
       "      <td>1282</td>\n",
       "      <td>0.007746</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.777509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      step  elapsed_tokens  elapsed_examples  training_loss  \\\n",
       "256    257           14857               257       0.087228   \n",
       "513    514           29706               514       0.082291   \n",
       "770    771           44563               771       0.008254   \n",
       "1026  1027           59379              1027       0.034236   \n",
       "1281  1282           74082              1282       0.007746   \n",
       "\n",
       "      training_sequence_accuracy  training_token_accuracy  validation_loss  \\\n",
       "256                          0.0                      0.5         0.041054   \n",
       "513                          0.0                      0.5              NaN   \n",
       "770                          1.0                      1.0              NaN   \n",
       "1026                         1.0                      1.0              NaN   \n",
       "1281                         1.0                      1.0              NaN   \n",
       "\n",
       "      validation_sequence_accuracy  validation_token_accuracy  \\\n",
       "256                            1.0                        1.0   \n",
       "513                            NaN                        NaN   \n",
       "770                            NaN                        NaN   \n",
       "1026                           NaN                        NaN   \n",
       "1281                           NaN                        NaN   \n",
       "\n",
       "      classification/accuracy  classification/weighted_f1_score  \n",
       "256                  0.234375                          0.106061  \n",
       "513                  0.296875                          0.186984  \n",
       "770                  0.562500                          0.510378  \n",
       "1026                 0.703125                          0.637904  \n",
       "1281                 0.781250                          0.777509  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('result_google_swap.csv')\n",
    "results[results['classification/accuracy'].notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model_name, ft_model, test_data):\n",
    "    test_sentences = test_data['Sentence'].tolist()\n",
    "    score_true = test_data['Score'].tolist()\n",
    "    score_pred = []\n",
    "\n",
    "    for s in test_sentences:\n",
    "        res = openai.Completion.create(model = ft_model, \n",
    "                                       prompt = s + ' \\n\\n###\\n\\n', \n",
    "                                       max_tokens = 1, \n",
    "                                       temperature = 0)\n",
    "        classify = res['choices'][0]['text'].strip()\n",
    "        score_pred.append(classify)\n",
    "\n",
    "    # map back to original labels\n",
    "    mapping = {'1':'-1', '2':'-0.5', '3':'0', '4':'0.5', '5':'1', 'Remove':'Remove'}\n",
    "    score_true = [mapping[score] for score in score_true]\n",
    "    score_pred = [mapping[score] for score in score_pred]\n",
    "\n",
    "    print(model_name)\n",
    "    print(classification_report(score_true, score_pred))\n",
    "    print('balanced accuracy: ', balanced_accuracy_score(score_true, score_pred))\n",
    "\n",
    "    # calculate mean squared error by excluding 'Remove' classes\n",
    "    ordinal_label_idx = []\n",
    "    for idx, cls in enumerate(list):\n",
    "        if cls != 'Remove':\n",
    "            ordinal_label_idx.append(idx)\n",
    "\n",
    "    score_true_modified = [float(cls) for index, cls in enumerate(score_true) if index in ordinal_label_idx]\n",
    "    score_pred_modified = [cls for index, cls in enumerate(score_pred) if index in ordinal_label_idx]\n",
    "    score_pred_modified = [float(cls) if cls != 'Remove' else 0 for cls in score_pred_modified]\n",
    "\n",
    "    squared_diff = [(actual - predicted) ** 2 for actual, predicted in zip(score_true_modified, score_pred_modified)]\n",
    "    mse = np.mean(squared_diff)\n",
    "    print('Mean squared error:', round(mse,2))\n",
    "\n",
    "    print('_ '*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada (no data augmentation)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -0.5       0.67      0.50      0.57         4\n",
      "          -1       0.50      0.50      0.50         8\n",
      "           0       0.65      0.92      0.76        12\n",
      "         0.5       0.60      0.43      0.50         7\n",
      "           1       0.60      0.50      0.55         6\n",
      "      Remove       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.67      0.59      0.61        40\n",
      "weighted avg       0.63      0.62      0.61        40\n",
      "\n",
      "balanced accuracy:  0.5853174603174602\n",
      "Mean squared error: 0.14\n",
      "______________________________\n",
      "Ada with paraphrase\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -0.5       0.50      0.25      0.33         4\n",
      "          -1       0.71      0.62      0.67         8\n",
      "           0       0.79      0.92      0.85        12\n",
      "         0.5       0.44      0.57      0.50         7\n",
      "           1       0.50      0.50      0.50         6\n",
      "      Remove       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.65        40\n",
      "   macro avg       0.66      0.59      0.61        40\n",
      "weighted avg       0.66      0.65      0.64        40\n",
      "\n",
      "balanced accuracy:  0.5882936507936508\n",
      "Mean squared error: 0.29\n",
      "______________________________\n",
      "Ada with Random word sub\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -0.5       0.25      0.25      0.25         4\n",
      "          -1       0.62      0.62      0.62         8\n",
      "           0       0.85      0.92      0.88        12\n",
      "         0.5       0.57      0.57      0.57         7\n",
      "           1       0.67      0.67      0.67         6\n",
      "      Remove       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.68        40\n",
      "   macro avg       0.66      0.62      0.63        40\n",
      "weighted avg       0.68      0.68      0.67        40\n",
      "\n",
      "balanced accuracy:  0.6160714285714285\n",
      "Mean squared error: 0.04\n",
      "______________________________\n",
      "Ada with Wordnet word sub\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -0.5       0.50      0.50      0.50         4\n",
      "          -1       0.75      0.75      0.75         8\n",
      "           0       0.80      1.00      0.89        12\n",
      "         0.5       0.67      0.57      0.62         7\n",
      "           1       0.60      0.50      0.55         6\n",
      "      Remove       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.73        40\n",
      "   macro avg       0.72      0.66      0.68        40\n",
      "weighted avg       0.72      0.72      0.72        40\n",
      "\n",
      "balanced accuracy:  0.6646825396825397\n",
      "Mean squared error: 0.14\n",
      "______________________________\n",
      "Ada with Google word sub\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -0.5       0.40      0.50      0.44         4\n",
      "          -1       0.71      0.62      0.67         8\n",
      "           0       0.92      0.92      0.92        12\n",
      "         0.5       0.50      0.57      0.53         7\n",
      "           1       0.60      0.50      0.55         6\n",
      "      Remove       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.69      0.69      0.68        40\n",
      "weighted avg       0.71      0.70      0.70        40\n",
      "\n",
      "balanced accuracy:  0.685515873015873\n",
      "Mean squared error: 0.14\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(\"Ada (no data augmentation)\",'ada:ft-personal:nlp-ada-no-aug-2023-06-16-05-05-35', test)\n",
    "evaluate_model(\"Ada with paraphrase\",'ada:ft-personal:nlp-ada-para-2023-06-16-06-46-41', test)\n",
    "evaluate_model(\"Ada with Random word sub\",'ada:ft-personal:nlp-ada-random-swap-2023-06-16-07-07-12', test)\n",
    "evaluate_model(\"Ada with Wordnet word sub\",'ada:ft-personal:nlp-ada-wordnet-swap-2023-06-16-07-16-24', test)\n",
    "evaluate_model(\"Ada with Google word sub\",'ada:ft-personal:nlp-ada-google-2023-06-16-07-26-36', test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
