{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DgTlf5hc3tg",
        "outputId": "af9da1a8-7fbd-428b-fe16-b10703a4832a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/SMU_MITB_NLP/project\n"
          ]
        }
      ],
      "source": [
        "# connect google drive folder if using colab, gpu will be needed for gpu enabled models\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SMU_MITB_NLP/project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rUJ0bSXUc2m2"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'FOMC Labelled Sentences.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# run cell to read file from data if using colab\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m labelled_sentences \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39m'\u001b[39;49m\u001b[39mFOMC Labelled Sentences.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m statements \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_excel(\u001b[39m'\u001b[39m\u001b[39mFOMC Statements 1997-2023.xlsx\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\billy\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\billy\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\billy\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    483\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\billy\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1652\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1653\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m   1654\u001b[0m     )\n\u001b[0;32m   1655\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1656\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\billy\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1523\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1525\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1526\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1527\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1528\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1529\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\billy\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FOMC Labelled Sentences.xlsx'"
          ]
        }
      ],
      "source": [
        "# run cell to read file from data if using colab\n",
        "\n",
        "import pandas as pd\n",
        "labelled_sentences = pd.read_excel('FOMC Labelled Sentences.xlsx')\n",
        "statements = pd.read_excel('FOMC Statements 1997-2023.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "eBbuEpb6UwSd",
        "outputId": "c61e8975-b18e-4c39-a234-3186663dfa67"
      },
      "outputs": [],
      "source": [
        "# run cell if using from local/github\n",
        "\n",
        "import pandas as pd\n",
        "# read file from data\n",
        "labelled_sentences = pd.read_excel('data/FOMC Labelled Sentences.xlsx')\n",
        "statements = pd.read_excel('data/FOMC Statements 1997-2023.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5jAde_tc2m3",
        "outputId": "7ac98e62-5f85-4456-9742-35451f925390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 0.0    57\n",
            "-1.0    42\n",
            " 0.5    36\n",
            " 1.0    32\n",
            "-0.5    21\n",
            "Name: Score, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "labelled_sentences.head()\n",
        "\n",
        "# remove row if score contains 'Remove'\n",
        "labelled_sentences = labelled_sentences[labelled_sentences['Score'] != 'Remove']\n",
        "labelled_sentences.shape\n",
        "\n",
        "class_counts = labelled_sentences['Score'].value_counts()\n",
        "print(class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqX0CzCYngtU",
        "outputId": "4df884fd-3a58-48ac-d8cd-e81c80ba2786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "     ------------------------------------- 410.5/410.5 kB 12.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from nlpaug) (1.5.2)\n",
            "Collecting gdown>=4.0.0\n",
            "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from nlpaug) (2.28.1)\n",
            "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from nlpaug) (1.23.5)\n",
            "Requirement already satisfied: filelock in c:\\users\\billy\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.9.0)\n",
            "Requirement already satisfied: six in c:\\users\\billy\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.11.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\billy\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.64.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2022.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.3.2.post1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\billy\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\billy\\anaconda3\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n",
            "Installing collected packages: gdown, nlpaug\n",
            "Successfully installed gdown-4.7.1 nlpaug-1.1.11\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "!pip install nlpaug # needed if using colab, else skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "izM6WhSPUwSe"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9tNJNF4UwSe",
        "outputId": "1231f939-378b-4bfa-c9a6-a36b552448fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 0.0    45\n",
            "-1.0    33\n",
            " 0.5    29\n",
            " 1.0    26\n",
            "-0.5    17\n",
            "Name: Score, dtype: int64\n",
            " 0.0    12\n",
            "-1.0     9\n",
            " 0.5     7\n",
            " 1.0     6\n",
            "-0.5     4\n",
            "Name: Score, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# leave out 20% of the data for testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# only keep sentence and score columns\n",
        "\n",
        "\n",
        "labelled_sentences = labelled_sentences[['Sentence', 'Score']]\n",
        "\n",
        "train, test = train_test_split(labelled_sentences, test_size=0.2, random_state=23, stratify=labelled_sentences['Score'])\n",
        "\n",
        "class_counts_train = train['Score'].value_counts()\n",
        "class_counts_test = test['Score'].value_counts()\n",
        "# Print the counts\n",
        "print(class_counts_train)\n",
        "print(class_counts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1sfbvtNZUwSe"
      },
      "outputs": [],
      "source": [
        "# Function to create new dataframe with augmented data and label by providing augmenter\n",
        "\n",
        "def augment_dataset(data, augmenter, num_augmented=2):\n",
        "    data_augmented = data.copy()\n",
        "    augmented_data = []\n",
        "\n",
        "    for index, row in data_augmented.iterrows():\n",
        "        for _ in range(num_augmented):\n",
        "            augmented_data.append([ ' '.join(augmenter.augment(row['Sentence'])), row['Score']])\n",
        "\n",
        "    augmented_data_df = pd.DataFrame(augmented_data, columns=['Sentence', 'Score'])\n",
        "\n",
        "    return augmented_data_df\n",
        "\n",
        "def combine_datasets(original_data, augmented_data):\n",
        "    # Combine the original dataset and the augmented dataset into a single dataframe\n",
        "    combined_data = pd.concat([original_data, augmented_data], ignore_index=True)\n",
        "    \n",
        "    return combined_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "eACpkUw-UwSf",
        "outputId": "46e29fc6-bd5e-4809-866d-9a0b062d1b42"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Inflation has picked upward in late months, pr...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ostentatiousness get picked upward in recent m...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The ongoing public wellness crisis continues t...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The on going public wellness crisis continues ...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>_x000D_ _x000D_ Although the necessary realloc...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>In contrast, the probability, though minor, of...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>The Committee expects that, with appropriate p...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>The Committee expects that, with appropriate i...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>The Federal Reserve is prepare to modify these...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>The Federal Reserve is fix to modify these pla...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  Score\n",
              "0    Inflation has picked upward in late months, pr...    1.0\n",
              "1    Ostentatiousness get picked upward in recent m...    1.0\n",
              "2    The ongoing public wellness crisis continues t...   -1.0\n",
              "3    The on going public wellness crisis continues ...   -1.0\n",
              "4    _x000D_ _x000D_ Although the necessary realloc...    0.5\n",
              "..                                                 ...    ...\n",
              "295  In contrast, the probability, though minor, of...   -1.0\n",
              "296  The Committee expects that, with appropriate p...    0.5\n",
              "297  The Committee expects that, with appropriate i...    0.5\n",
              "298  The Federal Reserve is prepare to modify these...   -0.5\n",
              "299  The Federal Reserve is fix to modify these pla...   -0.5\n",
              "\n",
              "[300 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "augmented_data = augment_dataset(train, aug)\n",
        "augmented_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "LldQoq5mUwSf",
        "outputId": "9e6d9df2-6828-4802-a1fb-8619c7268482"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Inflation has picked up in recent months, main...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The ongoing public health crisis continues to ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>_x000D_\\n_x000D_\\nAlthough the necessary reall...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Recent developments are likely to result in ti...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In determining the timing and size of future a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>In contrast, the probability, though minor, of...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>The Committee expects that, with appropriate p...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>The Committee expects that, with appropriate i...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>The Federal Reserve is prepare to modify these...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>The Federal Reserve is fix to modify these pla...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>450 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence Score\n",
              "0    Inflation has picked up in recent months, main...     1\n",
              "1    The ongoing public health crisis continues to ...    -1\n",
              "2    _x000D_\\n_x000D_\\nAlthough the necessary reall...   0.5\n",
              "3    Recent developments are likely to result in ti...    -1\n",
              "4    In determining the timing and size of future a...     0\n",
              "..                                                 ...   ...\n",
              "445  In contrast, the probability, though minor, of...  -1.0\n",
              "446  The Committee expects that, with appropriate p...   0.5\n",
              "447  The Committee expects that, with appropriate i...   0.5\n",
              "448  The Federal Reserve is prepare to modify these...  -0.5\n",
              "449  The Federal Reserve is fix to modify these pla...  -0.5\n",
              "\n",
              "[450 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_augmented = combine_datasets(train, augmented_data)\n",
        "train_augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gSrhcNwUwSf"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JHqxerIz4571"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRB3-PMjVBvP"
      },
      "outputs": [],
      "source": [
        "# !pip install keras-self-attention # needed if using colab, else skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtG-OQtac2m4"
      },
      "outputs": [],
      "source": [
        "# RNN\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Bidirectional, SimpleRNN, LeakyReLU, Dropout, MultiHeadAttention, Flatten\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujGd8qRnc2m4",
        "outputId": "4d48568f-d07e-49d3-ba24-b959cdc11ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 16s 954ms/step - loss: 1.6003 - accuracy: 0.3067 - val_loss: 1.6049 - val_accuracy: 0.1579\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 1s 231ms/step - loss: 1.5698 - accuracy: 0.3400 - val_loss: 1.6165 - val_accuracy: 0.1579\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 1s 175ms/step - loss: 1.5582 - accuracy: 0.3400 - val_loss: 1.6829 - val_accuracy: 0.1579\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 1s 209ms/step - loss: 1.5404 - accuracy: 0.3400 - val_loss: 1.6452 - val_accuracy: 0.1579\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 1.5362 - accuracy: 0.3400 - val_loss: 1.6254 - val_accuracy: 0.1579\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 1s 190ms/step - loss: 1.5292 - accuracy: 0.3400 - val_loss: 1.6341 - val_accuracy: 0.1579\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 1s 104ms/step - loss: 1.5169 - accuracy: 0.3400 - val_loss: 1.6501 - val_accuracy: 0.1579\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 1.4801 - accuracy: 0.3400 - val_loss: 1.6333 - val_accuracy: 0.1579\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 1s 197ms/step - loss: 1.4060 - accuracy: 0.3400 - val_loss: 1.6127 - val_accuracy: 0.1579\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 1.2683 - accuracy: 0.4600 - val_loss: 1.5818 - val_accuracy: 0.2368\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 1s 299ms/step - loss: 1.1604 - accuracy: 0.5200 - val_loss: 1.6382 - val_accuracy: 0.2368\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 1.0983 - accuracy: 0.5133 - val_loss: 1.6138 - val_accuracy: 0.2632\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 2s 346ms/step - loss: 1.0969 - accuracy: 0.5333 - val_loss: 1.6594 - val_accuracy: 0.2632\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 1.0123 - accuracy: 0.5533 - val_loss: 1.6651 - val_accuracy: 0.2895\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 1s 95ms/step - loss: 1.0211 - accuracy: 0.6000 - val_loss: 1.7483 - val_accuracy: 0.3421\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.9441 - accuracy: 0.6400 - val_loss: 1.6043 - val_accuracy: 0.3158\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.8786 - accuracy: 0.7200 - val_loss: 1.5866 - val_accuracy: 0.5263\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 1s 189ms/step - loss: 0.8033 - accuracy: 0.7533 - val_loss: 1.6530 - val_accuracy: 0.4474\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 1s 268ms/step - loss: 0.7293 - accuracy: 0.6867 - val_loss: 1.6508 - val_accuracy: 0.3684\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 1s 151ms/step - loss: 0.6509 - accuracy: 0.6933 - val_loss: 1.7716 - val_accuracy: 0.3421\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd00a59d0f0>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "RNN Modelling - \n",
        "\n",
        "Data is first processed using the process_sentences function\n",
        "Tokenizer is created and fit on the training and development sentences\n",
        "Additional preprocessing to ensure the parameters and formats are right\n",
        "Sequential model is created with multiple layers - and to be tested with different layers and parameters\n",
        "Model is compiled with binary_crossentropy loss function for binary classification\n",
        "\"\"\"\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "class_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(class_mapping.keys())\n",
        "y_train = y_train.map(class_mapping)\n",
        "y_test = y_test.map(class_mapping)\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in test_sequences))\n",
        "\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Define the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index for padding\n",
        "\n",
        "# Create a Sequential model, tested many different layers and parameters\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
        "# model.add(Bidirectional(LSTM(50)))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(SimpleRNN(units=32))\n",
        "# model.add(LeakyReLU(alpha=0.01))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 30, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(30)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(Bidirectional(LSTM(units=16, return_sequences=True, dropout=0.5, recurrent_dropout=0.7)))\n",
        "# model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model for multi-class classification\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6) # stop the training process, for example if the validation loss doesn't decrease for 2 consecutive epochs (patience=2). to prevent overfitting\n",
        "\n",
        "# Include early stopping in the model fit\n",
        "rnn = model.fit(train_sequences, y_train, epochs=20, validation_data=(test_sequences, y_test))#, callbacks=[early_stopping])\n",
        "rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k6lXPeMUwSg",
        "outputId": "e2848013-75c0-4cce-914c-cd939a4e5bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "12/12 [==============================] - 13s 282ms/step - loss: 1.5962 - accuracy: 0.3222 - val_loss: 1.5585 - val_accuracy: 0.3556\n",
            "Epoch 2/20\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 1.5605 - accuracy: 0.3361 - val_loss: 1.5215 - val_accuracy: 0.3556\n",
            "Epoch 3/20\n",
            "12/12 [==============================] - 1s 115ms/step - loss: 1.5078 - accuracy: 0.3361 - val_loss: 1.4556 - val_accuracy: 0.3556\n",
            "Epoch 4/20\n",
            "12/12 [==============================] - 1s 127ms/step - loss: 1.3563 - accuracy: 0.3556 - val_loss: 1.3527 - val_accuracy: 0.4667\n",
            "Epoch 5/20\n",
            "12/12 [==============================] - 1s 120ms/step - loss: 1.1635 - accuracy: 0.4750 - val_loss: 1.1564 - val_accuracy: 0.5111\n",
            "Epoch 6/20\n",
            "12/12 [==============================] - 2s 135ms/step - loss: 1.0956 - accuracy: 0.5278 - val_loss: 1.1026 - val_accuracy: 0.5444\n",
            "Epoch 7/20\n",
            "12/12 [==============================] - 1s 120ms/step - loss: 0.9702 - accuracy: 0.6361 - val_loss: 1.0242 - val_accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "12/12 [==============================] - 1s 113ms/step - loss: 0.8247 - accuracy: 0.6472 - val_loss: 1.0262 - val_accuracy: 0.5111\n",
            "Epoch 9/20\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.7309 - accuracy: 0.6583 - val_loss: 0.8906 - val_accuracy: 0.5667\n",
            "Epoch 10/20\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.6044 - accuracy: 0.7111 - val_loss: 0.8249 - val_accuracy: 0.6333\n",
            "Epoch 11/20\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 0.5406 - accuracy: 0.7722 - val_loss: 0.9842 - val_accuracy: 0.5778\n",
            "Epoch 12/20\n",
            "12/12 [==============================] - 2s 128ms/step - loss: 0.4870 - accuracy: 0.8000 - val_loss: 0.9936 - val_accuracy: 0.6222\n",
            "Epoch 13/20\n",
            "12/12 [==============================] - 2s 131ms/step - loss: 0.4721 - accuracy: 0.7861 - val_loss: 0.8085 - val_accuracy: 0.6333\n",
            "Epoch 14/20\n",
            "12/12 [==============================] - 1s 125ms/step - loss: 0.3730 - accuracy: 0.8806 - val_loss: 0.8145 - val_accuracy: 0.7111\n",
            "Epoch 15/20\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.2923 - accuracy: 0.9361 - val_loss: 0.8177 - val_accuracy: 0.6778\n",
            "Epoch 16/20\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.2270 - accuracy: 0.9556 - val_loss: 0.8814 - val_accuracy: 0.7111\n",
            "Epoch 17/20\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 0.1765 - accuracy: 0.9722 - val_loss: 1.0371 - val_accuracy: 0.6667\n",
            "Epoch 18/20\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 0.1304 - accuracy: 0.9861 - val_loss: 0.8539 - val_accuracy: 0.6889\n",
            "Epoch 19/20\n",
            "12/12 [==============================] - 1s 115ms/step - loss: 0.1172 - accuracy: 0.9806 - val_loss: 1.2333 - val_accuracy: 0.6556\n",
            "Epoch 20/20\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 0.1019 - accuracy: 0.9806 - val_loss: 1.0787 - val_accuracy: 0.7000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1de278f6370>"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "RNN Modelling - \n",
        "\n",
        "Data is first processed using the process_sentences function\n",
        "Tokenizer is created and fit on the training and development sentences\n",
        "Additional preprocessing to ensure the parameters and formats are right\n",
        "Sequential model is created with multiple layers - and to be tested with different layers and parameters\n",
        "Model is compiled with binary_crossentropy loss function for binary classification\n",
        "\"\"\"\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_augmented['Sentence'], train_augmented['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "class_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(class_mapping.keys())\n",
        "y_train = y_train.map(class_mapping)\n",
        "y_test = y_test.map(class_mapping)\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in test_sequences))\n",
        "\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Define the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index for padding\n",
        "\n",
        "# Create a Sequential model, tested many different layers and parameters\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
        "# model.add(Bidirectional(LSTM(50)))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(SimpleRNN(units=32))\n",
        "# model.add(LeakyReLU(alpha=0.01))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 30, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(30)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(Bidirectional(LSTM(units=16, return_sequences=True, dropout=0.5, recurrent_dropout=0.7)))\n",
        "# model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model for multi-class classification\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6) # stop the training process, for example if the validation loss doesn't decrease for 2 consecutive epochs (patience=2). to prevent overfitting\n",
        "\n",
        "# Include early stopping in the model fit\n",
        "rnn = model.fit(train_sequences, y_train, epochs=20, validation_data=(test_sequences, y_test))#, callbacks=[early_stopping])\n",
        "rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3oxwcKjUwSh",
        "outputId": "67ca3960-cf6e-4918-f17f-353dfbc2be5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 20ms/step\n",
            "Accuracy: 42.11%\n",
            "[ 1 -1  0  1 -1 -1 -1  1  1  0  1  0 -1  1  0  0  0 -1 -1  0  1  0  0  0\n",
            "  0  0  1  0  1  1  1  0  1  1  1 -1  1 -1]\n",
            "[1 -1 1 1 -1 -1 -1 0.5 0.5 1 0.5 -1 -1 0.5 0 0.5 -0.5 0.5 1 -0.5 1 0.5 0 0\n",
            " -1 0.5 -1 0 1 -1 1 -0.5 -1 1 1 0 -1 0]\n"
          ]
        }
      ],
      "source": [
        "# Assuming your original test sentences are in a pandas DataFrame column named 'Sentence'\n",
        "# Tokenize the test sentences\n",
        "test_sequences_original = tokenizer.texts_to_sequences(test['Sentence'])\n",
        "\n",
        "# Pad the sequences\n",
        "test_sequences_original = pad_sequences(test_sequences_original, maxlen=max_length, padding='post')\n",
        "\n",
        "# Predict class probabilities for the test set\n",
        "y_pred_prob = model.predict(test_sequences_original)\n",
        "\n",
        "# Select the class with the highest probability as the predicted class\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in class_mapping.items()}\n",
        "\n",
        "# Inverse map the classes to their original values\n",
        "y_pred = np.vectorize(inverse_class_mapping.get)(y_pred)\n",
        "\n",
        "y_test_original = test['Score']\n",
        "\n",
        "accuracy = np.mean(y_pred == y_test_original.values)\n",
        "\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "print(y_pred)\n",
        "print(y_test_original.values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0DOnr9IUwSh",
        "outputId": "4ff82222-a91f-471f-aefd-418ba410cfb5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>But domestic financial markets have recovered ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>The Committee will continue its purchases of T...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>In light of the substantial further progress t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>Job gains have been solid in recent months, an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>In a related action, the Board of Governors un...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>\\nThe Federal Reserve is committed to using it...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>The possibility that this excess could continu...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>However, a sustained moderation in inflation p...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>However, the Committee judges that some inflat...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>\\nInformation received since the Federal Open ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>This program, which would gradually reduce the...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>The erosion in current and prospective profita...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>Russia's war against Ukraine is causing tremen...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>However, a sustained moderation in inflation p...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>This action accelerates the release of this in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>_x000D_\\n_x000D_\\nAlthough the necessary reall...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>This policy, by keeping the Committee's holdin...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>When the Committee decides to begin to remove ...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>\\n_x000D_\\nThe Federal Open Market Committee v...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>The Committee is maintaining its existing poli...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>In support of these goals, the Committee decid...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>A range of recent labor market indicators, inc...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>Inflation _x000D_\\n        and longer-term inf...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>Overall financial conditions remain accommodat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>The erosion in current and prospective profita...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>The Committee expects that economic conditions...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Currently, the unemployment rate remains eleva...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>The Committee perceives the upside and downsid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>With progress on vaccinations and strong polic...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Recent developments are likely to result in ti...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>In light of the improving economy, Mr. Hoenig ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>With underlying inflation expected to be relat...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>The Committee currently anticipates that, even...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>Voting against the policy action was Thomas M....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Voting against the action was Esther L. George...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>The arrangement with the Bank of Canada would ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>Tight credit conditions, the ongoing housing c...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Available data suggest that household spending...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence Score\n",
              "16   But domestic financial markets have recovered ...     1\n",
              "188  The Committee will continue its purchases of T...    -1\n",
              "198  In light of the substantial further progress t...     1\n",
              "179  Job gains have been solid in recent months, an...     1\n",
              "95   In a related action, the Board of Governors un...    -1\n",
              "145  \\nThe Federal Reserve is committed to using it...    -1\n",
              "174  The possibility that this excess could continu...    -1\n",
              "21   However, a sustained moderation in inflation p...   0.5\n",
              "64   However, the Committee judges that some inflat...   0.5\n",
              "80   \\nInformation received since the Federal Open ...     1\n",
              "35   This program, which would gradually reduce the...   0.5\n",
              "133  The erosion in current and prospective profita...    -1\n",
              "186  Russia's war against Ukraine is causing tremen...    -1\n",
              "163  However, a sustained moderation in inflation p...   0.5\n",
              "113  This action accelerates the release of this in...     0\n",
              "134  _x000D_\\n_x000D_\\nAlthough the necessary reall...   0.5\n",
              "51   This policy, by keeping the Committee's holdin...  -0.5\n",
              "82   When the Committee decides to begin to remove ...   0.5\n",
              "74   \\n_x000D_\\nThe Federal Open Market Committee v...     1\n",
              "118  The Committee is maintaining its existing poli...  -0.5\n",
              "20   In support of these goals, the Committee decid...     1\n",
              "52   A range of recent labor market indicators, inc...   0.5\n",
              "169  Inflation _x000D_\\n        and longer-term inf...     0\n",
              "142  Overall financial conditions remain accommodat...     0\n",
              "168  The erosion in current and prospective profita...    -1\n",
              "167  The Committee expects that economic conditions...   0.5\n",
              "65   Currently, the unemployment rate remains eleva...    -1\n",
              "79   The Committee perceives the upside and downsid...     0\n",
              "9    With progress on vaccinations and strong polic...     1\n",
              "49   Recent developments are likely to result in ti...    -1\n",
              "70   In light of the improving economy, Mr. Hoenig ...     1\n",
              "106  With underlying inflation expected to be relat...  -0.5\n",
              "94   The Committee currently anticipates that, even...    -1\n",
              "180  Voting against the policy action was Thomas M....     1\n",
              "11   Voting against the action was Esther L. George...     1\n",
              "116  The arrangement with the Bank of Canada would ...     0\n",
              "122  Tight credit conditions, the ongoing housing c...    -1\n",
              "31   Available data suggest that household spending...     0"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlmIkNaHUwSh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8OmIwGRUwSi"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jcK237tEc2m5"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIokrzfVc2m6",
        "outputId": "e315145d-c6d6-48c4-f660-2e0526f92cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers # run this cell if using notebook in colab "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjtZSG5jc2m6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaForSequenceClassification, RobertaTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIwSO11jc2m7",
        "outputId": "75b6622f-ee9d-4d01-afcd-dc45161500ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=64)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=64)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load BertForSequenceClassification, adjust num_labels to your classification task\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 16 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL-YxCW5c2m7",
        "outputId": "866106e7-9154-4e1f-8d8c-1506c08edea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 16 ========\n",
            "\n",
            "  Average training loss: 1.61\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.24\n",
            "  Validation Loss: 1.62\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 16 ========\n",
            "\n",
            "  Average training loss: 1.49\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.18\n",
            "  Validation Loss: 1.54\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 16 ========\n",
            "\n",
            "  Average training loss: 1.37\n",
            "  Training epoch took: 0:00:01\n",
            "\n",
            "  Accuracy: 0.41\n",
            "  Validation Loss: 1.51\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 16 ========\n",
            "\n",
            "  Average training loss: 1.26\n",
            "  Training epoch took: 0:00:01\n",
            "\n",
            "  Accuracy: 0.37\n",
            "  Validation Loss: 1.51\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 16 ========\n",
            "\n",
            "  Average training loss: 1.13\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.35\n",
            "  Validation Loss: 1.52\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 16 ========\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.45\n",
            "  Validation Loss: 1.44\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 16 ========\n",
            "\n",
            "  Average training loss: 0.90\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.45\n",
            "  Validation Loss: 1.43\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 16 ========\n",
            "\n",
            "  Average training loss: 0.76\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.45\n",
            "  Validation Loss: 1.34\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 16 ========\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.55\n",
            "  Validation Loss: 1.32\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 16 ========\n",
            "\n",
            "  Average training loss: 0.58\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.30\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 16 ========\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.20\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 16 ========\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 16 ========\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.21\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 16 ========\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.24\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 16 ========\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.26\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 16 / 16 ========\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.26\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmTnZiF0c2m8",
        "outputId": "9ed8feaa-5df9-440a-f30e-3bf10924aa13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Testing...\n",
            "Testing took: 0:00:00\n"
          ]
        }
      ],
      "source": [
        "test_input_ids, test_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=64)\n",
        "\n",
        "\n",
        "\n",
        "# Create the DataLoader for test set\n",
        "test_data = TensorDataset(test_input_ids, test_attention_mask)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "print(\"Running Testing...\")\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# List to store predictions\n",
        "test_predictions = []\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "    \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    \n",
        "    with torch.no_grad():        \n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        output = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask)\n",
        "        logits = output.logits\n",
        "        \n",
        "    # Move logits to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    # Store predictions\n",
        "    test_predictions.extend(np.argmax(logits, axis=1))\n",
        "\n",
        "# Measure how long the testing run took.\n",
        "testing_time = format_time(time.time() - t0)\n",
        "\n",
        "print(\"Testing took: {:}\".format(testing_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3cxnunlc2m8",
        "outputId": "2b7c30d3-9da6-4140-8993-9f4c31731acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: [5, 3, 5, 5, 1, 1, 1, 1, 2, 4, 3, 4, 1, 1, 3, 4, 1, 4, 3, 4, 5, 4, 4, 4, 4, 1, 4, 3, 5, 1, 4, 4, 1, 1, 5, 3, 1, 4]\n"
          ]
        }
      ],
      "source": [
        "# Store predictions with labels starting from 1\n",
        "test_predictions_bert = [prediction+1 for prediction in test_predictions]\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predictions:\", test_predictions_bert)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "raggnBDCqAJl"
      },
      "source": [
        "# ROBERTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fr2bCcylGUP",
        "outputId": "a7defa77-124d-4b0e-8d12-834544b7c7c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=100)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load RobertaForSequenceClassification\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 16 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkFucNnIlOLg",
        "outputId": "3f225f5f-8280-47ce-9028-32b80b36223b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 16 ========\n",
            "\n",
            "  Average training loss: 1.60\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.23\n",
            "  Validation Loss: 1.60\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 16 ========\n",
            "\n",
            "  Average training loss: 1.51\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.23\n",
            "  Validation Loss: 1.71\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 16 ========\n",
            "\n",
            "  Average training loss: 1.48\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.34\n",
            "  Validation Loss: 1.57\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 16 ========\n",
            "\n",
            "  Average training loss: 1.32\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.32\n",
            "  Validation Loss: 1.57\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 16 ========\n",
            "\n",
            "  Average training loss: 1.15\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.42\n",
            "  Validation Loss: 1.49\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 16 ========\n",
            "\n",
            "  Average training loss: 0.98\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 1.46\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 16 ========\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.48\n",
            "  Validation Loss: 1.42\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 16 ========\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.40\n",
            "  Validation Loss: 1.34\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 16 ========\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.31\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 16 ========\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.32\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 16 ========\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 16 ========\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 16 ========\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.04\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 16 ========\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.13\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 16 ========\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 16 / 16 ========\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.20\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9NdDzU4lkYH",
        "outputId": "97fb0cb9-49ae-416b-b483-1876fa78db21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-45-96255f1895ed>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_inputs = torch.tensor(test_input_ids)\n",
            "<ipython-input-45-96255f1895ed>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_masks = torch.tensor(test_attention_mask)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 60.53%\n"
          ]
        }
      ],
      "source": [
        "# Assuming test_sentences are in a pandas DataFrame column named 'Sentence'\n",
        "# Tokenize the test sentences\n",
        "test_input_ids, test_attention_mask = tokenize(test['Sentence'].values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert inputs to tensors\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_masks = torch.tensor(test_attention_mask)\n",
        "\n",
        "# Create DataLoader for the test data\n",
        "batch_size = 32\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predicted_scores = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "    logits = outputs[0]\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predicted_scores.extend(np.argmax(logits, axis=1).flatten())\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in score_mapping.items()}\n",
        "\n",
        "# Inverse map the classes to their original values\n",
        "predicted_scores = np.vectorize(inverse_class_mapping.get)(predicted_scores)\n",
        "\n",
        "accuracy = np.mean(predicted_scores == test['Score'].values)\n",
        "print(f'Test Accuracy: {accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcPPgkImZslL",
        "outputId": "4e4fa1e7-64bd-4eb1-b9b7-9d9d31fd1812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              Sentence  Predicted_Score\n",
            "16   But domestic financial markets have recovered ...              0.5\n",
            "188  The Committee will continue its purchases of T...             -0.5\n",
            "198  In light of the substantial further progress t...              1.0\n",
            "179  Job gains have been solid in recent months, an...              1.0\n",
            "95   In a related action, the Board of Governors un...             -1.0\n",
            "145  \\nThe Federal Reserve is committed to using it...             -1.0\n",
            "174  The possibility that this excess could continu...             -1.0\n",
            "21   However, a sustained moderation in inflation p...              0.5\n",
            "64   However, the Committee judges that some inflat...             -0.5\n",
            "80   \\nInformation received since the Federal Open ...              1.0\n",
            "35   This program, which would gradually reduce the...             -1.0\n",
            "133  The erosion in current and prospective profita...             -1.0\n",
            "186  Russia's war against Ukraine is causing tremen...             -1.0\n",
            "163  However, a sustained moderation in inflation p...              0.5\n",
            "113  This action accelerates the release of this in...              0.0\n",
            "134  _x000D_\\n_x000D_\\nAlthough the necessary reall...              0.5\n",
            "51   This policy, by keeping the Committee's holdin...             -1.0\n",
            "82   When the Committee decides to begin to remove ...             -0.5\n",
            "74   \\n_x000D_\\nThe Federal Open Market Committee v...              0.0\n",
            "118  The Committee is maintaining its existing poli...              0.0\n",
            "20   In support of these goals, the Committee decid...              1.0\n",
            "52   A range of recent labor market indicators, inc...              1.0\n",
            "169  Inflation _x000D_\\n        and longer-term inf...             -0.5\n",
            "142  Overall financial conditions remain accommodat...             -0.5\n",
            "168  The erosion in current and prospective profita...             -1.0\n",
            "167  The Committee expects that economic conditions...             -1.0\n",
            "65   Currently, the unemployment rate remains eleva...             -0.5\n",
            "79   The Committee perceives the upside and downsid...              0.0\n",
            "9    With progress on vaccinations and strong polic...              1.0\n",
            "49   Recent developments are likely to result in ti...             -1.0\n",
            "70   In light of the improving economy, Mr. Hoenig ...              1.0\n",
            "106  With underlying inflation expected to be relat...              0.5\n",
            "94   The Committee currently anticipates that, even...             -1.0\n",
            "180  Voting against the policy action was Thomas M....             -1.0\n",
            "11   Voting against the action was Esther L. George...              1.0\n",
            "116  The arrangement with the Bank of Canada would ...              0.0\n",
            "122  Tight credit conditions, the ongoing housing c...             -1.0\n",
            "31   Available data suggest that household spending...              0.5\n"
          ]
        }
      ],
      "source": [
        "# Add the original sentences and their predicted scores to a DataFrame\n",
        "predictions_df = pd.DataFrame({'Sentence': test['Sentence'], 'Predicted_Score': predicted_scores})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(predictions_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "79EocouQY78b"
      },
      "source": [
        "# RoBERTa Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmq-aui3Y9d4",
        "outputId": "39ef1060-47fb-4692-a618-61542d549985"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=100)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load RobertaForSequenceClassification\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-large\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 24 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNieKcu8Y_gN",
        "outputId": "8649e3d6-defd-4e75-adb7-e61f364ff91c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 24 ========\n",
            "\n",
            "  Average training loss: 1.64\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.23\n",
            "  Validation Loss: 1.55\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 24 ========\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.23\n",
            "  Validation Loss: 1.62\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 24 ========\n",
            "\n",
            "  Average training loss: 1.51\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.21\n",
            "  Validation Loss: 1.74\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 24 ========\n",
            "\n",
            "  Average training loss: 1.46\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.18\n",
            "  Validation Loss: 1.66\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 24 ========\n",
            "\n",
            "  Average training loss: 1.35\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.24\n",
            "  Validation Loss: 1.62\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 24 ========\n",
            "\n",
            "  Average training loss: 1.28\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.21\n",
            "  Validation Loss: 1.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 24 ========\n",
            "\n",
            "  Average training loss: 1.17\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.24\n",
            "  Validation Loss: 1.55\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 24 ========\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.29\n",
            "  Validation Loss: 1.61\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 24 ========\n",
            "\n",
            "  Average training loss: 0.96\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.39\n",
            "  Validation Loss: 1.48\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 24 ========\n",
            "\n",
            "  Average training loss: 0.84\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.30\n",
            "  Validation Loss: 1.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 24 ========\n",
            "\n",
            "  Average training loss: 0.72\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.46\n",
            "  Validation Loss: 1.39\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 24 ========\n",
            "\n",
            "  Average training loss: 0.62\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.33\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 13 / 24 ========\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.45\n",
            "  Validation Loss: 1.41\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 24 ========\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.44\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 24 ========\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.27\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 16 / 24 ========\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.55\n",
            "  Validation Loss: 1.35\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 17 / 24 ========\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.27\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 18 / 24 ========\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.55\n",
            "  Validation Loss: 1.33\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 19 / 24 ========\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.36\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 20 / 24 ========\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.39\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 21 / 24 ========\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.40\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 22 / 24 ========\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.38\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 23 / 24 ========\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.39\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 24 / 24 ========\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.41\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0ZiP6BnZFos"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMbFpo9_cdVB"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6fqhaMM6cd68"
      },
      "source": [
        "# BERT Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iruw_-D8cf4b",
        "outputId": "64c11be0-ee0e-4984-940e-9dbffbe2f741"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=64)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=64)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load BertForSequenceClassification, adjust num_labels to your classification task\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-large-uncased\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 3e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 20 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSRzrxIEciPN",
        "outputId": "fecc6490-d120-486f-ecd8-bd0b3eb233c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "\n",
            "  Average training loss: 1.70\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.41\n",
            "  Validation Loss: 1.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "\n",
            "  Average training loss: 1.43\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.28\n",
            "  Validation Loss: 1.71\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "\n",
            "  Average training loss: 1.26\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.31\n",
            "  Validation Loss: 1.67\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "\n",
            "  Average training loss: 1.13\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.44\n",
            "  Validation Loss: 1.41\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.32\n",
            "  Validation Loss: 1.49\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "\n",
            "  Average training loss: 0.85\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 1.23\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "\n",
            "  Average training loss: 0.61\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.48\n",
            "  Validation Loss: 1.31\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.05\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.07\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.34\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.02\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.01\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.93\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 0.94\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.94\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.95\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.95\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eME9Sc0Ocu51"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FBV_lzmVdu3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUyloaSPVdsb"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1yk_sreUVfHO"
      },
      "source": [
        "# RoBERTa with augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDDHxhNIVg6B",
        "outputId": "2b0a682a-22fe-42b4-e66d-d7ec218d1b8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_augmented['Sentence'], train_augmented['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=100)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load RobertaForSequenceClassification\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 12 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLgOCwY9Vnot",
        "outputId": "d535b122-2ace-4db5-c036-4b4e11580522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 12 ========\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.36\n",
            "  Validation Loss: 1.49\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 12 ========\n",
            "\n",
            "  Average training loss: 1.32\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.54\n",
            "  Validation Loss: 1.22\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 12 ========\n",
            "\n",
            "  Average training loss: 1.12\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.97\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 12 ========\n",
            "\n",
            "  Average training loss: 0.78\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 12 ========\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0:00:07\n",
            "\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 0.41\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 12 ========\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epoch took: 0:00:07\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.32\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 12 ========\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.20\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 12 ========\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.22\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 12 ========\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.35\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.94\n",
            "  Validation Loss: 0.21\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.93\n",
            "  Validation Loss: 0.22\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.23\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdf7whC3V3Lw",
        "outputId": "7ac905e2-3cc8-4a55-f518-0cea90da643d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-96255f1895ed>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_inputs = torch.tensor(test_input_ids)\n",
            "<ipython-input-31-96255f1895ed>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_masks = torch.tensor(test_attention_mask)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 57.89%\n"
          ]
        }
      ],
      "source": [
        "# Assuming test_sentences are in a pandas DataFrame column named 'Sentence'\n",
        "# Tokenize the test sentences\n",
        "test_input_ids, test_attention_mask = tokenize(test['Sentence'].values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert inputs to tensors\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_masks = torch.tensor(test_attention_mask)\n",
        "\n",
        "# Create DataLoader for the test data\n",
        "batch_size = 32\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predicted_scores = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "    logits = outputs[0]\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predicted_scores.extend(np.argmax(logits, axis=1).flatten())\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in score_mapping.items()}\n",
        "\n",
        "# Inverse map the classes to their original values\n",
        "predicted_scores = np.vectorize(inverse_class_mapping.get)(predicted_scores)\n",
        "\n",
        "accuracy = np.mean(predicted_scores == test['Score'].values)\n",
        "print(f'Test Accuracy: {accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46eXj5gYXFhY",
        "outputId": "64aa4209-d17b-4e3f-eed5-67a738fd73a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              Sentence  Predicted_Score\n",
            "16   But domestic financial markets have recovered ...              0.5\n",
            "188  The Committee will continue its purchases of T...             -0.5\n",
            "198  In light of the substantial further progress t...              1.0\n",
            "179  Job gains have been solid in recent months, an...              1.0\n",
            "95   In a related action, the Board of Governors un...             -1.0\n",
            "145  \\nThe Federal Reserve is committed to using it...             -1.0\n",
            "174  The possibility that this excess could continu...             -1.0\n",
            "21   However, a sustained moderation in inflation p...              0.5\n",
            "64   However, the Committee judges that some inflat...             -0.5\n",
            "80   \\nInformation received since the Federal Open ...              1.0\n",
            "35   This program, which would gradually reduce the...              1.0\n",
            "133  The erosion in current and prospective profita...             -1.0\n",
            "186  Russia's war against Ukraine is causing tremen...             -1.0\n",
            "163  However, a sustained moderation in inflation p...              0.5\n",
            "113  This action accelerates the release of this in...              0.0\n",
            "134  _x000D_\\n_x000D_\\nAlthough the necessary reall...              0.5\n",
            "51   This policy, by keeping the Committee's holdin...             -1.0\n",
            "82   When the Committee decides to begin to remove ...              0.0\n",
            "74   \\n_x000D_\\nThe Federal Open Market Committee v...              1.0\n",
            "118  The Committee is maintaining its existing poli...              0.0\n",
            "20   In support of these goals, the Committee decid...              1.0\n",
            "52   A range of recent labor market indicators, inc...              1.0\n",
            "169  Inflation _x000D_\\n        and longer-term inf...             -0.5\n",
            "142  Overall financial conditions remain accommodat...             -0.5\n",
            "168  The erosion in current and prospective profita...             -1.0\n",
            "167  The Committee expects that economic conditions...              0.5\n",
            "65   Currently, the unemployment rate remains eleva...              0.0\n",
            "79   The Committee perceives the upside and downsid...              0.0\n",
            "9    With progress on vaccinations and strong polic...              1.0\n",
            "49   Recent developments are likely to result in ti...             -1.0\n",
            "70   In light of the improving economy, Mr. Hoenig ...              1.0\n",
            "106  With underlying inflation expected to be relat...              0.5\n",
            "94   The Committee currently anticipates that, even...             -0.5\n",
            "180  Voting against the policy action was Thomas M....             -1.0\n",
            "11   Voting against the action was Esther L. George...              1.0\n",
            "116  The arrangement with the Bank of Canada would ...              1.0\n",
            "122  Tight credit conditions, the ongoing housing c...             -1.0\n",
            "31   Available data suggest that household spending...              0.5\n"
          ]
        }
      ],
      "source": [
        "# Add the original sentences and their predicted scores to a DataFrame\n",
        "predictions_df = pd.DataFrame({'Sentence': test['Sentence'], 'Predicted_Score': predicted_scores})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(predictions_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ws7CU4XHcn",
        "outputId": "e3039090-f3b8-430e-97c0-723137e4aab3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.5, -0.5,  1. ,  1. , -1. , -1. , -1. ,  0.5, -0.5,  1. ,  1. ,\n",
              "       -1. , -1. ,  0.5,  0. ,  0.5, -1. ,  0. ,  1. ,  0. ,  1. ,  1. ,\n",
              "       -0.5, -0.5, -1. ,  0.5,  0. ,  0. ,  1. , -1. ,  1. ,  0.5, -0.5,\n",
              "       -1. ,  1. ,  1. , -1. ,  0.5])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxmpzXpDdJzt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaForSequenceClassification, RobertaTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dhFgICQidK5u"
      },
      "source": [
        "# RoBERTA Augmented with MC Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FduWGtHOdJxT",
        "outputId": "9ec9ac2b-0e8d-4071-c8f5-89b0300369d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing MCRoberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing MCRoberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MCRoberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of MCRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_augmented['Sentence'], train_augmented['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=100)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data, replacement=False, generator=torch.Generator().manual_seed(23))\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# RoBERTa model with Monte Carlo Dropout\n",
        "class MCRoberta(RobertaForSequenceClassification):\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "    def eval(self):\n",
        "        self.training = True\n",
        "\n",
        "# Load MCRobertaForSequenceClassification\n",
        "model = MCRoberta.from_pretrained(\n",
        "    \"roberta-base\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 12 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nZMvsICdJuz",
        "outputId": "be3c72ab-d5d6-452e-a7e4-ab81b13ab8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.85\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.84\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.89\n",
            "Accuracy for class 3: 0.85\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.60\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.87\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.89\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.58\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.84\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 0.85\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.84\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.81\n",
            "Accuracy for class 3: 0.92\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.87\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.89\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.55\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.87\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.88\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 1.00\n",
            "  Validation Loss: 0.47\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.87\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.89\n",
            "Accuracy for class 3: 0.92\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.86\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.51\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.86\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.59\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.86\n",
            "Accuracy for class 0: 0.90\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.81\n",
            "Accuracy for class 3: 0.92\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.61\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def flat_accuracy_per_class(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    correct_per_class = defaultdict(int)\n",
        "    total_per_class = defaultdict(int)\n",
        "\n",
        "    for pred, label in zip(pred_flat, labels_flat):\n",
        "        if pred == label:\n",
        "            correct_per_class[label] += 1\n",
        "        total_per_class[label] += 1\n",
        "\n",
        "    accuracies_per_class = {label: correct / total for label, correct, total in zip(correct_per_class.keys(), correct_per_class.values(), total_per_class.values())}\n",
        "    return accuracies_per_class\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    correct_preds_per_class = defaultdict(int)\n",
        "    total_preds_per_class = defaultdict(int)\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "        for label in np.unique(label_ids):\n",
        "            correct_preds_per_class[label] += np.sum((pred_flat == label_ids) & (label_ids == label))\n",
        "            total_preds_per_class[label] += np.sum(label_ids == label)\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "\n",
        "    for label in total_preds_per_class.keys():\n",
        "        accuracy = correct_preds_per_class[label] / total_preds_per_class[label]\n",
        "        print(f\"Accuracy for class {label}: {accuracy:.2f}\")\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "i1bin-itdpvk",
        "outputId": "e6a191fc-b5d5-4778-f79e-dd6ff0741806"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-d7ae4fddb137>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_inputs = torch.tensor(test_input_ids)\n",
            "<ipython-input-29-d7ae4fddb137>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_masks = torch.tensor(test_attention_mask)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 52.63%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ec629e23-334d-4d0f-8e6e-78677fadeb10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Ground Truth</th>\n",
              "      <th>Predicted_Score</th>\n",
              "      <th>Uncertainty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>\\nThe Federal Reserve is committed to using it...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.798450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>In a related action, the Board of Governors ap...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.203834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>However, a sustained moderation in inflation p...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.265498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Currently, the unemployment rate remains eleva...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.904383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>In determining how long to maintain this targe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.190304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Moreover, the high level of resource utilizati...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.566761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>The Committee continues to closely monitor inf...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.813860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>The Committee expects that economic conditions...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.315373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>\\n\\n_x000D_\\nThe Federal Open Market Committee...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.724857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Increases in the prices of energy and other co...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.217486</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec629e23-334d-4d0f-8e6e-78677fadeb10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ec629e23-334d-4d0f-8e6e-78677fadeb10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ec629e23-334d-4d0f-8e6e-78677fadeb10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              Sentence Ground Truth  \\\n",
              "43   \\nThe Federal Reserve is committed to using it...           -1   \n",
              "120  In a related action, the Board of Governors ap...           -1   \n",
              "163  However, a sustained moderation in inflation p...          0.5   \n",
              "65   Currently, the unemployment rate remains eleva...           -1   \n",
              "178  In determining how long to maintain this targe...            0   \n",
              "8    Moreover, the high level of resource utilizati...            1   \n",
              "148  The Committee continues to closely monitor inf...            0   \n",
              "167  The Committee expects that economic conditions...          0.5   \n",
              "23   \\n\\n_x000D_\\nThe Federal Open Market Committee...            0   \n",
              "47   Increases in the prices of energy and other co...            1   \n",
              "\n",
              "     Predicted_Score  Uncertainty  \n",
              "43              -0.5     0.798450  \n",
              "120             -1.0     0.203834  \n",
              "163              0.5     0.265498  \n",
              "65              -0.5     0.904383  \n",
              "178              0.0     0.190304  \n",
              "8               -1.0     1.566761  \n",
              "148             -0.5     0.813860  \n",
              "167             -1.0     0.315373  \n",
              "23               1.0     0.724857  \n",
              "47               1.0     0.217486  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_input_ids, test_attention_mask = tokenize(test['Sentence'].values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert inputs to tensors\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_masks = torch.tensor(test_attention_mask)\n",
        "\n",
        "# Create DataLoader for the test data\n",
        "batch_size = 32\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# For the MCRoberta model, we keep it in train mode to enable dropout\n",
        "model.train()\n",
        "\n",
        "predicted_scores = []\n",
        "uncertainties = []\n",
        "\n",
        "n_mc_samples = 30\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask = batch\n",
        "    \n",
        "    mc_samples = []\n",
        "\n",
        "    for _ in range(n_mc_samples):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            \n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        mc_samples.append(logits)\n",
        "        \n",
        "    mc_samples = np.array(mc_samples)\n",
        "    predicted_score = mc_samples.mean(axis=0)\n",
        "    uncertainty = mc_samples.std(axis=0)\n",
        "\n",
        "    predicted_scores.extend(np.argmax(predicted_score, axis=1).flatten())\n",
        "    uncertainties.extend(uncertainty.max(axis=1).flatten())\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in score_mapping.items()}\n",
        "\n",
        "# Inverse map the classes to their original values\n",
        "predicted_scores = np.vectorize(inverse_class_mapping.get)(predicted_scores)\n",
        "\n",
        "accuracy = np.mean(predicted_scores == test['Score'].values)\n",
        "print(f'Test Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "# Add the original sentences, their predicted scores and uncertainties to a DataFrame\n",
        "predictions_df = pd.DataFrame({'Sentence': test['Sentence'], 'Ground Truth': test['Score'], 'Predicted_Score': predicted_scores, 'Uncertainty': uncertainties})\n",
        "\n",
        "# Print the DataFrame\n",
        "predictions_df[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRRsbihq70u_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix, precision_score, recall_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhDiTWx_fDaW",
        "outputId": "1d2e5481-3f57-4b47-b553-54ebe54848c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['-1.0' '-1.0' '0.5' '-1.0' '0.0' '1.0' '0.0' '0.5' '0.0' '1.0' '0.0'\n",
            " '-1.0' '0.5' '0.0' '0.0' '0.0' '0.5' '-0.5' '0.0' '1.0' '-1.0' '0.5'\n",
            " '-1.0' '-1.0' '-0.5' '-0.5' '1.0' '-1.0' '0.0' '0.0' '0.0' '-1.0' '0.0'\n",
            " '-0.5' '1.0' '0.5' '1.0' '0.5']\n",
            "['-0.5' '-1.0' '0.5' '-0.5' '0.0' '-1.0' '-0.5' '-1.0' '1.0' '1.0' '0.0'\n",
            " '-0.5' '0.5' '0.0' '0.0' '0.0' '0.5' '-0.5' '0.0' '0.5' '-1.0' '-0.5'\n",
            " '-1.0' '1.0' '1.0' '-0.5' '0.5' '-0.5' '-0.5' '0.5' '-0.5' '-1.0' '0.0'\n",
            " '0.5' '-0.5' '0.5' '1.0' '0.5']\n",
            "F1 Score: 55.96%\n",
            "Balanced Accuracy: 51.51%\n",
            "Precision: 65.83%\n",
            "Recall: 52.63%\n"
          ]
        }
      ],
      "source": [
        "# Convert arrays to string type\n",
        "y_true_str = test['Score'].values.astype(float).astype(str)\n",
        "predicted_scores_str = predicted_scores.astype(float).astype(str)\n",
        "\n",
        "print(y_true_str)\n",
        "print(predicted_scores_str)\n",
        "\n",
        "# Compute F1 score and Balanced Accuracy\n",
        "f1 = f1_score(y_true_str, predicted_scores_str, average='weighted')\n",
        "balanced_accuracy = balanced_accuracy_score(y_true_str, predicted_scores_str)\n",
        "\n",
        "# Compute Precision and Recall\n",
        "precision = precision_score(y_true_str, predicted_scores_str, average='weighted')\n",
        "recall = recall_score(y_true_str, predicted_scores_str, average='weighted')\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "confusion_mat = confusion_matrix(y_true_str, predicted_scores_str)\n",
        "\n",
        "# Print the scores\n",
        "print(f'F1 Score: {f1*100:.2f}%')\n",
        "print(f'Balanced Accuracy: {balanced_accuracy*100:.2f}%')\n",
        "print(f'Precision: {precision*100:.2f}%')\n",
        "print(f'Recall: {recall*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLISU6MRFQS2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 0.0    57\n",
            "-1.0    42\n",
            " 0.5    36\n",
            " 1.0    32\n",
            "-0.5    21\n",
            "Name: Score, dtype: int64\n",
            " 0.0    45\n",
            "-1.0    33\n",
            " 0.5    29\n",
            " 1.0    26\n",
            "-0.5    17\n",
            "Name: Score, dtype: int64\n",
            " 0.0    12\n",
            "-1.0     9\n",
            " 0.5     7\n",
            " 1.0     6\n",
            "-0.5     4\n",
            "Name: Score, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# leave out 20% of the data for testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# only keep sentence and score columns\n",
        "\n",
        "labelled_sentences.head()\n",
        "\n",
        "# remove row if score contains 'Remove'\n",
        "labelled_sentences = labelled_sentences[labelled_sentences['Score'] != 'Remove']\n",
        "labelled_sentences.shape\n",
        "\n",
        "class_counts = labelled_sentences['Score'].value_counts()\n",
        "print(class_counts)\n",
        "\n",
        "labelled_sentences = labelled_sentences[['Sentence', 'Score']]\n",
        "\n",
        "train, test = train_test_split(labelled_sentences, test_size=0.2, random_state=23, stratify=labelled_sentences['Score'])\n",
        "\n",
        "class_counts_train = train['Score'].value_counts()\n",
        "class_counts_test = test['Score'].value_counts()\n",
        "# Print the counts\n",
        "print(class_counts_train)\n",
        "print(class_counts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In addition, the Committee intends to purchase $600 billion of longer-term Treasury securities by the end of the second quarter of 2011, a pace of about $75 billion per month.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['Sentence'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "    new_df = pd.DataFrame(columns=['prompt', 'completion'])\n",
        "    new_df['prompt'] = df['Sentence'].apply(lambda x: 'Given the following sentence, analyze the sentiment and assign a score. The sentiment score should range from 0 to 4, where 0 indicates extremely negative sentiment, 1 indicates negative sentiment, 2 indicates neutral sentiment, 3 indicates positive sentiment, and 4 indicates extremely positive sentiment. ' + x + ' ->')\n",
        "    score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "    # map the scores to labels\n",
        "    new_df['completion'] = df['Score'].map(score_mapping)\n",
        "    # append space to the beginning of the completion\n",
        "    new_df['completion'] = new_df['completion'].apply(lambda x: ' ' + str(x))    \n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_new = preprocess(train)\n",
        "test_new = preprocess(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Given the following sentence, analyze the sentiment and assign a score. The sentiment score should range from 0 to 4, where 0 indicates extremely negative sentiment, 1 indicates negative sentiment, 2 indicates neutral sentiment, 3 indicates positive sentiment, and 4 indicates extremely positive sentiment. In addition, the Committee intends to purchase $600 billion of longer-term Treasury securities by the end of the second quarter of 2011, a pace of about $75 billion per month. ->'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_new['prompt'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' 0'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_new['completion'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                prompt completion\n",
              "189  In the following sentence, calculate a sentime...          4\n",
              "182  In the following sentence, calculate a sentime...          0\n",
              "134  In the following sentence, calculate a sentime...          3\n",
              "49   In the following sentence, calculate a sentime...          0\n",
              "24   In the following sentence, calculate a sentime...          2\n",
              "..                                                 ...        ...\n",
              "30   In the following sentence, calculate a sentime...          0\n",
              "20   In the following sentence, calculate a sentime...          4\n",
              "17   In the following sentence, calculate a sentime...          0\n",
              "153  In the following sentence, calculate a sentime...          3\n",
              "56   In the following sentence, calculate a sentime...          1\n",
              "\n",
              "[150 rows x 2 columns]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In addition, the Committee intends to purchase $600 billion of longer-term Treasury securities by the end of the second quarter of 2011, a pace of about $75 billion per month.'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['Sentence'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_gpt_new, test_gpt_new = train_test_split(train_new, test_size=0.2, random_state=23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save train and test data to jsonl files\n",
        "train_gpt_new.to_json('train_gpt_new.jsonl', orient='records', lines=True)\n",
        "test_gpt_new.to_json('test_gpt_new.jsonl', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 120 prompt-completion pairs\n",
            "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
            "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
            "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
            "- There are 10 duplicated prompt-completion sets. These are rows: [14, 44, 52, 78, 82, 91, 96, 111, 115, 117]\n",
            "- All prompts end with suffix ` ->`\n",
            "- All prompts start with prefix `In the following sentence, calculate a sentiment score of -1 to 1 in steps of 0.5. `. Fine-tuning doesn't require the instruction specifying the task, or a few-shot example scenario. Most of the time you should only add the input data into the prompt, and the desired output into the completion\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Remove 10 duplicate rows [Y/n]: Y\n",
            "- [Recommended] Remove prefix `In the following sentence, calculate a sentiment score of -1 to 1 in steps of 0.5. ` from all prompts [Y/n]: Y\n",
            "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified files to `train_gpt_prepared_train.jsonl` and `train_gpt_prepared_valid.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"train_gpt_prepared_train.jsonl\" -v \"train_gpt_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 5\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt.\n",
            "Once your model starts training, it'll approximately take 4.97 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Owner\\anaconda3\\envs\\nlp_project_3.8\\lib\\site-packages\\openai\\validators.py:283: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x[\"prompt\"] = x[\"prompt\"].str[len(prefix) :]\n"
          ]
        }
      ],
      "source": [
        "# !openai tools fine_tunes.prepare_data -f train_gpt.jsonl -q\n",
        "# # run this cell if you need the openai to prepare the data for you, the output will be in 2 files, train and validation\n",
        "# # example of the output and below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file from train_gpt_new.jsonl: file-TgGcFbOri3f2Z66HlrkyXqQI\n",
            "Uploaded file from test_gpt_new.jsonl: file-5iwB9IESirfngT0UHvMtnI2K\n",
            "Created fine-tune: ft-GpQfGFn85D5qtSgrVtstBIEr\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-06-05 21:00:30] Created fine-tune: ft-GpQfGFn85D5qtSgrVtstBIEr\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-GpQfGFn85D5qtSgrVtstBIEr\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Upload progress:   0%|          | 0.00/64.2k [00:00<?, ?it/s]\n",
            "Upload progress: 100%|██████████| 64.2k/64.2k [00:00<00:00, 64.6Mit/s]\n",
            "\n",
            "Upload progress:   0%|          | 0.00/17.4k [00:00<?, ?it/s]\n",
            "Upload progress: 100%|██████████| 17.4k/17.4k [00:00<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# create fine tunes model\n",
        "!openai api fine_tunes.create -t \"train_gpt_new.jsonl\" -v \"test_gpt_new.jsonl\" --compute_classification_metrics --classification_n_classes 5 -m ada --n_epochs 12 --suffix \"nlp_prod_ada\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-06-05 21:00:30] Created fine-tune: ft-GpQfGFn85D5qtSgrVtstBIEr\n",
            "[2023-06-05 21:02:27] Fine-tune costs $0.05\n",
            "[2023-06-05 21:02:27] Fine-tune enqueued. Queue number: 6\n",
            "[2023-06-05 21:05:23] Fine-tune is in the queue. Queue number: 5\n",
            "[2023-06-05 21:07:49] Fine-tune is in the queue. Queue number: 4\n",
            "[2023-06-05 21:08:57] Fine-tune is in the queue. Queue number: 3\n",
            "[2023-06-05 21:11:38] Fine-tune is in the queue. Queue number: 2\n",
            "[2023-06-05 21:12:32] Fine-tune is in the queue. Queue number: 1\n",
            "[2023-06-05 21:12:34] Fine-tune is in the queue. Queue number: 0\n",
            "[2023-06-05 21:17:31] Fine-tune started\n",
            "[2023-06-05 21:18:05] Completed epoch 1/12\n",
            "[2023-06-05 21:18:26] Completed epoch 2/12\n",
            "[2023-06-05 21:18:45] Completed epoch 3/12\n",
            "[2023-06-05 21:19:04] Completed epoch 4/12\n",
            "[2023-06-05 21:19:22] Completed epoch 5/12\n",
            "[2023-06-05 21:19:41] Completed epoch 6/12\n",
            "[2023-06-05 21:20:01] Completed epoch 7/12\n",
            "[2023-06-05 21:20:19] Completed epoch 8/12\n",
            "[2023-06-05 21:20:38] Completed epoch 9/12\n",
            "[2023-06-05 21:20:57] Completed epoch 10/12\n",
            "[2023-06-05 21:21:16] Completed epoch 11/12\n",
            "[2023-06-05 21:21:35] Completed epoch 12/12\n",
            "[2023-06-05 21:21:55] Uploaded model: ada:ft-personal:nlp-prod-ada-2023-06-05-13-21-55\n",
            "[2023-06-05 21:21:56] Uploaded result file: file-iUqzfZPAK3OsW3peAcm3ZdDq\n",
            "[2023-06-05 21:21:56] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal:nlp-prod-ada-2023-06-05-13-21-55 -p <YOUR_PROMPT>\n"
          ]
        }
      ],
      "source": [
        "!openai api fine_tunes.follow -i ft-GpQfGFn85D5qtSgrVtstBIEr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save results to csv\n",
        "!openai api fine_tunes.results -i ft-GpQfGFn85D5qtSgrVtstBIEr > result.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>step</th>\n",
              "      <th>elapsed_tokens</th>\n",
              "      <th>elapsed_examples</th>\n",
              "      <th>training_loss</th>\n",
              "      <th>training_sequence_accuracy</th>\n",
              "      <th>training_token_accuracy</th>\n",
              "      <th>validation_loss</th>\n",
              "      <th>validation_sequence_accuracy</th>\n",
              "      <th>validation_token_accuracy</th>\n",
              "      <th>classification/accuracy</th>\n",
              "      <th>classification/weighted_f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>122</td>\n",
              "      <td>11354</td>\n",
              "      <td>122</td>\n",
              "      <td>0.030539</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>242</td>\n",
              "      <td>22554</td>\n",
              "      <td>242</td>\n",
              "      <td>0.034937</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.240981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>363</td>\n",
              "      <td>33843</td>\n",
              "      <td>363</td>\n",
              "      <td>0.020344</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.314444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>483</td>\n",
              "      <td>45091</td>\n",
              "      <td>483</td>\n",
              "      <td>0.023194</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.280769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>604</td>\n",
              "      <td>56380</td>\n",
              "      <td>604</td>\n",
              "      <td>0.001493</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.465201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>724</th>\n",
              "      <td>725</td>\n",
              "      <td>67637</td>\n",
              "      <td>725</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.587908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845</th>\n",
              "      <td>846</td>\n",
              "      <td>78918</td>\n",
              "      <td>846</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.433333</td>\n",
              "      <td>0.448052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>965</th>\n",
              "      <td>966</td>\n",
              "      <td>90158</td>\n",
              "      <td>966</td>\n",
              "      <td>0.004032</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.488203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>1086</td>\n",
              "      <td>101438</td>\n",
              "      <td>1086</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.488203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1207</th>\n",
              "      <td>1208</td>\n",
              "      <td>112752</td>\n",
              "      <td>1208</td>\n",
              "      <td>0.003920</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.494949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1326</th>\n",
              "      <td>1327</td>\n",
              "      <td>123999</td>\n",
              "      <td>1327</td>\n",
              "      <td>0.009102</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.488203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1440</th>\n",
              "      <td>1441</td>\n",
              "      <td>134569</td>\n",
              "      <td>1441</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.026243</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.488203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      step  elapsed_tokens  elapsed_examples  training_loss  \\\n",
              "121    122           11354               122       0.030539   \n",
              "241    242           22554               242       0.034937   \n",
              "362    363           33843               363       0.020344   \n",
              "482    483           45091               483       0.023194   \n",
              "603    604           56380               604       0.001493   \n",
              "724    725           67637               725       0.002268   \n",
              "845    846           78918               846       0.001686   \n",
              "965    966           90158               966       0.004032   \n",
              "1085  1086          101438              1086       0.000728   \n",
              "1207  1208          112752              1208       0.003920   \n",
              "1326  1327          123999              1327       0.009102   \n",
              "1440  1441          134569              1441       0.002858   \n",
              "\n",
              "      training_sequence_accuracy  training_token_accuracy  validation_loss  \\\n",
              "121                          0.0                      0.0              NaN   \n",
              "241                          0.0                      0.0              NaN   \n",
              "362                          1.0                      1.0              NaN   \n",
              "482                          0.0                      0.0              NaN   \n",
              "603                          1.0                      1.0              NaN   \n",
              "724                          1.0                      1.0              NaN   \n",
              "845                          1.0                      1.0              NaN   \n",
              "965                          1.0                      1.0              NaN   \n",
              "1085                         1.0                      1.0              NaN   \n",
              "1207                         1.0                      1.0              NaN   \n",
              "1326                         1.0                      1.0              NaN   \n",
              "1440                         1.0                      1.0         0.026243   \n",
              "\n",
              "      validation_sequence_accuracy  validation_token_accuracy  \\\n",
              "121                            NaN                        NaN   \n",
              "241                            NaN                        NaN   \n",
              "362                            NaN                        NaN   \n",
              "482                            NaN                        NaN   \n",
              "603                            NaN                        NaN   \n",
              "724                            NaN                        NaN   \n",
              "845                            NaN                        NaN   \n",
              "965                            NaN                        NaN   \n",
              "1085                           NaN                        NaN   \n",
              "1207                           NaN                        NaN   \n",
              "1326                           NaN                        NaN   \n",
              "1440                           0.0                        0.0   \n",
              "\n",
              "      classification/accuracy  classification/weighted_f1_score  \n",
              "121                  0.200000                          0.066667  \n",
              "241                  0.300000                          0.240981  \n",
              "362                  0.366667                          0.314444  \n",
              "482                  0.366667                          0.280769  \n",
              "603                  0.500000                          0.465201  \n",
              "724                  0.600000                          0.587908  \n",
              "845                  0.433333                          0.448052  \n",
              "965                  0.500000                          0.488203  \n",
              "1085                 0.500000                          0.488203  \n",
              "1207                 0.500000                          0.494949  \n",
              "1326                 0.500000                          0.488203  \n",
              "1440                 0.500000                          0.488203  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = pd.read_csv('result.csv')\n",
        "results[results['classification/accuracy'].notnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot: >"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAGdCAYAAAD5ZcJyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFsUlEQVR4nO3deXxU5cH28d9km4SQDEsgCSSEsIUlYQtb2HEJoigIrRRrtK0bCj4ij1pQrIitwVatxQKKTytFLdIWUKpYCcpqUBQSAdnCGggJgQCZsGSbOe8f0bwNmwwmObNc389n/uDMfU6uw5HM5T0z57YYhmEgIiIi4uX8zA4gIiIiUh9UekRERMQnqPSIiIiIT1DpEREREZ+g0iMiIiI+QaVHREREfIJKj4iIiPgElR4RERHxCQFmB6gtTqeTo0ePEhYWhsViMTuOiIiIXAXDMCgpKaFFixb4+dXtXIzXlJ6jR48SGxtrdgwRERG5BocPHyYmJqZOf4bXlJ6wsDCg6i8tPDzc5DQiIiJyNex2O7GxsdWv43XJa0rP929phYeHq/SIiIh4mPr4aIo+yCwiIiI+QaVHREREfIJKj4iIiPgElR4RERHxCSo9IiIi4hNUekRERMQnqPSIiIiIT1DpEREREZ+g0iMiIiI+QaVHREREfIJKj4iIiPgElR4RERHxCV6z4KiIiNQNwzBYvbuQz/cW4TQMs+OIiX41IJ7YJg3MjnHNVHpEROSyMvee4A8rd5OVe9rsKOIGbu3WQqVHRES8y5bcU7z0yW4y9xUBEBzox5ieMTRuEGhyMjFTZHiw2RF+FJUeERGptuOonVcydrNqZyEAgf4Wft43joeHtaV5mGe/4Imo9IiICPuPn+GVjD18uDUfAD8L/CQ5hv+5vj0xjT337QyR/6bSIyLiw46cOsfsT3NYsiUPh7PqQ8oju0bz2I0daNusocnpRGqXSo+IiA8qLCll7up9/P3LXModTgCu79icKakd6NLCZnI6kbqh0iMi4kNOnyvnjXX7WfD5Qc5XOABIadOUx4cnkBzX2OR0InVLpUdExAecKavkrxsO8Oa6/ZSUVQLQPbYRTwxPYEC7CJPTidQPlR4RES9WWuHgnS8OMXfNPk6eLQegY1QYj6cmcH2n5lgsFpMTitQflR4RES9U4XDyj68P89qneymwlwIQHxHKYzd2YGRSNH5+Kjvie1R6RES8iMNpsPybPP6YkUPuyXMAtLAF8+gN7RnbM4YAfy25KL5LpUdExAsYhsEn3xbw8so95BSeASCiYRATh7Xjzr6tsAb4m5xQxHwqPSIiHswwDNblnODllbvZeqQYgPDgAB4c0pZfDmhNgyD9mhf53jXNc86dO5f4+HiCg4NJTk5m/fr1lx27Zs0aLBbLRY9du3ZVj1mwYMElx5SWll5LPBERn7DpwEnGvfEF9/x1E1uPFNMgyJ9Jw9qx/tfXMXFYOxUekQu4/C9i8eLFTJ48mblz5zJgwADeeOMNRowYwY4dO2jVqtVl99u9ezfh4eHVf27WrFmN58PDw9m9e3eNbcHBWudFRORC244U89LK3azdcxyAoAA/0vrF8dDQtkQ0tJqcTsR9uVx6XnnlFe69917uu+8+AF599VU++eQT5s2bR3p6+mX3a968OY0aNbrs8xaLhaioKFfjiIj4jJxjJbySsYePtxcA4O9n4Y5esfzP9e2ItoWYnE7E/blUesrLy9m8eTNTp06tsT01NZXMzMwr7tujRw9KS0vp3Lkz06dPZ9iwYTWeP3PmDHFxcTgcDrp3787zzz9Pjx49Lnu8srIyysrKqv9st9tdORUREY+RW3SOVz/dw/tZeTgNsFhgVLcWTL6hA60jQs2OJ+IxXCo9J06cwOFwEBkZWWN7ZGQkBQUFl9wnOjqa+fPnk5ycTFlZGW+//TbXX389a9asYfDgwQB07NiRBQsWkJSUhN1u509/+hMDBgzgm2++oX379pc8bnp6Os8995wr8UVEPEpBcSmvfZbD4q8OU/ndYqDDu0Qy5cYEEqLCTE4n4nkshmEYVzv46NGjtGzZkszMTFJSUqq3/+53v+Ptt9+u8eHkK7n11luxWCwsX778ks87nU569uzJ4MGDmT179iXHXGqmJzY2luLi4hqfHRIR8TQnz5Yzb81eFm48RFll1WKgg9pH8HhqAt1iG5kbTqSW2e12bDZbvbx+uzTTExERgb+//0WzOoWFhRfN/lxJv379eOeddy77vJ+fH7179yYnJ+eyY6xWK1arPrAnIt7DXlrB/60/wF/W7+dsedVioL3iGvP48AT6tWlqcjoRz+dS6QkKCiI5OZmMjAxuv/326u0ZGRmMGjXqqo+TlZVFdHT0ZZ83DIPs7GySkpJciSci4pHOlzv428aDvL52H6fPVQDQpUU4jw9PYGiHZlofS6SWuPztrSlTppCWlkavXr1ISUlh/vz55ObmMmHCBACmTZtGXl4eCxcuBKq+3dW6dWu6dOlCeXk577zzDkuWLGHJkiXVx3zuuefo168f7du3x263M3v2bLKzs5kzZ04tnaaIiPspq3Tw3qbD/Hn1Xo6XVL1d37ZZKP+bmsBNXaK0PpZILXO59IwbN46ioiJmzpxJfn4+iYmJrFixgri4OADy8/PJzc2tHl9eXs7jjz9OXl4eISEhdOnShY8++oibb765eszp06d54IEHKCgowGaz0aNHD9atW0efPn1q4RRFRNxLpcPJ0qw8/rQqh7zT5wGIaRzC5Bs6MLp7C62PJVJHXPogszurzw9CiYhcC6fTYMX2fF7J2MP+42cBaB5m5ZHr2jGudyuCAlR2xPe47QeZRUTEdYZhsHp3IX/4ZA8786vuKdaoQSAPDWnL3SmtCQnSYqAi9UGlR0SkDmXuO8FLn+xmS+5pABpaA7hvUDz3DownLDjQ3HAiPkalR0SkDmQfPs1Ln+xmw94TAFgD/PhF/9ZMGNKWxqFBJqcT8U0qPSIitWhnvp2XV+5h1c5jAAT6W/hZ71ZMuq4dkeFaRFnETCo9IiK14MCJs/wxYw//3noUwwA/C4zpGcOj17cntkkDs+OJCCo9IiI/ytHT55n9aQ7/3HwEx3frY92SFM1jN7anXXOtjyXiTlR6RESuwfGSMuas3svfv8yl3FG1PtawhGb8b2oCiS1tJqcTkUtR6RERcUHxuQreWLePtz4/yPmKqvWx+sY34YnhCfRq3cTkdCJyJSo9IiJX4WxZJW99foA31u2npLQSgK4xNp4YnsDAdhFaH0vEA6j0iIj8gNyic/z0jUyO2avWx0qIDGNKagdSO0eq7Ih4EJUeEZErMAyDacu2csxeRkzjEB5PTeDWbi3w12KgIh5HpUdE5Ar+tfkIn+8twhrgxzv39qV1RKjZkUTkGml1OxGRyzheUsZvP9oJwGM3dlDhEfFwKj0iIpcx88MdFJ+voHN0OPcNjDc7joj8SCo9IiKX8OnOY/z7m6P4WeDFsV0J8NevSxFPp3/FIiIXOFNWyfT3twNw36A2JMXoZoMi3kClR0TkAn/4zy7yi0tp1aQBj93Qwew4IlJLVHpERP7L5kOnWPjFIQBeuD2JkCB/kxOJSG1R6RER+U55pZOpS7ZiGDC2ZwwD20eYHUlEapFKj4jId+at2UdO4RmahgYx/ZZOZscRkVqm0iMiAuQcK+HPq3MAePa2LjQODTI5kYjUNpUeEfF5TqfB1KXbqHAYXNexObd2jTY7kojUAZUeEfF5727KZfOhU4QG+fP86EQtIiripVR6RMSn5Ref58WPdwHwxPAEWjYKMTmRiNQVlR4R8VmGYfDM+9s5U1ZJj1aNSEtpbXYkEalDKj0i4rNWbCtg1c5CAv0tvDi2K/5+eltLxJup9IiITyo+V8Gzy78F4KGh7egQGWZyIhGpayo9IuKTXlixkxNnymjbLJSJw9qaHUdE6oFKj4j4nMy9J1j89WEAZo3tijVAS02I+AKVHhHxKaUVDqYt2wbAXf1a0bt1E5MTiUh9UekREZ/y6qocDhWdIyo8mCdv6mh2HBGpRyo9IuIztucV8+b6/QA8PzqR8OBAkxOJSH1S6RERn1DpcDJt6TYcToNbkqK5sXOk2ZFEpJ6p9IiIT3jr84NsyysmPDiAZ2/rbHYcETGBSo+IeL3conO8nLEbgKdv6UTzsGCTE4mIGVR6RMSrGYbBU8u2UVrhJKVNU+7oFWt2JBExiUqPiHi1JVvy2LD3BNYAP14Yk6QV1EV8mEqPiHitE2fK+O1HOwCYfEMH4iNCTU4kImZS6RERrzXz3zs4fa6CTtHh3Dco3uw4ImKyayo9c+fOJT4+nuDgYJKTk1m/fv1lx65ZswaLxXLRY9euXTXGLVmyhM6dO2O1WuncuTPLli27lmgiIgB8tusYy785ip8FXhybRKC//h9PxNe5/Ftg8eLFTJ48maeffpqsrCwGDRrEiBEjyM3NveJ+u3fvJj8/v/rRvn376uc2btzIuHHjSEtL45tvviEtLY077riDL7/80vUzEhGfd6askunLtgNw78B4usY0MjeQiLgFi2EYhis79O3bl549ezJv3rzqbZ06dWL06NGkp6dfNH7NmjUMGzaMU6dO0ahRo0sec9y4cdjtdj7++OPqbTfddBONGzdm0aJFV5XLbrdjs9koLi4mPDzclVMSES8zY/m3LMg8SGyTED6ZPJgGQQFmRxKRy6jP12+XZnrKy8vZvHkzqampNbanpqaSmZl5xX179OhBdHQ0119/PatXr67x3MaNGy865vDhw3/wmCIiF9qSe4q/bTwIwAu3J6nwiEg1l34bnDhxAofDQWRkzdu3R0ZGUlBQcMl9oqOjmT9/PsnJyZSVlfH2229z/fXXs2bNGgYPHgxAQUGBS8cEKCsro6ysrPrPdrvdlVMRES9UXulk6pKtGAaM6dmSQe2bmR1JRNzINf0v0IX3uTAM47L3vkhISCAhIaH6zykpKRw+fJiXXnqpuvS4ekyA9PR0nnvuuWuJLyJe6vW1+9hz7AxNQ4N45hYtNSEiNbn09lZERAT+/v4XzcAUFhZeNFNzJf369SMnJ6f6z1FRUS4fc9q0aRQXF1c/Dh8+fNU/X0S8z97CEv782V4AfnNrZxqHBpmcSETcjUulJygoiOTkZDIyMmpsz8jIoH///ld9nKysLKKjo6v/nJKSctExV65cecVjWq1WwsPDazxExDc5nQZTl2yj3OFkWEIzbuvWwuxIIuKGXH57a8qUKaSlpdGrVy9SUlKYP38+ubm5TJgwAaiagcnLy2PhwoUAvPrqq7Ru3ZouXbpQXl7OO++8w5IlS1iyZEn1MR999FEGDx7Miy++yKhRo/jggw9YtWoVGzZsqKXTFBFv9vdNuXx96BQNgvz57e1aakJELs3l0jNu3DiKioqYOXMm+fn5JCYmsmLFCuLi4gDIz8+vcc+e8vJyHn/8cfLy8ggJCaFLly589NFH3HzzzdVj+vfvz3vvvcf06dN55plnaNu2LYsXL6Zv3761cIoi4s0KikuZ9XHVzU6fGJ5Ay0YhJicSEXfl8n163JXu0yPiewzD4P6Fm1m18xjdYxux5KH++PtplkfEk7jtfXpERNzJx9sLWLXzGAF+Fl4c21WFR0SuSKVHRDxS8bkKnl3+LQAPD21LQlSYyYlExN2p9IiIR0r/eCfHS8po2yyUide1MzuOiHgAlR4R8TiZ+07w3ldV9+aaNbYr1gB/kxOJiCdQ6RERj1Ja4eCppdsA+HnfVvRu3cTkRCLiKVR6RMSj/OnTHA4WnSMy3MqvR3Q0O46IeBCVHhHxGDuO2pm/bj8Az49KJDw40OREIuJJVHpExCNUOpxMXboVh9Pg5qQoUrtEmR1JRDyMSo+IeIQFmQfZeqSYsOAAZtzaxew4IuKBVHpExO3lFp3jpZW7AXj65k40Dw82OZGIeCKVHhFxa4Zh8PT72yitcNKvTRPG9Y41O5KIeCiVHhFxa8uy8lifc4KgAD/Sx3TVCuoics1UekTEbZ04U8bMD3cAMPmG9sRHhJqcSEQ8mUqPiLit5z/cwelzFXSKDuf+QW3MjiMiHk6lR0Tc0updhXyQfRQ/C8wak0Sgv35diciPo98iIuJ2zpZVMv397QD8akA83WIbmRtIRLyCSo+IuJ2XVu4m7/R5YhqHMCW1g9lxRMRLqPSIiFvJyj3FgsyDALxwexINggLMDSQiXkOlR0TcRnmlk6lLtmEYMKZHSwZ3aGZ2JBHxIio9IuI23li7j93HSmgSGsT0kZ3NjiMiXkalR0Tcwt7CM7z22V4Anr21M01Cg0xOJCLeRqVHREzndBo8tXQb5Q4nQxOacVu3FmZHEhEvpNIjIqZb9FUumw6epEGQP78dnailJkSkTqj0iIipCopLmbViFwCPpyYQ07iByYlExFup9IiIaQzD4JkPtlNSVkm32Ebc07+12ZFExIup9IiIaf6zvYCMHccI8LPw4tgk/P30tpaI1B2VHpEfUFJaweP//IaXV+7m9Llys+N4jeLzFfxm+bcAPDS0LR2jwk1OJCLeTrc6FfkBL6zYxb82HwFgQeZBHhjUhl8OjKehVf98foxZH+/keEkZbZqFMnFYO7PjiIgP0EyPyBV8sb+IRZtyAWjbLJSS0kpeztjD4N+v5v/W76e0wmFyQs+0cV8RizYdBiD99iSCA/1NTiQivkClR+QySiscTF2yFYDxfVqR8dgQZo/vQXxEKCfPlvPbj3Yy9A9rePfLQ1Q4nCan9RylFQ6eWrYNgDv7tqJvm6YmJxIRX6HSI3IZr67K4WDROSLDrUy7uSN+fhZu69aCjMcG8+LYJFrYgimwl/L0su1c//JalmUdweE0zI7t9mZ/msOBE2dpHmZl6oiOZscRER+i0iNyCdvzinlz/X4Anh+VSHhwYPVzAf5+jOvditVPDGXGrZ2JaBhE7slzPLb4G0b8aR3/2V6AYaj8XMqOo3bmr/vu73V0zb9XEZG6ptIjcoEKh5Mn/7UVh9PglqRoUrtEXXKcNcCfXwyIZ92Tw3jypgRsIYHsOXaGCe9sZtScz1m757jKz39xOA2mLt1KpdPgpi5RDL/M36uISF1R6RG5wJvr97Mj344tJJAZt3X5wfENggJ4eGg71j05jEeua0eDIH+2Hinmnr9uYtz8L/jq4Ml6SO3+3vr8AFuPFBMWHMBzo37471VEpLap9Ij8l/3Hz/DqqhwAnhnZmWZh1qve1xYSyP+mJrDuyWHcOzCeoAA/Nh04yU9f38gv3trE9rziuort9g6fPMfLK/cA8NTNnYgMDzY5kYj4IpUeke84nQZTl26jvNLJoPYRjO3Z8pqOE9HQyjMjO7P2iaHc2bcVAX4W1uw+zsjXNvDQO5vJOVZSy8ndm2EYPLVsG+crHPSNb8K4XrFmRxIRH6XSI/KdRV/lsunASUIC/Xnh9qQfvdJ3tC2EF25P4tP/HcLtPVpiscDH2wsY/uo6pvwjm9yic7WU3L29n53H+pwTBAX4kT4mCT8tNSEiJlHpEeGClb6HJxDbpPZW+o5rGsofx3XnP48OZniXSJwGLN2Sx3Uvr2H6+9s4Zi+ttZ/lborOlDHz3zsAePT69rRp1tDkRCLiy1R6xOcZhsH097dRUlZJ99hG/KKOVvpOiArjjbRefDBxAIPaR1DpNHjni1wG/341L6zYycmz3reu1/Mf7uDUuQo6RoXxwOA2ZscRER+n0iM+78Ot+azaWUigv4UXx3at85W+u8U24u17+7L4gX70bt2Yskon89ftZ/DvV/NKxh7spRV1+vPry+rdhbyffRQ/C8wa25VAf/26ERFzXdNvoblz5xIfH09wcDDJycmsX7/+qvb7/PPPCQgIoHv37jW2L1iwAIvFctGjtNR7p/3FPZw6W86M71b6fnhoOxKiwurtZ/dt05R/PJjCW7/sTWLLcM6UVTL70xwG/341r6/dx/lyz13X62xZJdOXbQfglwPi6R7byNxAIiJcQ+lZvHgxkydP5umnnyYrK4tBgwYxYsQIcnNzr7hfcXExd999N9dff/0lnw8PDyc/P7/GIzhYX2uVuvX8RzsoOltO++YNeXhY23r/+RaLhWEJzfn3pIHM+3lP2jVvyOlzFcz6eBeD/7Cav2UepKzS88rPyyv3kHf6PDGNQ/jf1A5mxxERAa6h9Lzyyivce++93HfffXTq1IlXX32V2NhY5s2bd8X9HnzwQe68805SUlIu+bzFYiEqKqrGQ6Qurd1znKVb8rB89/aLNcC8lb4tFgsjkqL5ZPJgXv5pN2Iah3C8pIxnl3/LdS+t5R9fH6bSQxY1zco9xVuZBwD43e1JNAgKMDmRiEgVl0pPeXk5mzdvJjU1tcb21NRUMjMzL7vfW2+9xb59+3j22WcvO+bMmTPExcURExPDyJEjycrKumKWsrIy7HZ7jYfI1TpbVslTS6tW+r4npTXJcY1NTlTF38/C2OQYPvvfoTw/OpHmYVbyTp/nyX9tJfXVdXy49ShON17UtLzSybSl2zAMuL1HS4Z0aGZ2JBGRai6VnhMnTuBwOIiMjKyxPTIykoKCgkvuk5OTw9SpU3n33XcJCLj0//F17NiRBQsWsHz5chYtWkRwcDADBgwgJyfnslnS09Ox2WzVj9hY3fBMrt5LK3eTd/o8LRuF8MTwBLPjXCQowI+0fnGse3IYT93ckcYNAtl//CyT/p7FLa9t4LNdx9xyXa/56/axq6CEJqFBPDOys9lxRERquKYPMl940zbDMC55IzeHw8Gdd97Jc889R4cOl39fv1+/ftx1111069aNQYMG8Y9//IMOHTrw2muvXXafadOmUVxcXP04fPjwtZyK+KDNh06xIPMgAC+MSSLU6r5vvwQH+vPA4Lase3IYj93QgYbWAHbm2/nVgq8ZOy+TjfuKzI5Ybd/xM8z+dC8AvxnZmSahQSYnEhGpyaXf9hEREfj7+180q1NYWHjR7A9ASUkJX3/9NVlZWUyaNAkAp9OJYRgEBASwcuVKrrvuuov28/Pzo3fv3lec6bFarVitV78ukghAWaWDqUu2YhgwpqfnvP0SFhzIoze05+6UOF5ft4+/ZR5kS+5pxr/5BQPbRfD48ARTvyHldBpMW7qNcoeTIR2aMap7C9OyiIhcjkszPUFBQSQnJ5ORkVFje0ZGBv37979ofHh4ONu2bSM7O7v6MWHCBBISEsjOzqZv376X/DmGYZCdnU10dLQr8UR+0NzV+8gpPEPT0CCeucXz3n5pHBrEtBGdWPfEMO5OiSPQ38KGvScYPedz7l/4NbsKzPls23tfHa5ewuO3oxN/9BIeIiJ1weV5/SlTppCWlkavXr1ISUlh/vz55ObmMmHCBKDqbae8vDwWLlyIn58fiYmJNfZv3rw5wcHBNbY/99xz9OvXj/bt22O325k9ezbZ2dnMmTPnR56eyP+351gJc9dUvf0y47YuNPbgt1+ahwczc1Qi9w9qw58+zWHpliNk7DjGqp3HuLVrCx67sQPxEaH1kuWYvZT0FTuB2l/CQ0SkNrlcesaNG0dRUREzZ84kPz+fxMREVqxYQVxcHAD5+fk/eM+eC50+fZoHHniAgoICbDYbPXr0YN26dfTp08fVeCKX5HAaPPmvrVQ4DG7o1JyRXb1jFjG2SQNe+mk3Jgxpyx9X7eGjrfks/+YoH23L56fJMfzP9e1p0SikTjP85oPtlJRV0i3GVmdLeIiI1AaL4Y5fAbkGdrsdm81GcXEx4eHhZscRN/PXDQeY+eEOGloDyJgymGhb3RYBs2zPK+aVjD18tqsQgCB/P+7s24qJw9rRLKz2PwP3n+35THhnCwF+Fv79yEA6Revfnoi4pj5fv7UYjni9wyfP8YdPdgMw7eaOXlt4ABJb2vjrL3qz5KEU+rVpQrnDyYLMgwz+/Wp+/59dFJ+rvXW9is9X8JsPqpbwmDCkrQqPiLg9lR7xaoZh8NSybZyvcNAnvgnje7cyO1K9SI5rwqL7+/HOvX3pFmPjfIWDuWv2MfD3n/Hnz3I4W1b5o3/GrI93UVhSRpuIUCZd164WUouI1C2VHvFqS7bksT7nBEEBfswak4RfHa+g7k4sFgsD20fw/sQBzE9LJiEyjJLSSl5auYfBv1/NXzYcoLTi2tb1+mJ/EYs2VX1274UxSQQHmreEh4jI1VLpEa91vKSM5z/cAcDkG9rTpllDkxOZw2KxkNolihWPDuJPP+tO66YNKDpbzvMf7mDYS2tYtCmXChfW9SqtcFQv4TG+Tyv6tWlaV9FFRGqVSo94rRn//pbi8xV0jg7n/kFtzI5jOn8/C6O6tyRjyhBmjUki2hZMfnEp05Zu44ZX1vJ+Vh6Oq1jX68+f7WX/ibM0D7MydUTHekguIlI7VHrEK638toCPtubj72fh9z/pSqC//lP/XqC/Hz/r04rVjw/lNyM7E9EwiENF55i8OJub/7SeT74tuOy6Xjvz7by+dh8AM0clYgsJrM/oIiI/il4JxOvYSyt45oPtANw/qA2JLW0mJ3JPwYH+/GpgPGufGMYTwxMIDw5g97ESHnx7M6PnfM66PcdrlB+H02Dqkq1UOg2Gd4nkpsQoE9OLiLhOpUe8TvqKXRyzlxEfEcrkG9qbHcfthVoDmDisHeufvI6Jw9rSIMifb44Uc/dfN/Gz+V/w9cGTACzIPMg3R4oJswYwc1TiDxxVRMT96OaE4lW+2F/Ez+Z/AcB7D/TTh2yvwfGSMuau2cu7X+RS/t0HnId0aMamAyc5X+HghduTuLOvb3z1X0Tqnm5OKHINSiuqVlAHfavox2gWZuXZW7uw5omhjO8Ti7+fhbV7jlff6+hnvWPNjigick1cXntLxF29uiqHg0XniAy3Mu1mfavox2rRKIT0MV15YHBbZn+aw77jZ/j92K4+da8jEfEuKj3iFbbnFfPm+v0APD8qkfBgfauotsRHhPLHcd3NjiEi8qPp7S3xeBUOJ0/+aysOp8EtSdGkdtG3ikRE5GIqPeLx3ly/nx35dmwhgcy4rYvZcURExE2p9IhH23/8DK+uygHgmZGdaRZmNTmRiIi4K5Ue8VhOp8HUpdsor3QyqH0EY3u2NDuSiIi4MZUe8ViLvspl04GThAT688LtSVgs+laRiIhcnkqPeKSC4lJmrdgFwOPDE4ht0sDkRCIi4u5UesTjGIbB9Pe3UVJWSffYRvyif2uzI4mIiAdQ6RGP8+HWfFbtLCTQ38KLY7vir5vliYjIVVDpEY9y6mw5M5Z/C8DDQ9uREBVmciIREfEUKj3iUZ7/aAdFZ8tp37whDw9ra3YcERHxICo94jHW7jnO0i15WCwwa2xXrAH+ZkcSEREPotIjHuFsWSVPLd0GwD0prUmOa2xyIhER8TQqPeIRXlq5m7zT52nZKIQnhieYHUdERDyQSo+4vc2HTrEg8yAAL4xJItQaYG4gERHxSCo94tbKKh1MXbIVw4AxPVsypEMzsyOJiIiHUukRtzZ39T5yCs/QNDSIZ27pbHYcERHxYCo94rb2HCth7pq9AMy4rQuNQ4NMTiQiIp5MpUfcksNp8OS/tlLhMLihU3NGdo02O5KIiHg4lR5xS3/LPEj24dM0tAbw/OhEraAuIiI/mkqPuJ3DJ8/xh092AzDt5o5E20JMTiQiIt5ApUfcimEYPLVsG+crHPSJb8L43q3MjiQiIl5CpUfcypIteazPOUFQgB+zxiThpxXURUSklqj0iNs4XlLG8x/uAGDyDe1p06yhyYlERMSbqPSI25jx728pPl9B5+hw7h/Uxuw4IiLiZVR6xC2s/LaAj7bm4+9n4fc/6Uqgv/7TFBGR2qVXFjGdvbSCZz7YDsB9g+JJbGkzOZGIiHgjlR4xXfqKXRyzl9G6aQMeu6GD2XFERMRLXVPpmTt3LvHx8QQHB5OcnMz69euvar/PP/+cgIAAunfvftFzS5YsoXPnzlitVjp37syyZcuuJZp4mC/2F7FoUy4As8Z2JTjQ3+REIiLirVwuPYsXL2by5Mk8/fTTZGVlMWjQIEaMGEFubu4V9ysuLubuu+/m+uuvv+i5jRs3Mm7cONLS0vjmm29IS0vjjjvu4Msvv3Q1nniQ0oqqFdQBxvdpRb82TU1OJCIi3sxiGIbhyg59+/alZ8+ezJs3r3pbp06dGD16NOnp6Zfd72c/+xnt27fH39+f999/n+zs7Ornxo0bh91u5+OPP67edtNNN9G4cWMWLVp0Vbnsdjs2m43i4mLCw8NdOSUxyayPd/H62n1EhlvJmDKE8OBAsyOJiEg9q8/Xb5dmesrLy9m8eTOpqak1tqemppKZmXnZ/d566y327dvHs88+e8nnN27ceNExhw8ffsVjlpWVYbfbazzEc2zPK+bN9fsBeH5UogqPiIjUOZdKz4kTJ3A4HERGRtbYHhkZSUFBwSX3ycnJYerUqbz77rsEBARcckxBQYFLxwRIT0/HZrNVP2JjY105FTFRhcPJk//aisNpcEtSNKldosyOJCIiPuCaPsh84YrXhmFcchVsh8PBnXfeyXPPPUeHDlf+Vs7VHvN706ZNo7i4uPpx+PBhF85AzPTm+v3syLdjCwlkxm1dzI4jIiI+4tJTL5cRERGBv7//RTMwhYWFF83UAJSUlPD111+TlZXFpEmTAHA6nRiGQUBAACtXruS6664jKirqqo/5PavVitVqdSW+uIH9x8/w6qocAJ4Z2ZlmYbqGIiJSP1ya6QkKCiI5OZmMjIwa2zMyMujfv/9F48PDw9m2bRvZ2dnVjwkTJpCQkEB2djZ9+/YFICUl5aJjrly58pLHFM/ldBpMXbqN8kong9pHMLZnS7MjiYiID3FppgdgypQppKWl0atXL1JSUpg/fz65ublMmDABqHrbKS8vj4ULF+Ln50diYmKN/Zs3b05wcHCN7Y8++iiDBw/mxRdfZNSoUXzwwQesWrWKDRs2/MjTE3ey6KtcNh04SUigPy/cnnTFty9FRERqm8ulZ9y4cRQVFTFz5kzy8/NJTExkxYoVxMXFAZCfn/+D9+y5UP/+/XnvvfeYPn06zzzzDG3btmXx4sXVM0Hi+fKLzzNrxS4AHh+eQGyTBiYnEhERX+PyfXrcle7T474Mw+D+hV+zamch3WMbseSh/vj7aZZHRETc+D49Itfiw635rNpZSKC/hRfHdlXhERERU6j0SJ06dbacGcu/BeDhoe1IiAozOZGIiPgqlR6pU89/tIOis+W0b96Qh4e1NTuOiIj4MJUeqTNr9xxn6ZY8LJaqFdStAVpBXUREzKPSI3XibFklTy3dBsA9Ka1JjmtsciIREfF1Kj1SJ/7wyW7yTp+nZaMQnhieYHYcERERlR6pfZsPneJvGw8C8MKYJEKtLt8OSkREpNap9EitKqt0MHXJVgwDxvRsyZAOzcyOJCIiAqj0SC2bu3ofOYVnaBoaxDO3dDY7joiISDWVHqk1e46VMHfNXgBm3NaFxqFBJicSERH5/1R6pFY4nAZP/msrFQ6DGzo1Z2TXaLMjiYiI1KDSI7ViQeZBsg+fpqE1gOdHJ2oFdRERcTsqPfKjHT55jpc+2Q3A1BEdibaFmJxIRETkYio98qMYhsFTy7ZxvsJBn/gm3NmnldmRRERELkmlR36UJVvyWJ9zgqAAP2aNScJPK6iLiIibUumRa3a8pIznP9wBwOQb2tOmWUOTE4mIiFyeSo9csxn//pbi8xV0jg7n/kFtzI4jIiJyRSo9ck1WflvAR1vz8fez8PufdCXQX/8piYiIe9MrlbjMXlrBMx9sB+C+QfEktrSZnEhEROSHqfSIy9JX7OKYvYzWTRvw2A0dzI4jIiJyVVR6xCVf7C9i0aZcAGaN7UpwoL/JiURERK6OSo9ctdKKqhXUAcb3aUW/Nk1NTiQiInL1VHrkqr26KoeDReeIDLcy7eaOZscRERFxiUqPXJXtecW8uX4/AM+PSiQ8ONDkRCIiIq5R6ZEfVOFw8uS/tuJwGtySFE1qlyizI4mIiLhMpUd+0Jvr97Mj344tJJAZt3UxO46IiMg1UemRKzp88hyvrsoB4JmRnWkWZjU5kYiIyLVR6ZErmrtmL+WVTlLaNGVsz5ZmxxEREblmKj1yWfnF5/nX5iMATEntgMWiFdRFRMRzqfTIZc1ft58Kh0Gf+Cb0bt3E7DgiIiI/ikqPXFLRmbLqOy9PGtbO5DQiIiI/nkqPXNJfPz9AaYWTrjE2BrWPMDuOiIjIj6bSIxcpPl/BwsxDAEwc1k6f5REREa+g0iMXeXvjQUrKKukQ2ZAbO0WaHUdERKRWqPRIDefKK/nLhgMAPDy0HX5+muURERHvoNIjNfz9y1xOnaugVZMGjOwabXYcERGRWqPSI9XKKh3Vi4o+NLQtAf76z0NERLyHXtWk2pLNeRyzlxEVHswY3X1ZRES8jEqPAFDpcPL62n0APDC4DdYAf5MTiYiI1C6VHgHg31uPknvyHE1Cgxjfp5XZcURERGrdNZWeuXPnEh8fT3BwMMnJyaxfv/6yYzds2MCAAQNo2rQpISEhdOzYkT/+8Y81xixYsACLxXLRo7S09FriiYucToO5q6tmee4dGE9IkGZ5RETE+wS4usPixYuZPHkyc+fOZcCAAbzxxhuMGDGCHTt20KrVxTMEoaGhTJo0ia5duxIaGsqGDRt48MEHCQ0N5YEHHqgeFx4ezu7du2vsGxwcfA2nJK5auaOAnMIzhAUHkJYSZ3YcERGROmExDMNwZYe+ffvSs2dP5s2bV72tU6dOjB49mvT09Ks6xpgxYwgNDeXtt98GqmZ6Jk+ezOnTp12JUoPdbsdms1FcXEx4ePg1H8fXGIbBrX/ewPY8O5OGtePx4QlmRxIRER9Sn6/fLr29VV5ezubNm0lNTa2xPTU1lczMzKs6RlZWFpmZmQwZMqTG9jNnzhAXF0dMTAwjR44kKyvriscpKyvDbrfXeIjr1uWcYHuenZBAf341MN7sOCIiInXGpdJz4sQJHA4HkZE1lyaIjIykoKDgivvGxMRgtVrp1asXEydO5L777qt+rmPHjixYsIDly5ezaNEigoODGTBgADk5OZc9Xnp6OjabrfoRGxvryqnId+Z8theAO/u2oklokMlpRERE6o7Ln+kBLlqA0jCMH1yUcv369Zw5c4YvvviCqVOn0q5dO8aPHw9Av3796NevX/XYAQMG0LNnT1577TVmz559yeNNmzaNKVOmVP/Zbrer+Lho04GTbDp4kiB/P+4f1MbsOCIiInXKpdITERGBv7//RbM6hYWFF83+XCg+vuqtk6SkJI4dO8aMGTOqS8+F/Pz86N279xVneqxWK1ar1ZX4coE/r66a5RmbHEOUTR8aFxER7+bS21tBQUEkJyeTkZFRY3tGRgb9+/e/6uMYhkFZWdkVn8/OziY6Wms/1ZWtR06zbs9x/P0sPDSkrdlxRERE6pzLb29NmTKFtLQ0evXqRUpKCvPnzyc3N5cJEyYAVW875eXlsXDhQgDmzJlDq1at6NixI1B1356XXnqJRx55pPqYzz33HP369aN9+/bY7XZmz55NdnY2c+bMqY1zlEuY890sz23dWtCqaQOT04iIiNQ9l0vPuHHjKCoqYubMmeTn55OYmMiKFSuIi6u6v0t+fj65ubnV451OJ9OmTePAgQMEBATQtm1bZs2axYMPPlg95vTp0zzwwAMUFBRgs9no0aMH69ato0+fPrVwinKhnGMlfPLtMQAeHqpZHhER8Q0u36fHXek+PVfvscXZLMvK46YuUbyelmx2HBER8WFue58e8Xy5RedY/s1RACYOa2dyGhERkfqj0uNj5q3dh8NpMKRDM5JibGbHERERqTcqPT6koLiUJZuPAJrlERER36PS40Pmr9tPucNJn9ZN6BPfxOw4IiIi9Uqlx0cUnSnj75sOATDxOs3yiIiI71Hp8RFvfX6Q0gonSS1tDG4fYXYcERGReqfS4wPspRX8beNBoOqzPD+0TpqIiIg3UunxAW9vPERJaSXtmzcktfOV10gTERHxVio9Xu5ceSV/2XAAgIeHtcXPT7M8IiLim1R6vNyiTYc5ebacVk0acGvXFmbHERERMY1Kjxcrq3Qwf90+ACYMaUuAvy63iIj4Lr0KerGlW/I4Zi8jKjyYscktzY4jIiJiKpUeL1XpcDJvTdUsz/2D22AN8Dc5kYiIiLlUerzUh1vzyT15jiahQYzvE2t2HBEREdOp9Hghp9Ngzuq9APxqQGsaBAWYnEhERMR8Kj1eaOWOY+QUniHMGkBaSmuz44iIiLgFlR4vYxj/f5bn7v5x2EICTU4kIiLiHlR6vMz6nBNsyysmJNCfXw2INzuOiIiI21Dp8TJ//m6WZ3yfVjRtaDU5jYiIiPtQ6fEiXx08yaYDJwn0t/DA4DZmxxEREXErKj1e5M+fVc3y/CQ5hihbsMlpRERE3ItKj5fYdqSYtXuO42epWnJCREREalLp8RLff2Prtm4tiGsaanIaERER96PS4wVyjpXwn28LAHh4WDuT04iIiLgnlR4v8P0aW8O7RNIhMszkNCIiIu5JpcfD5Rad44NvjgIwaVh7k9OIiIi4L5UeD/f6un04nAaDOzQjKcZmdhwRERG3pdLjwQqKS/nX10cAmDhU39gSERG5EpUeD/bm+v2UO5z0bt2Yvm2amh1HRETEran0eKiTZ8v5+5e5AEzUN7ZERER+kEqPh3rr8wOcr3CQ1NLGkA7NzI4jIiLi9lR6PJC9tIIFmQcBmDisLRaLxdxAIiIiHkClxwO9vfEQJaWVtGvekNTOUWbHERER8QgqPR7mfLmDv244AMDDQ9vi56dZHhERkauh0uNhFm3KpehsObFNQritWwuz44iIiHgMlR4PUlbpYP66/UDVSuoB/rp8IiIiV0uvmh5k2ZY8CuylRIZb+UlyjNlxREREPIpKj4eodDiZt7ZqYdH7B7XBGuBvciIRERHPotLjIT7als+honM0bhDInX1bmR1HRETE41xT6Zk7dy7x8fEEBweTnJzM+vXrLzt2w4YNDBgwgKZNmxISEkLHjh354x//eNG4JUuW0LlzZ6xWK507d2bZsmXXEs0rOZ0Gc1bvBeBXA+JpEBRgciIRERHP43LpWbx4MZMnT+bpp58mKyuLQYMGMWLECHJzcy85PjQ0lEmTJrFu3Tp27tzJ9OnTmT59OvPnz68es3HjRsaNG0daWhrffPMNaWlp3HHHHXz55ZfXfmZeJGPnMfYcO0OYNYC7+7c2O46IiIhHshiGYbiyQ9++fenZsyfz5s2r3tapUydGjx5Nenr6VR1jzJgxhIaG8vbbbwMwbtw47HY7H3/8cfWYm266icaNG7No0aKrOqbdbsdms1FcXEx4eLgLZ+TeDMNg1JzP2XqkmIeHtuXJmzqaHUlERKTW1Ofrt0szPeXl5WzevJnU1NQa21NTU8nMzLyqY2RlZZGZmcmQIUOqt23cuPGiYw4fPvyKxywrK8Nut9d4eKP1OSfYeqSY4EA/7h0Yb3YcERERj+VS6Tlx4gQOh4PIyMga2yMjIykoKLjivjExMVitVnr16sXEiRO57777qp8rKChw+Zjp6enYbLbqR2xsrCun4jG+/yzP+D6taNrQanIaERERz3VNH2S+cIFLwzB+cNHL9evX8/XXX/P666/z6quvXvS2lavHnDZtGsXFxdWPw4cPu3gW7u/rgyf58sBJAv0tPDC4jdlxREREPJpLXwOKiIjA39//ohmYwsLCi2ZqLhQfX/XWTFJSEseOHWPGjBmMHz8egKioKJePabVasVq9e+bjz9/N8vwkOYZoW4jJaURERDybSzM9QUFBJCcnk5GRUWN7RkYG/fv3v+rjGIZBWVlZ9Z9TUlIuOubKlStdOqa32Z5XzJrdx/GzwIOD25odR0RExOO5fMOXKVOmkJaWRq9evUhJSWH+/Pnk5uYyYcIEoOptp7y8PBYuXAjAnDlzaNWqFR07Vn3raMOGDbz00ks88sgj1cd89NFHGTx4MC+++CKjRo3igw8+YNWqVWzYsKE2ztEjff9Znlu7taB1RKjJaURERDyfy6Vn3LhxFBUVMXPmTPLz80lMTGTFihXExcUBkJ+fX+OePU6nk2nTpnHgwAECAgJo27Yts2bN4sEHH6we079/f9577z2mT5/OM888Q9u2bVm8eDF9+/athVP0PHsLS/jPt1Vv9z08tJ3JaURERLyDy/fpcVfedJ+eKf/IZumWPFI7RzL/7l5mxxEREakzbnufHql7h0+e44PsowBMuk6zPCIiIrVFpcfNvL52Hw6nwaD2EXSNaWR2HBEREa+h0uNGjtlL+efXRwCYOEyzPCIiIrVJpceNvLluP+UOJ73iGtM3vonZcURERLyKSo+bOHm2nHe/rPrW28Tr2v3gHa5FRETENSo9bmLB5wc4X+EgsWU4Qzs0MzuOiIiI11HpcQMlpRUsyDwIwMShmuURERGpCyo9buDtLw5hL62kbbNQhneJMjuOiIiIV1LpMdn5cgd/WX8AqLr7sp+fZnlERETqgkqPyd77Kpeis+XENA7htu4tzI4jIiLitVR6TFRe6WT+uv0ATBjSlkB/XQ4REZG6oldZEy3dcoT84lKah1n5SXKM2XFERES8mkqPSSodTuat3QfAA4PbEBzob3IiERER76bSY5KPtuVzqOgcjRsEMr5PK7PjiIiIeD2VHhM4nQZzV1fN8vxyQDyh1gCTE4mIiHg/lR4TrNp5jN3HSmhoDeCelNZmxxEREfEJKj31zDAM5qzeC0BaShy2BoEmJxIREfENKj31bMPeE3xzpJjgQD/uHRhvdhwRERGfodJTz76f5flZ71ZENLSanEZERMR3qPTUo82HTvLF/pME+lt4cEgbs+OIiIj4FJWeevTnz6pmecb2jCHaFmJyGhEREd+i0lNPtucVs3r3cfwsVUtOiIiISP1S6aknc9dUzfKM7NqC1hGhJqcRERHxPSo99WBvYQkfby8AYOKwdianERER8U0qPfVg3pr9GAbc2DmShKgws+OIiIj4JJWeOnb45Dnez84DYJJmeUREREyj0lPH3li3D4fTYFD7CLrFNjI7joiIiM9S6alDhfZS/vH1EQAeHqpZHhERETOp9NShN9fvp7zSSXJcY/q1aWJ2HBEREZ+m0lNHTp0t590vc4Gqz/JYLBaTE4mIiPg2lZ468tbnBzhX7qBLi3CGJjQzO46IiIjPU+mpAyWlFSzIPAhU3ZdHszwiIiLmU+mpA+98kYu9tJK2zUK5qUuU2XFEREQElZ5aV1rh4C8b9gPw0NB2+PlplkdERMQdqPTUsvc25XLiTDkxjUMY1b2F2XFERETkOyo9tai80skb66pmeR4c0pZAf/31ioiIuAu9KteiZVlHyC8upXmYlZ8mx5gdR0RERP6LSk8tcTgN5q3ZB8D9g9oQHOhvciIRERH5byo9teSjbfkcLDpHowaB3Nm3ldlxRERE5AIqPbXA6TSYu3ovAL/sH0+oNcDkRCIiInKhayo9c+fOJT4+nuDgYJKTk1m/fv1lxy5dupQbb7yRZs2aER4eTkpKCp988kmNMQsWLMBisVz0KC0tvZZ49e7TXYXsKiihoTWAX/RvbXYcERERuQSXS8/ixYuZPHkyTz/9NFlZWQwaNIgRI0aQm5t7yfHr1q3jxhtvZMWKFWzevJlhw4Zx6623kpWVVWNceHg4+fn5NR7BwcHXdlb1yDAM/vzdLM9d/eKwNQg0OZGIiIhcisUwDMOVHfr27UvPnj2ZN29e9bZOnToxevRo0tPTr+oYXbp0Ydy4cfzmN78BqmZ6Jk+ezOnTp12JUoPdbsdms1FcXEx4ePg1H8dVG3JOcNdfvsQa4MeGX19HszBrvf1sERERT1efr98uzfSUl5ezefNmUlNTa2xPTU0lMzPzqo7hdDopKSmhSZMmNbafOXOGuLg4YmJiGDly5EUzQRcqKyvDbrfXeJhhznezPOP7tFLhERERcWMulZ4TJ07gcDiIjIyssT0yMpKCgoKrOsbLL7/M2bNnueOOO6q3dezYkQULFrB8+XIWLVpEcHAwAwYMICcn57LHSU9Px2azVT9iY2NdOZVasfnQKTbuLyLQ38IDg9vU+88XERGRq3dNH2S+cNVwwzCuaiXxRYsWMWPGDBYvXkzz5s2rt/fr14+77rqLbt26MWjQIP7xj3/QoUMHXnvttcsea9q0aRQXF1c/Dh8+fC2n8qN8P8szpkcMLRqF1PvPFxERkavn0nerIyIi8Pf3v2hWp7Cw8KLZnwstXryYe++9l3/+85/ccMMNVxzr5+dH7969rzjTY7VasVrNezvp26PFfLarED8LTBja1rQcIiIicnVcmukJCgoiOTmZjIyMGtszMjLo37//ZfdbtGgRv/jFL/j73//OLbfc8oM/xzAMsrOziY6OdiVevZq7uuruy7d0bUF8RKjJaUREROSHuHwXvSlTppCWlkavXr1ISUlh/vz55ObmMmHCBKDqbae8vDwWLlwIVBWeu+++mz/96U/069evepYoJCQEm80GwHPPPUe/fv1o3749drud2bNnk52dzZw5c2rrPGvV3sIzrNieD8DEYZrlERER8QQul55x48ZRVFTEzJkzyc/PJzExkRUrVhAXFwdAfn5+jXv2vPHGG1RWVjJx4kQmTpxYvf2ee+5hwYIFAJw+fZoHHniAgoICbDYbPXr0YN26dfTp0+dHnl7dmLdmH4YBN3SKpGNU/X09XkRERK6dy/fpcVf19T3/wyfPMfSlNTicBu9PHED32EZ19rNERES8ndvep0dg/rr9OJwGA9tFqPCIiIh4EJUeFxTaS1n8ddVX4x/WZ3lEREQ8ikqPC/5vwwHKK530bNWIlDZNzY4jIiIiLlDpuUqnzpbzzheHAJh0XburuhmjiIiIuA+Vnqv0VuZBzpU76BwdzrCE5j+8g4iIiLgVlZ6rcKaskgWfHwBg4jDN8oiIiHgilZ6r8M4Xh7CXVtKmWSg3JUaZHUdERESugUrPDyitcPB/66tmeR4a0hZ/P83yiIiIeCKVnh+w+KvDnDhTRstGIYzu0dLsOCIiInKNVHquoMLh5I21VQuLThjShkB//XWJiIh4Kr2KX0GAn4U//LQbNydF8dNesWbHERERkR/B5QVHfYnFYmFAuwgGtIswO4qIiIj8SJrpEREREZ+g0iMiIiI+QaVHREREfIJKj4iIiPgElR4RERHxCSo9IiIi4hNUekRERMQnqPSIiIiIT1DpEREREZ+g0iMiIiI+QaVHREREfIJKj4iIiPgElR4RERHxCV6zyrphGADY7XaTk4iIiMjV+v51+/vX8brkNaWnpKQEgNjYWJOTiIiIiKtKSkqw2Wx1+jMsRn1Uq3rgdDo5evQoYWFhWCwWs+OYwm63Exsby+HDhwkPDzc7jlyBrpVn0fXyHLpWnuX767Vjxw4SEhLw86vbT914zUyPn58fMTExZsdwC+Hh4frH7iF0rTyLrpfn0LXyLC1btqzzwgP6ILOIiIj4CJUeERER8QkqPV7EarXy7LPPYrVazY4iP0DXyrPoenkOXSvPUt/Xy2s+yCwiIiJyJZrpEREREZ+g0iMiIiI+QaVHREREfIJKj4iIiPgElR43lp6eTu/evQkLC6N58+aMHj2a3bt31xhjGAYzZsygRYsWhISEMHToUL799tsaY8rKynjkkUeIiIggNDSU2267jSNHjtTnqfik9PR0LBYLkydPrt6m6+U+8vLyuOuuu2jatCkNGjSge/fubN68ufp5XSv3UVlZyfTp04mPjyckJIQ2bdowc+ZMnE5n9RhdL3OsW7eOW2+9lRYtWmCxWHj//fdrPF9b1+XUqVOkpaVhs9mw2WykpaVx+vRp1wMb4raGDx9uvPXWW8b27duN7Oxs45ZbbjFatWplnDlzpnrMrFmzjLCwMGPJkiXGtm3bjHHjxhnR0dGG3W6vHjNhwgSjZcuWRkZGhrFlyxZj2LBhRrdu3YzKykozTssnbNq0yWjdurXRtWtX49FHH63eruvlHk6ePGnExcUZv/jFL4wvv/zSOHDggLFq1Spj79691WN0rdzHb3/7W6Np06bGhx9+aBw4cMD45z//aTRs2NB49dVXq8foepljxYoVxtNPP20sWbLEAIxly5bVeL62rstNN91kJCYmGpmZmUZmZqaRmJhojBw50uW8Kj0epLCw0ACMtWvXGoZhGE6n04iKijJmzZpVPaa0tNSw2WzG66+/bhiGYZw+fdoIDAw03nvvveoxeXl5hp+fn/Gf//ynfk/AR5SUlBjt27c3MjIyjCFDhlSXHl0v9/HrX//aGDhw4GWf17VyL7fccovxq1/9qsa2MWPGGHfddZdhGLpe7uLC0lNb12XHjh0GYHzxxRfVYzZu3GgAxq5du1zKqLe3PEhxcTEATZo0AeDAgQMUFBSQmppaPcZqtTJkyBAyMzMB2Lx5MxUVFTXGtGjRgsTExOoxUrsmTpzILbfcwg033FBju66X+1i+fDm9evXipz/9Kc2bN6dHjx68+eab1c/rWrmXgQMH8umnn7Jnzx4AvvnmGzZs2MDNN98M6Hq5q9q6Lhs3bsRms9G3b9/qMf369cNms7l87bxmwVFvZxgGU6ZMYeDAgSQmJgJQUFAAQGRkZI2xkZGRHDp0qHpMUFAQjRs3vmjM9/tL7XnvvffYsmULX3311UXP6Xq5j/379zNv3jymTJnCU089xaZNm/if//kfrFYrd999t66Vm/n1r39NcXExHTt2xN/fH4fDwe9+9zvGjx8P6N+Wu6qt61JQUEDz5s0vOn7z5s1dvnYqPR5i0qRJbN26lQ0bNlz0nMViqfFnwzAu2nahqxkjrjl8+DCPPvooK1euJDg4+LLjdL3M53Q66dWrFy+88AIAPXr04Ntvv2XevHncfffd1eN0rdzD4sWLeeedd/j73/9Oly5dyM7OZvLkybRo0YJ77rmnepyul3uqjetyqfHXcu309pYHeOSRR1i+fDmrV68mJiamentUVBTARU23sLCwullHRUVRXl7OqVOnLjtGasfmzZspLCwkOTmZgIAAAgICWLt2LbNnzyYgIKD671vXy3zR0dF07ty5xrZOnTqRm5sL6N+Wu3niiSeYOnUqP/vZz0hKSiItLY3HHnuM9PR0QNfLXdXWdYmKiuLYsWMXHf/48eMuXzuVHjdmGAaTJk1i6dKlfPbZZ8THx9d4Pj4+nqioKDIyMqq3lZeXs3btWvr37w9AcnIygYGBNcbk5+ezffv26jFSO66//nq2bdtGdnZ29aNXr178/Oc/Jzs7mzZt2uh6uYkBAwZcdPuHPXv2EBcXB+jflrs5d+4cfn41X678/f2rv7Ku6+Weauu6pKSkUFxczKZNm6rHfPnllxQXF7t+7Vz62LPUq4ceesiw2WzGmjVrjPz8/OrHuXPnqsfMmjXLsNlsxtKlS41t27YZ48ePv+TXAWNiYoxVq1YZW7ZsMa677jp9TbOe/Pe3twxD18tdbNq0yQgICDB+97vfGTk5Oca7775rNGjQwHjnnXeqx+hauY977rnHaNmyZfVX1pcuXWpEREQYTz75ZPUYXS9zlJSUGFlZWUZWVpYBGK+88oqRlZVlHDp0yDCM2rsuN910k9G1a1dj48aNxsaNG42kpCR9Zd3bAJd8vPXWW9VjnE6n8eyzzxpRUVGG1Wo1Bg8ebGzbtq3Gcc6fP29MmjTJaNKkiRESEmKMHDnSyM3Nreez8U0Xlh5dL/fx73//20hMTDSsVqvRsWNHY/78+TWe17VyH3a73Xj00UeNVq1aGcHBwUabNm2Mp59+2igrK6seo+tljtWrV1/ydeqee+4xDKP2rktRUZHx85//3AgLCzPCwsKMn//858apU6dczmsxDMNwccZKRERExOPoMz0iIiLiE1R6RERExCeo9IiIiIhPUOkRERERn6DSIyIiIj5BpUdERER8gkqPiIiI+ASVHhEREfEJKj0iIiLiE1R6RERExCeo9IiIiIhPUOkRERERn/D/AGSSl8QtJUdIAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "results[results['classification/accuracy'].notnull()]['classification/accuracy'].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TRAIN AUGMENTED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 0.0    108\n",
            "-1.0     79\n",
            " 0.5     70\n",
            " 1.0     62\n",
            "-0.5     41\n",
            "Name: Score, dtype: int64\n",
            " 0.0    27\n",
            "-1.0    20\n",
            " 0.5    17\n",
            " 1.0    16\n",
            "-0.5    10\n",
            "Name: Score, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# leave out 20% of the data for testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# only keep sentence and score columns\n",
        "\n",
        "train, test = train_test_split(train_augmented, test_size=0.2, random_state=23, stratify=train_augmented['Score'])\n",
        "\n",
        "class_counts_train = train['Score'].value_counts()\n",
        "class_counts_test = test['Score'].value_counts()\n",
        "# Print the counts\n",
        "print(class_counts_train)\n",
        "print(class_counts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "    new_df = pd.DataFrame(columns=['prompt', 'completion'])\n",
        "    new_df['prompt'] = df['Sentence'].apply(lambda x: 'Given the following sentence, analyze the sentiment and assign a score. The sentiment score should range from 0 to 4, where 0 indicates extremely negative sentiment, 1 indicates negative sentiment, 2 indicates neutral sentiment, 3 indicates positive sentiment, and 4 indicates extremely positive sentiment. ' + x + ' ->')\n",
        "    score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "    # map the scores to labels\n",
        "    new_df['completion'] = df['Score'].map(score_mapping)\n",
        "    # append space to the beginning of the completion\n",
        "    new_df['completion'] = new_df['completion'].apply(lambda x: ' ' + str(x))    \n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_new = preprocess(train)\n",
        "test_new = preprocess(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_gpt_new, test_gpt_new = train_test_split(train_new, test_size=0.2, random_state=23, stratify=train_new['completion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save train and test data to jsonl files\n",
        "train_gpt_new.to_json('train_gpt_new_at.jsonl', orient='records', lines=True)\n",
        "test_gpt_new.to_json('test_gpt_new_at.jsonl', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file from train_gpt_new_at.jsonl: file-E0anZl7Y5325NZ07fcbTzYkL\n",
            "Uploaded file from test_gpt_new_at.jsonl: file-gFQMrDAnnfaHMmHGV02wRgK9\n",
            "Created fine-tune: ft-9dRrQZjRHe2BCVhHfGRhCucK\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-06-05 21:41:43] Created fine-tune: ft-9dRrQZjRHe2BCVhHfGRhCucK\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-9dRrQZjRHe2BCVhHfGRhCucK\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Upload progress:   0%|          | 0.00/159k [00:00<?, ?it/s]\n",
            "Upload progress: 100%|██████████| 159k/159k [00:00<?, ?it/s]\n",
            "\n",
            "Upload progress:   0%|          | 0.00/38.0k [00:00<?, ?it/s]\n",
            "Upload progress: 100%|██████████| 38.0k/38.0k [00:00<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# create fine tunes model\n",
        "!openai api fine_tunes.create -t \"train_gpt_new_at.jsonl\" -v \"test_gpt_new_at.jsonl\" --compute_classification_metrics --classification_n_classes 5 -m ada --n_epochs 8 --suffix \"nlp_prod_ada\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-06-05 21:41:43] Created fine-tune: ft-9dRrQZjRHe2BCVhHfGRhCucK\n",
            "[2023-06-05 21:43:41] Fine-tune costs $0.09\n",
            "[2023-06-05 21:43:42] Fine-tune enqueued. Queue number: 0\n",
            "[2023-06-05 21:53:43] Fine-tune started\n",
            "[2023-06-05 21:54:45] Completed epoch 1/8\n",
            "[2023-06-05 21:55:33] Completed epoch 2/8\n",
            "[2023-06-05 21:56:20] Completed epoch 3/8\n",
            "[2023-06-05 21:57:06] Completed epoch 4/8\n",
            "[2023-06-05 21:57:52] Completed epoch 5/8\n",
            "[2023-06-05 21:58:39] Completed epoch 6/8\n",
            "[2023-06-05 21:59:26] Completed epoch 7/8\n",
            "[2023-06-05 22:00:12] Completed epoch 8/8\n",
            "[2023-06-05 22:00:40] Uploaded model: ada:ft-personal:nlp-prod-ada-2023-06-05-14-00-39\n",
            "[2023-06-05 22:00:41] Uploaded result file: file-WLgeXKKfsEra7imb9vIRGWyD\n",
            "[2023-06-05 22:00:41] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal:nlp-prod-ada-2023-06-05-14-00-39 -p <YOUR_PROMPT>\n"
          ]
        }
      ],
      "source": [
        "!openai api fine_tunes.follow -i ft-9dRrQZjRHe2BCVhHfGRhCucK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save results to csv\n",
        "!openai api fine_tunes.results -i ft-9dRrQZjRHe2BCVhHfGRhCucK > result.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>step</th>\n",
              "      <th>elapsed_tokens</th>\n",
              "      <th>elapsed_examples</th>\n",
              "      <th>training_loss</th>\n",
              "      <th>training_sequence_accuracy</th>\n",
              "      <th>training_token_accuracy</th>\n",
              "      <th>validation_loss</th>\n",
              "      <th>validation_sequence_accuracy</th>\n",
              "      <th>validation_token_accuracy</th>\n",
              "      <th>classification/accuracy</th>\n",
              "      <th>classification/weighted_f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>289</td>\n",
              "      <td>28097</td>\n",
              "      <td>289</td>\n",
              "      <td>0.022610</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.028064</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.355336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>577</td>\n",
              "      <td>56105</td>\n",
              "      <td>577</td>\n",
              "      <td>0.018040</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.016267</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.721041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>865</th>\n",
              "      <td>866</td>\n",
              "      <td>84210</td>\n",
              "      <td>866</td>\n",
              "      <td>0.018978</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.777825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1154</th>\n",
              "      <td>1155</td>\n",
              "      <td>112291</td>\n",
              "      <td>1155</td>\n",
              "      <td>0.004636</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.877833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1443</th>\n",
              "      <td>1444</td>\n",
              "      <td>140396</td>\n",
              "      <td>1444</td>\n",
              "      <td>0.009210</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.844889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1731</th>\n",
              "      <td>1732</td>\n",
              "      <td>168540</td>\n",
              "      <td>1732</td>\n",
              "      <td>0.007871</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.916672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021</th>\n",
              "      <td>2022</td>\n",
              "      <td>196558</td>\n",
              "      <td>2022</td>\n",
              "      <td>0.003779</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.903831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2304</th>\n",
              "      <td>2305</td>\n",
              "      <td>224201</td>\n",
              "      <td>2305</td>\n",
              "      <td>0.007867</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.009680</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.890314</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      step  elapsed_tokens  elapsed_examples  training_loss  \\\n",
              "288    289           28097               289       0.022610   \n",
              "576    577           56105               577       0.018040   \n",
              "865    866           84210               866       0.018978   \n",
              "1154  1155          112291              1155       0.004636   \n",
              "1443  1444          140396              1444       0.009210   \n",
              "1731  1732          168540              1732       0.007871   \n",
              "2021  2022          196558              2022       0.003779   \n",
              "2304  2305          224201              2305       0.007867   \n",
              "\n",
              "      training_sequence_accuracy  training_token_accuracy  validation_loss  \\\n",
              "288                          1.0                      1.0         0.028064   \n",
              "576                          1.0                      1.0         0.016267   \n",
              "865                          1.0                      1.0              NaN   \n",
              "1154                         1.0                      1.0              NaN   \n",
              "1443                         1.0                      1.0              NaN   \n",
              "1731                         1.0                      1.0              NaN   \n",
              "2021                         1.0                      1.0              NaN   \n",
              "2304                         1.0                      1.0         0.009680   \n",
              "\n",
              "      validation_sequence_accuracy  validation_token_accuracy  \\\n",
              "288                            1.0                        1.0   \n",
              "576                            1.0                        1.0   \n",
              "865                            NaN                        NaN   \n",
              "1154                           NaN                        NaN   \n",
              "1443                           NaN                        NaN   \n",
              "1731                           NaN                        NaN   \n",
              "2021                           NaN                        NaN   \n",
              "2304                           1.0                        1.0   \n",
              "\n",
              "      classification/accuracy  classification/weighted_f1_score  \n",
              "288                  0.416667                          0.355336  \n",
              "576                  0.722222                          0.721041  \n",
              "865                  0.777778                          0.777825  \n",
              "1154                 0.875000                          0.877833  \n",
              "1443                 0.847222                          0.844889  \n",
              "1731                 0.916667                          0.916672  \n",
              "2021                 0.902778                          0.903831  \n",
              "2304                 0.888889                          0.890314  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = pd.read_csv('result.csv')\n",
        "results[results['classification/accuracy'].notnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot: >"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "results[results['classification/accuracy'].notnull()]['classification/accuracy'].plot()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
