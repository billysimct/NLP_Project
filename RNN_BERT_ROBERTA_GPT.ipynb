{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DgTlf5hc3tg",
        "outputId": "af9da1a8-7fbd-428b-fe16-b10703a4832a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/SMU_MITB_NLP/project\n"
          ]
        }
      ],
      "source": [
        "# connect google drive folder if using colab, gpu will be needed for gpu enabled models\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SMU_MITB_NLP/project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUJ0bSXUc2m2"
      },
      "outputs": [],
      "source": [
        "# run cell to read file from data if using colab\n",
        "\n",
        "import pandas as pd\n",
        "labelled_sentences = pd.read_excel('FOMC Labelled Sentences.xlsx')\n",
        "statements = pd.read_excel('FOMC Statements 1997-2023.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "eBbuEpb6UwSd",
        "outputId": "c61e8975-b18e-4c39-a234-3186663dfa67"
      },
      "outputs": [],
      "source": [
        "# run cell if using from local/github\n",
        "\n",
        "import pandas as pd\n",
        "# read file from data\n",
        "labelled_sentences = pd.read_excel('data/FOMC Labelled Sentences.xlsx')\n",
        "statements = pd.read_excel('data/FOMC Statements 1997-2023.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5jAde_tc2m3",
        "outputId": "7ac98e62-5f85-4456-9742-35451f925390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score\n",
            "0       57\n",
            "-1      42\n",
            "0.5     36\n",
            "1       32\n",
            "-0.5    21\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "labelled_sentences.head()\n",
        "\n",
        "# remove row if score contains 'Remove'\n",
        "labelled_sentences = labelled_sentences[labelled_sentences['Score'] != 'Remove']\n",
        "labelled_sentences.shape\n",
        "\n",
        "class_counts = labelled_sentences['Score'].value_counts()\n",
        "print(class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqX0CzCYngtU",
        "outputId": "4df884fd-3a58-48ac-d8cd-e81c80ba2786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.27.1)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nlpaug # needed if using colab, else skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "izM6WhSPUwSe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Owner\\anaconda3\\envs\\nlp_project_3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9tNJNF4UwSe",
        "outputId": "1231f939-378b-4bfa-c9a6-a36b552448fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score\n",
            "0       45\n",
            "-1      33\n",
            "0.5     29\n",
            "1       26\n",
            "-0.5    17\n",
            "Name: count, dtype: int64\n",
            "Score\n",
            "0       12\n",
            "-1       9\n",
            "0.5      7\n",
            "1        6\n",
            "-0.5     4\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# leave out 20% of the data for testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# only keep sentence and score columns\n",
        "\n",
        "\n",
        "labelled_sentences = labelled_sentences[['Sentence', 'Score']]\n",
        "\n",
        "train, test = train_test_split(labelled_sentences, test_size=0.2, random_state=23, stratify=labelled_sentences['Score'])\n",
        "\n",
        "class_counts_train = train['Score'].value_counts()\n",
        "class_counts_test = test['Score'].value_counts()\n",
        "# Print the counts\n",
        "print(class_counts_train)\n",
        "print(class_counts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1sfbvtNZUwSe"
      },
      "outputs": [],
      "source": [
        "# Function to create new dataframe with augmented data and label by providing augmenter\n",
        "\n",
        "def augment_dataset(data, augmenter, num_augmented=2):\n",
        "    data_augmented = data.copy()\n",
        "    augmented_data = []\n",
        "\n",
        "    for index, row in data_augmented.iterrows():\n",
        "        for _ in range(num_augmented):\n",
        "            augmented_data.append([ ' '.join(augmenter.augment(row['Sentence'])), row['Score']])\n",
        "\n",
        "    augmented_data_df = pd.DataFrame(augmented_data, columns=['Sentence', 'Score'])\n",
        "\n",
        "    return augmented_data_df\n",
        "\n",
        "def combine_datasets(original_data, augmented_data):\n",
        "    # Combine the original dataset and the augmented dataset into a single dataframe\n",
        "    combined_data = pd.concat([original_data, augmented_data], ignore_index=True)\n",
        "    \n",
        "    return combined_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "eACpkUw-UwSf",
        "outputId": "46e29fc6-bd5e-4809-866d-9a0b062d1b42"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Puffiness have picked up in late months, in th...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Puffiness has picked up in recent calendar mon...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The ongoing public health crisis cover to weig...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The ongoing public health crisis continue to c...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>_x000D_ _x000D_ Although the necessary realloc...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>In direct contrast, the probability, though no...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>The Commission expect that, with appropriate p...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>The Committee anticipate that, with appropriat...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>The Fed Reserve follow prepared to change thes...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>The Federal Reservation is make to modify thes...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  Score\n",
              "0    Puffiness have picked up in late months, in th...    1.0\n",
              "1    Puffiness has picked up in recent calendar mon...    1.0\n",
              "2    The ongoing public health crisis cover to weig...   -1.0\n",
              "3    The ongoing public health crisis continue to c...   -1.0\n",
              "4    _x000D_ _x000D_ Although the necessary realloc...    0.5\n",
              "..                                                 ...    ...\n",
              "295  In direct contrast, the probability, though no...   -1.0\n",
              "296  The Commission expect that, with appropriate p...    0.5\n",
              "297  The Committee anticipate that, with appropriat...    0.5\n",
              "298  The Fed Reserve follow prepared to change thes...   -0.5\n",
              "299  The Federal Reservation is make to modify thes...   -0.5\n",
              "\n",
              "[300 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "augmented_data = augment_dataset(train, aug)\n",
        "augmented_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "LldQoq5mUwSf",
        "outputId": "9e6d9df2-6828-4802-a1fb-8619c7268482"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Inflation has picked up in recent months, main...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The ongoing public health crisis continues to ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>_x000D_\\n_x000D_\\nAlthough the necessary reall...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Recent developments are likely to result in ti...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In determining the timing and size of future a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>In direct contrast, the probability, though no...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>The Commission expect that, with appropriate p...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>The Committee anticipate that, with appropriat...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>The Fed Reserve follow prepared to change thes...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>The Federal Reservation is make to modify thes...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>450 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence Score\n",
              "0    Inflation has picked up in recent months, main...     1\n",
              "1    The ongoing public health crisis continues to ...    -1\n",
              "2    _x000D_\\n_x000D_\\nAlthough the necessary reall...   0.5\n",
              "3    Recent developments are likely to result in ti...    -1\n",
              "4    In determining the timing and size of future a...     0\n",
              "..                                                 ...   ...\n",
              "445  In direct contrast, the probability, though no...  -1.0\n",
              "446  The Commission expect that, with appropriate p...   0.5\n",
              "447  The Committee anticipate that, with appropriat...   0.5\n",
              "448  The Fed Reserve follow prepared to change thes...  -0.5\n",
              "449  The Federal Reservation is make to modify thes...  -0.5\n",
              "\n",
              "[450 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_augmented = combine_datasets(train, augmented_data)\n",
        "train_augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gSrhcNwUwSf"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JHqxerIz4571"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRB3-PMjVBvP"
      },
      "outputs": [],
      "source": [
        "# !pip install keras-self-attention # needed if using colab, else skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtG-OQtac2m4"
      },
      "outputs": [],
      "source": [
        "# RNN\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Bidirectional, SimpleRNN, LeakyReLU, Dropout, MultiHeadAttention, Flatten\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujGd8qRnc2m4",
        "outputId": "4d48568f-d07e-49d3-ba24-b959cdc11ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 16s 954ms/step - loss: 1.6003 - accuracy: 0.3067 - val_loss: 1.6049 - val_accuracy: 0.1579\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 1s 231ms/step - loss: 1.5698 - accuracy: 0.3400 - val_loss: 1.6165 - val_accuracy: 0.1579\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 1s 175ms/step - loss: 1.5582 - accuracy: 0.3400 - val_loss: 1.6829 - val_accuracy: 0.1579\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 1s 209ms/step - loss: 1.5404 - accuracy: 0.3400 - val_loss: 1.6452 - val_accuracy: 0.1579\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 1s 235ms/step - loss: 1.5362 - accuracy: 0.3400 - val_loss: 1.6254 - val_accuracy: 0.1579\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 1s 190ms/step - loss: 1.5292 - accuracy: 0.3400 - val_loss: 1.6341 - val_accuracy: 0.1579\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 1s 104ms/step - loss: 1.5169 - accuracy: 0.3400 - val_loss: 1.6501 - val_accuracy: 0.1579\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 1.4801 - accuracy: 0.3400 - val_loss: 1.6333 - val_accuracy: 0.1579\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 1s 197ms/step - loss: 1.4060 - accuracy: 0.3400 - val_loss: 1.6127 - val_accuracy: 0.1579\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 1.2683 - accuracy: 0.4600 - val_loss: 1.5818 - val_accuracy: 0.2368\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 1s 299ms/step - loss: 1.1604 - accuracy: 0.5200 - val_loss: 1.6382 - val_accuracy: 0.2368\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 1.0983 - accuracy: 0.5133 - val_loss: 1.6138 - val_accuracy: 0.2632\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 2s 346ms/step - loss: 1.0969 - accuracy: 0.5333 - val_loss: 1.6594 - val_accuracy: 0.2632\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 1.0123 - accuracy: 0.5533 - val_loss: 1.6651 - val_accuracy: 0.2895\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 1s 95ms/step - loss: 1.0211 - accuracy: 0.6000 - val_loss: 1.7483 - val_accuracy: 0.3421\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.9441 - accuracy: 0.6400 - val_loss: 1.6043 - val_accuracy: 0.3158\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.8786 - accuracy: 0.7200 - val_loss: 1.5866 - val_accuracy: 0.5263\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 1s 189ms/step - loss: 0.8033 - accuracy: 0.7533 - val_loss: 1.6530 - val_accuracy: 0.4474\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 1s 268ms/step - loss: 0.7293 - accuracy: 0.6867 - val_loss: 1.6508 - val_accuracy: 0.3684\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 1s 151ms/step - loss: 0.6509 - accuracy: 0.6933 - val_loss: 1.7716 - val_accuracy: 0.3421\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd00a59d0f0>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "RNN Modelling - \n",
        "\n",
        "Data is first processed using the process_sentences function\n",
        "Tokenizer is created and fit on the training and development sentences\n",
        "Additional preprocessing to ensure the parameters and formats are right\n",
        "Sequential model is created with multiple layers - and to be tested with different layers and parameters\n",
        "Model is compiled with binary_crossentropy loss function for binary classification\n",
        "\"\"\"\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "class_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(class_mapping.keys())\n",
        "y_train = y_train.map(class_mapping)\n",
        "y_test = y_test.map(class_mapping)\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in test_sequences))\n",
        "\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Define the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index for padding\n",
        "\n",
        "# Create a Sequential model, tested many different layers and parameters\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
        "# model.add(Bidirectional(LSTM(50)))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(SimpleRNN(units=32))\n",
        "# model.add(LeakyReLU(alpha=0.01))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 30, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(30)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(Bidirectional(LSTM(units=16, return_sequences=True, dropout=0.5, recurrent_dropout=0.7)))\n",
        "# model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model for multi-class classification\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6) # stop the training process, for example if the validation loss doesn't decrease for 2 consecutive epochs (patience=2). to prevent overfitting\n",
        "\n",
        "# Include early stopping in the model fit\n",
        "rnn = model.fit(train_sequences, y_train, epochs=20, validation_data=(test_sequences, y_test))#, callbacks=[early_stopping])\n",
        "rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k6lXPeMUwSg",
        "outputId": "e2848013-75c0-4cce-914c-cd939a4e5bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "12/12 [==============================] - 13s 282ms/step - loss: 1.5962 - accuracy: 0.3222 - val_loss: 1.5585 - val_accuracy: 0.3556\n",
            "Epoch 2/20\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 1.5605 - accuracy: 0.3361 - val_loss: 1.5215 - val_accuracy: 0.3556\n",
            "Epoch 3/20\n",
            "12/12 [==============================] - 1s 115ms/step - loss: 1.5078 - accuracy: 0.3361 - val_loss: 1.4556 - val_accuracy: 0.3556\n",
            "Epoch 4/20\n",
            "12/12 [==============================] - 1s 127ms/step - loss: 1.3563 - accuracy: 0.3556 - val_loss: 1.3527 - val_accuracy: 0.4667\n",
            "Epoch 5/20\n",
            "12/12 [==============================] - 1s 120ms/step - loss: 1.1635 - accuracy: 0.4750 - val_loss: 1.1564 - val_accuracy: 0.5111\n",
            "Epoch 6/20\n",
            "12/12 [==============================] - 2s 135ms/step - loss: 1.0956 - accuracy: 0.5278 - val_loss: 1.1026 - val_accuracy: 0.5444\n",
            "Epoch 7/20\n",
            "12/12 [==============================] - 1s 120ms/step - loss: 0.9702 - accuracy: 0.6361 - val_loss: 1.0242 - val_accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "12/12 [==============================] - 1s 113ms/step - loss: 0.8247 - accuracy: 0.6472 - val_loss: 1.0262 - val_accuracy: 0.5111\n",
            "Epoch 9/20\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.7309 - accuracy: 0.6583 - val_loss: 0.8906 - val_accuracy: 0.5667\n",
            "Epoch 10/20\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.6044 - accuracy: 0.7111 - val_loss: 0.8249 - val_accuracy: 0.6333\n",
            "Epoch 11/20\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 0.5406 - accuracy: 0.7722 - val_loss: 0.9842 - val_accuracy: 0.5778\n",
            "Epoch 12/20\n",
            "12/12 [==============================] - 2s 128ms/step - loss: 0.4870 - accuracy: 0.8000 - val_loss: 0.9936 - val_accuracy: 0.6222\n",
            "Epoch 13/20\n",
            "12/12 [==============================] - 2s 131ms/step - loss: 0.4721 - accuracy: 0.7861 - val_loss: 0.8085 - val_accuracy: 0.6333\n",
            "Epoch 14/20\n",
            "12/12 [==============================] - 1s 125ms/step - loss: 0.3730 - accuracy: 0.8806 - val_loss: 0.8145 - val_accuracy: 0.7111\n",
            "Epoch 15/20\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.2923 - accuracy: 0.9361 - val_loss: 0.8177 - val_accuracy: 0.6778\n",
            "Epoch 16/20\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.2270 - accuracy: 0.9556 - val_loss: 0.8814 - val_accuracy: 0.7111\n",
            "Epoch 17/20\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 0.1765 - accuracy: 0.9722 - val_loss: 1.0371 - val_accuracy: 0.6667\n",
            "Epoch 18/20\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 0.1304 - accuracy: 0.9861 - val_loss: 0.8539 - val_accuracy: 0.6889\n",
            "Epoch 19/20\n",
            "12/12 [==============================] - 1s 115ms/step - loss: 0.1172 - accuracy: 0.9806 - val_loss: 1.2333 - val_accuracy: 0.6556\n",
            "Epoch 20/20\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 0.1019 - accuracy: 0.9806 - val_loss: 1.0787 - val_accuracy: 0.7000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1de278f6370>"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "RNN Modelling - \n",
        "\n",
        "Data is first processed using the process_sentences function\n",
        "Tokenizer is created and fit on the training and development sentences\n",
        "Additional preprocessing to ensure the parameters and formats are right\n",
        "Sequential model is created with multiple layers - and to be tested with different layers and parameters\n",
        "Model is compiled with binary_crossentropy loss function for binary classification\n",
        "\"\"\"\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_augmented['Sentence'], train_augmented['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "class_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(class_mapping.keys())\n",
        "y_train = y_train.map(class_mapping)\n",
        "y_test = y_test.map(class_mapping)\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in test_sequences))\n",
        "\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Define the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index for padding\n",
        "\n",
        "# Create a Sequential model, tested many different layers and parameters\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
        "# model.add(Bidirectional(LSTM(50)))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(SimpleRNN(units=32))\n",
        "# model.add(LeakyReLU(alpha=0.01))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 30, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(30)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
        "# model.add(Bidirectional(LSTM(units=16, return_sequences=True, dropout=0.5, recurrent_dropout=0.7)))\n",
        "# model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model for multi-class classification\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6) # stop the training process, for example if the validation loss doesn't decrease for 2 consecutive epochs (patience=2). to prevent overfitting\n",
        "\n",
        "# Include early stopping in the model fit\n",
        "rnn = model.fit(train_sequences, y_train, epochs=20, validation_data=(test_sequences, y_test))#, callbacks=[early_stopping])\n",
        "rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3oxwcKjUwSh",
        "outputId": "67ca3960-cf6e-4918-f17f-353dfbc2be5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 20ms/step\n",
            "Accuracy: 42.11%\n",
            "[ 1 -1  0  1 -1 -1 -1  1  1  0  1  0 -1  1  0  0  0 -1 -1  0  1  0  0  0\n",
            "  0  0  1  0  1  1  1  0  1  1  1 -1  1 -1]\n",
            "[1 -1 1 1 -1 -1 -1 0.5 0.5 1 0.5 -1 -1 0.5 0 0.5 -0.5 0.5 1 -0.5 1 0.5 0 0\n",
            " -1 0.5 -1 0 1 -1 1 -0.5 -1 1 1 0 -1 0]\n"
          ]
        }
      ],
      "source": [
        "# Assuming your original test sentences are in a pandas DataFrame column named 'Sentence'\n",
        "# Tokenize the test sentences\n",
        "test_sequences_original = tokenizer.texts_to_sequences(test['Sentence'])\n",
        "\n",
        "# Pad the sequences\n",
        "test_sequences_original = pad_sequences(test_sequences_original, maxlen=max_length, padding='post')\n",
        "\n",
        "# Predict class probabilities for the test set\n",
        "y_pred_prob = model.predict(test_sequences_original)\n",
        "\n",
        "# Select the class with the highest probability as the predicted class\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in class_mapping.items()}\n",
        "\n",
        "# Inverse map the classes to their original values\n",
        "y_pred = np.vectorize(inverse_class_mapping.get)(y_pred)\n",
        "\n",
        "y_test_original = test['Score']\n",
        "\n",
        "accuracy = np.mean(y_pred == y_test_original.values)\n",
        "\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "print(y_pred)\n",
        "print(y_test_original.values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0DOnr9IUwSh",
        "outputId": "4ff82222-a91f-471f-aefd-418ba410cfb5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>But domestic financial markets have recovered ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>The Committee will continue its purchases of T...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>In light of the substantial further progress t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>Job gains have been solid in recent months, an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>In a related action, the Board of Governors un...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>\\nThe Federal Reserve is committed to using it...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>The possibility that this excess could continu...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>However, a sustained moderation in inflation p...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>However, the Committee judges that some inflat...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>\\nInformation received since the Federal Open ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>This program, which would gradually reduce the...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>The erosion in current and prospective profita...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>Russia's war against Ukraine is causing tremen...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>However, a sustained moderation in inflation p...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>This action accelerates the release of this in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>_x000D_\\n_x000D_\\nAlthough the necessary reall...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>This policy, by keeping the Committee's holdin...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>When the Committee decides to begin to remove ...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>\\n_x000D_\\nThe Federal Open Market Committee v...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>The Committee is maintaining its existing poli...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>In support of these goals, the Committee decid...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>A range of recent labor market indicators, inc...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>Inflation _x000D_\\n        and longer-term inf...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>Overall financial conditions remain accommodat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>The erosion in current and prospective profita...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>The Committee expects that economic conditions...</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Currently, the unemployment rate remains eleva...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>The Committee perceives the upside and downsid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>With progress on vaccinations and strong polic...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Recent developments are likely to result in ti...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>In light of the improving economy, Mr. Hoenig ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>With underlying inflation expected to be relat...</td>\n",
              "      <td>-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>The Committee currently anticipates that, even...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>Voting against the policy action was Thomas M....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Voting against the action was Esther L. George...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>The arrangement with the Bank of Canada would ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>Tight credit conditions, the ongoing housing c...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Available data suggest that household spending...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence Score\n",
              "16   But domestic financial markets have recovered ...     1\n",
              "188  The Committee will continue its purchases of T...    -1\n",
              "198  In light of the substantial further progress t...     1\n",
              "179  Job gains have been solid in recent months, an...     1\n",
              "95   In a related action, the Board of Governors un...    -1\n",
              "145  \\nThe Federal Reserve is committed to using it...    -1\n",
              "174  The possibility that this excess could continu...    -1\n",
              "21   However, a sustained moderation in inflation p...   0.5\n",
              "64   However, the Committee judges that some inflat...   0.5\n",
              "80   \\nInformation received since the Federal Open ...     1\n",
              "35   This program, which would gradually reduce the...   0.5\n",
              "133  The erosion in current and prospective profita...    -1\n",
              "186  Russia's war against Ukraine is causing tremen...    -1\n",
              "163  However, a sustained moderation in inflation p...   0.5\n",
              "113  This action accelerates the release of this in...     0\n",
              "134  _x000D_\\n_x000D_\\nAlthough the necessary reall...   0.5\n",
              "51   This policy, by keeping the Committee's holdin...  -0.5\n",
              "82   When the Committee decides to begin to remove ...   0.5\n",
              "74   \\n_x000D_\\nThe Federal Open Market Committee v...     1\n",
              "118  The Committee is maintaining its existing poli...  -0.5\n",
              "20   In support of these goals, the Committee decid...     1\n",
              "52   A range of recent labor market indicators, inc...   0.5\n",
              "169  Inflation _x000D_\\n        and longer-term inf...     0\n",
              "142  Overall financial conditions remain accommodat...     0\n",
              "168  The erosion in current and prospective profita...    -1\n",
              "167  The Committee expects that economic conditions...   0.5\n",
              "65   Currently, the unemployment rate remains eleva...    -1\n",
              "79   The Committee perceives the upside and downsid...     0\n",
              "9    With progress on vaccinations and strong polic...     1\n",
              "49   Recent developments are likely to result in ti...    -1\n",
              "70   In light of the improving economy, Mr. Hoenig ...     1\n",
              "106  With underlying inflation expected to be relat...  -0.5\n",
              "94   The Committee currently anticipates that, even...    -1\n",
              "180  Voting against the policy action was Thomas M....     1\n",
              "11   Voting against the action was Esther L. George...     1\n",
              "116  The arrangement with the Bank of Canada would ...     0\n",
              "122  Tight credit conditions, the ongoing housing c...    -1\n",
              "31   Available data suggest that household spending...     0"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlmIkNaHUwSh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8OmIwGRUwSi"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jcK237tEc2m5"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIokrzfVc2m6",
        "outputId": "e315145d-c6d6-48c4-f660-2e0526f92cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers # run this cell if using notebook in colab "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjtZSG5jc2m6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaForSequenceClassification, RobertaTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIwSO11jc2m7",
        "outputId": "75b6622f-ee9d-4d01-afcd-dc45161500ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=64)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=64)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load BertForSequenceClassification, adjust num_labels to your classification task\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 16 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL-YxCW5c2m7",
        "outputId": "866106e7-9154-4e1f-8d8c-1506c08edea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 16 ========\n",
            "\n",
            "  Average training loss: 1.61\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.24\n",
            "  Validation Loss: 1.62\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 16 ========\n",
            "\n",
            "  Average training loss: 1.49\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.18\n",
            "  Validation Loss: 1.54\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 16 ========\n",
            "\n",
            "  Average training loss: 1.37\n",
            "  Training epoch took: 0:00:01\n",
            "\n",
            "  Accuracy: 0.41\n",
            "  Validation Loss: 1.51\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 16 ========\n",
            "\n",
            "  Average training loss: 1.26\n",
            "  Training epoch took: 0:00:01\n",
            "\n",
            "  Accuracy: 0.37\n",
            "  Validation Loss: 1.51\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 16 ========\n",
            "\n",
            "  Average training loss: 1.13\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.35\n",
            "  Validation Loss: 1.52\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 16 ========\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.45\n",
            "  Validation Loss: 1.44\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 16 ========\n",
            "\n",
            "  Average training loss: 0.90\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.45\n",
            "  Validation Loss: 1.43\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 16 ========\n",
            "\n",
            "  Average training loss: 0.76\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.45\n",
            "  Validation Loss: 1.34\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 16 ========\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.55\n",
            "  Validation Loss: 1.32\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 16 ========\n",
            "\n",
            "  Average training loss: 0.58\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.30\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 16 ========\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.20\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 16 ========\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.19\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 16 ========\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.21\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 16 ========\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.24\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 16 ========\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.26\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 16 / 16 ========\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.26\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmTnZiF0c2m8",
        "outputId": "9ed8feaa-5df9-440a-f30e-3bf10924aa13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Testing...\n",
            "Testing took: 0:00:00\n"
          ]
        }
      ],
      "source": [
        "test_input_ids, test_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=64)\n",
        "\n",
        "\n",
        "\n",
        "# Create the DataLoader for test set\n",
        "test_data = TensorDataset(test_input_ids, test_attention_mask)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "print(\"Running Testing...\")\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# List to store predictions\n",
        "test_predictions = []\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "    \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    \n",
        "    with torch.no_grad():        \n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        output = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask)\n",
        "        logits = output.logits\n",
        "        \n",
        "    # Move logits to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    # Store predictions\n",
        "    test_predictions.extend(np.argmax(logits, axis=1))\n",
        "\n",
        "# Measure how long the testing run took.\n",
        "testing_time = format_time(time.time() - t0)\n",
        "\n",
        "print(\"Testing took: {:}\".format(testing_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3cxnunlc2m8",
        "outputId": "2b7c30d3-9da6-4140-8993-9f4c31731acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: [5, 3, 5, 5, 1, 1, 1, 1, 2, 4, 3, 4, 1, 1, 3, 4, 1, 4, 3, 4, 5, 4, 4, 4, 4, 1, 4, 3, 5, 1, 4, 4, 1, 1, 5, 3, 1, 4]\n"
          ]
        }
      ],
      "source": [
        "# Store predictions with labels starting from 1\n",
        "test_predictions_bert = [prediction+1 for prediction in test_predictions]\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predictions:\", test_predictions_bert)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "raggnBDCqAJl"
      },
      "source": [
        "# ROBERTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fr2bCcylGUP",
        "outputId": "a7defa77-124d-4b0e-8d12-834544b7c7c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=100)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load RobertaForSequenceClassification\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 16 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkFucNnIlOLg",
        "outputId": "3f225f5f-8280-47ce-9028-32b80b36223b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 16 ========\n",
            "\n",
            "  Average training loss: 1.60\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.23\n",
            "  Validation Loss: 1.60\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 16 ========\n",
            "\n",
            "  Average training loss: 1.51\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.23\n",
            "  Validation Loss: 1.71\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 16 ========\n",
            "\n",
            "  Average training loss: 1.48\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.34\n",
            "  Validation Loss: 1.57\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 16 ========\n",
            "\n",
            "  Average training loss: 1.32\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.32\n",
            "  Validation Loss: 1.57\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 16 ========\n",
            "\n",
            "  Average training loss: 1.15\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.42\n",
            "  Validation Loss: 1.49\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 16 ========\n",
            "\n",
            "  Average training loss: 0.98\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 1.46\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 16 ========\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.48\n",
            "  Validation Loss: 1.42\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 16 ========\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.40\n",
            "  Validation Loss: 1.34\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 16 ========\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.31\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 16 ========\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.32\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 16 ========\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 16 ========\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 16 ========\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.04\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 16 ========\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.13\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 16 ========\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 16 / 16 ========\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epoch took: 0:00:03\n",
            "\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.20\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9NdDzU4lkYH",
        "outputId": "97fb0cb9-49ae-416b-b483-1876fa78db21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-45-96255f1895ed>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_inputs = torch.tensor(test_input_ids)\n",
            "<ipython-input-45-96255f1895ed>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_masks = torch.tensor(test_attention_mask)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 60.53%\n"
          ]
        }
      ],
      "source": [
        "# Assuming test_sentences are in a pandas DataFrame column named 'Sentence'\n",
        "# Tokenize the test sentences\n",
        "test_input_ids, test_attention_mask = tokenize(test['Sentence'].values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert inputs to tensors\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_masks = torch.tensor(test_attention_mask)\n",
        "\n",
        "# Create DataLoader for the test data\n",
        "batch_size = 32\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predicted_scores = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "    logits = outputs[0]\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predicted_scores.extend(np.argmax(logits, axis=1).flatten())\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in score_mapping.items()}\n",
        "\n",
        "# Inverse map the classes to their original values\n",
        "predicted_scores = np.vectorize(inverse_class_mapping.get)(predicted_scores)\n",
        "\n",
        "accuracy = np.mean(predicted_scores == test['Score'].values)\n",
        "print(f'Test Accuracy: {accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcPPgkImZslL",
        "outputId": "4e4fa1e7-64bd-4eb1-b9b7-9d9d31fd1812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              Sentence  Predicted_Score\n",
            "16   But domestic financial markets have recovered ...              0.5\n",
            "188  The Committee will continue its purchases of T...             -0.5\n",
            "198  In light of the substantial further progress t...              1.0\n",
            "179  Job gains have been solid in recent months, an...              1.0\n",
            "95   In a related action, the Board of Governors un...             -1.0\n",
            "145  \\nThe Federal Reserve is committed to using it...             -1.0\n",
            "174  The possibility that this excess could continu...             -1.0\n",
            "21   However, a sustained moderation in inflation p...              0.5\n",
            "64   However, the Committee judges that some inflat...             -0.5\n",
            "80   \\nInformation received since the Federal Open ...              1.0\n",
            "35   This program, which would gradually reduce the...             -1.0\n",
            "133  The erosion in current and prospective profita...             -1.0\n",
            "186  Russia's war against Ukraine is causing tremen...             -1.0\n",
            "163  However, a sustained moderation in inflation p...              0.5\n",
            "113  This action accelerates the release of this in...              0.0\n",
            "134  _x000D_\\n_x000D_\\nAlthough the necessary reall...              0.5\n",
            "51   This policy, by keeping the Committee's holdin...             -1.0\n",
            "82   When the Committee decides to begin to remove ...             -0.5\n",
            "74   \\n_x000D_\\nThe Federal Open Market Committee v...              0.0\n",
            "118  The Committee is maintaining its existing poli...              0.0\n",
            "20   In support of these goals, the Committee decid...              1.0\n",
            "52   A range of recent labor market indicators, inc...              1.0\n",
            "169  Inflation _x000D_\\n        and longer-term inf...             -0.5\n",
            "142  Overall financial conditions remain accommodat...             -0.5\n",
            "168  The erosion in current and prospective profita...             -1.0\n",
            "167  The Committee expects that economic conditions...             -1.0\n",
            "65   Currently, the unemployment rate remains eleva...             -0.5\n",
            "79   The Committee perceives the upside and downsid...              0.0\n",
            "9    With progress on vaccinations and strong polic...              1.0\n",
            "49   Recent developments are likely to result in ti...             -1.0\n",
            "70   In light of the improving economy, Mr. Hoenig ...              1.0\n",
            "106  With underlying inflation expected to be relat...              0.5\n",
            "94   The Committee currently anticipates that, even...             -1.0\n",
            "180  Voting against the policy action was Thomas M....             -1.0\n",
            "11   Voting against the action was Esther L. George...              1.0\n",
            "116  The arrangement with the Bank of Canada would ...              0.0\n",
            "122  Tight credit conditions, the ongoing housing c...             -1.0\n",
            "31   Available data suggest that household spending...              0.5\n"
          ]
        }
      ],
      "source": [
        "# Add the original sentences and their predicted scores to a DataFrame\n",
        "predictions_df = pd.DataFrame({'Sentence': test['Sentence'], 'Predicted_Score': predicted_scores})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(predictions_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "79EocouQY78b"
      },
      "source": [
        "# RoBERTa Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmq-aui3Y9d4",
        "outputId": "39ef1060-47fb-4692-a618-61542d549985"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=100)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load RobertaForSequenceClassification\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-large\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 24 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNieKcu8Y_gN",
        "outputId": "8649e3d6-defd-4e75-adb7-e61f364ff91c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 24 ========\n",
            "\n",
            "  Average training loss: 1.64\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.23\n",
            "  Validation Loss: 1.55\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 24 ========\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.23\n",
            "  Validation Loss: 1.62\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 24 ========\n",
            "\n",
            "  Average training loss: 1.51\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.21\n",
            "  Validation Loss: 1.74\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 24 ========\n",
            "\n",
            "  Average training loss: 1.46\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.18\n",
            "  Validation Loss: 1.66\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 24 ========\n",
            "\n",
            "  Average training loss: 1.35\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.24\n",
            "  Validation Loss: 1.62\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 24 ========\n",
            "\n",
            "  Average training loss: 1.28\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.21\n",
            "  Validation Loss: 1.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 24 ========\n",
            "\n",
            "  Average training loss: 1.17\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.24\n",
            "  Validation Loss: 1.55\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 24 ========\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.29\n",
            "  Validation Loss: 1.61\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 24 ========\n",
            "\n",
            "  Average training loss: 0.96\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.39\n",
            "  Validation Loss: 1.48\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 24 ========\n",
            "\n",
            "  Average training loss: 0.84\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.30\n",
            "  Validation Loss: 1.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 24 ========\n",
            "\n",
            "  Average training loss: 0.72\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.46\n",
            "  Validation Loss: 1.39\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 24 ========\n",
            "\n",
            "  Average training loss: 0.62\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.33\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 13 / 24 ========\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.45\n",
            "  Validation Loss: 1.41\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 24 ========\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.44\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 24 ========\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.27\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 16 / 24 ========\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.55\n",
            "  Validation Loss: 1.35\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 17 / 24 ========\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.27\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 18 / 24 ========\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.55\n",
            "  Validation Loss: 1.33\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 19 / 24 ========\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.36\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 20 / 24 ========\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.39\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 21 / 24 ========\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.40\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 22 / 24 ========\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.38\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 23 / 24 ========\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.39\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 24 / 24 ========\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epoch took: 0:00:08\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.41\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0ZiP6BnZFos"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMbFpo9_cdVB"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6fqhaMM6cd68"
      },
      "source": [
        "# BERT Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iruw_-D8cf4b",
        "outputId": "64c11be0-ee0e-4984-940e-9dbffbe2f741"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(labelled_sentences['Sentence'], labelled_sentences['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=64)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=64)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load BertForSequenceClassification, adjust num_labels to your classification task\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-large-uncased\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 3e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 20 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSRzrxIEciPN",
        "outputId": "fecc6490-d120-486f-ecd8-bd0b3eb233c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "\n",
            "  Average training loss: 1.70\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.41\n",
            "  Validation Loss: 1.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "\n",
            "  Average training loss: 1.43\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.28\n",
            "  Validation Loss: 1.71\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "\n",
            "  Average training loss: 1.26\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.31\n",
            "  Validation Loss: 1.67\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "\n",
            "  Average training loss: 1.13\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.44\n",
            "  Validation Loss: 1.41\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.32\n",
            "  Validation Loss: 1.49\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "\n",
            "  Average training loss: 0.85\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 1.23\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.17\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.08\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "\n",
            "  Average training loss: 0.61\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.48\n",
            "  Validation Loss: 1.31\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.05\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.07\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.34\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.02\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.01\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.93\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 0.94\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.94\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.95\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epoch took: 0:00:05\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.95\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eME9Sc0Ocu51"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FBV_lzmVdu3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUyloaSPVdsb"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1yk_sreUVfHO"
      },
      "source": [
        "# RoBERTa with augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDDHxhNIVg6B",
        "outputId": "2b0a682a-22fe-42b4-e66d-d7ec218d1b8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_augmented['Sentence'], train_augmented['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=100)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# Load RobertaForSequenceClassification\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 12 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLgOCwY9Vnot",
        "outputId": "d535b122-2ace-4db5-c036-4b4e11580522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 12 ========\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.36\n",
            "  Validation Loss: 1.49\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 12 ========\n",
            "\n",
            "  Average training loss: 1.32\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.54\n",
            "  Validation Loss: 1.22\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 12 ========\n",
            "\n",
            "  Average training loss: 1.12\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.97\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 12 ========\n",
            "\n",
            "  Average training loss: 0.78\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 12 ========\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0:00:07\n",
            "\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 0.41\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 12 ========\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epoch took: 0:00:07\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.32\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 12 ========\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.20\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 12 ========\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.22\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 12 ========\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.35\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.94\n",
            "  Validation Loss: 0.21\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.93\n",
            "  Validation Loss: 0.22\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.23\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdf7whC3V3Lw",
        "outputId": "7ac905e2-3cc8-4a55-f518-0cea90da643d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-96255f1895ed>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_inputs = torch.tensor(test_input_ids)\n",
            "<ipython-input-31-96255f1895ed>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_masks = torch.tensor(test_attention_mask)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 57.89%\n"
          ]
        }
      ],
      "source": [
        "# Assuming test_sentences are in a pandas DataFrame column named 'Sentence'\n",
        "# Tokenize the test sentences\n",
        "test_input_ids, test_attention_mask = tokenize(test['Sentence'].values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert inputs to tensors\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_masks = torch.tensor(test_attention_mask)\n",
        "\n",
        "# Create DataLoader for the test data\n",
        "batch_size = 32\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predicted_scores = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "    logits = outputs[0]\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predicted_scores.extend(np.argmax(logits, axis=1).flatten())\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in score_mapping.items()}\n",
        "\n",
        "# Inverse map the classes to their original values\n",
        "predicted_scores = np.vectorize(inverse_class_mapping.get)(predicted_scores)\n",
        "\n",
        "accuracy = np.mean(predicted_scores == test['Score'].values)\n",
        "print(f'Test Accuracy: {accuracy*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46eXj5gYXFhY",
        "outputId": "64aa4209-d17b-4e3f-eed5-67a738fd73a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              Sentence  Predicted_Score\n",
            "16   But domestic financial markets have recovered ...              0.5\n",
            "188  The Committee will continue its purchases of T...             -0.5\n",
            "198  In light of the substantial further progress t...              1.0\n",
            "179  Job gains have been solid in recent months, an...              1.0\n",
            "95   In a related action, the Board of Governors un...             -1.0\n",
            "145  \\nThe Federal Reserve is committed to using it...             -1.0\n",
            "174  The possibility that this excess could continu...             -1.0\n",
            "21   However, a sustained moderation in inflation p...              0.5\n",
            "64   However, the Committee judges that some inflat...             -0.5\n",
            "80   \\nInformation received since the Federal Open ...              1.0\n",
            "35   This program, which would gradually reduce the...              1.0\n",
            "133  The erosion in current and prospective profita...             -1.0\n",
            "186  Russia's war against Ukraine is causing tremen...             -1.0\n",
            "163  However, a sustained moderation in inflation p...              0.5\n",
            "113  This action accelerates the release of this in...              0.0\n",
            "134  _x000D_\\n_x000D_\\nAlthough the necessary reall...              0.5\n",
            "51   This policy, by keeping the Committee's holdin...             -1.0\n",
            "82   When the Committee decides to begin to remove ...              0.0\n",
            "74   \\n_x000D_\\nThe Federal Open Market Committee v...              1.0\n",
            "118  The Committee is maintaining its existing poli...              0.0\n",
            "20   In support of these goals, the Committee decid...              1.0\n",
            "52   A range of recent labor market indicators, inc...              1.0\n",
            "169  Inflation _x000D_\\n        and longer-term inf...             -0.5\n",
            "142  Overall financial conditions remain accommodat...             -0.5\n",
            "168  The erosion in current and prospective profita...             -1.0\n",
            "167  The Committee expects that economic conditions...              0.5\n",
            "65   Currently, the unemployment rate remains eleva...              0.0\n",
            "79   The Committee perceives the upside and downsid...              0.0\n",
            "9    With progress on vaccinations and strong polic...              1.0\n",
            "49   Recent developments are likely to result in ti...             -1.0\n",
            "70   In light of the improving economy, Mr. Hoenig ...              1.0\n",
            "106  With underlying inflation expected to be relat...              0.5\n",
            "94   The Committee currently anticipates that, even...             -0.5\n",
            "180  Voting against the policy action was Thomas M....             -1.0\n",
            "11   Voting against the action was Esther L. George...              1.0\n",
            "116  The arrangement with the Bank of Canada would ...              1.0\n",
            "122  Tight credit conditions, the ongoing housing c...             -1.0\n",
            "31   Available data suggest that household spending...              0.5\n"
          ]
        }
      ],
      "source": [
        "# Add the original sentences and their predicted scores to a DataFrame\n",
        "predictions_df = pd.DataFrame({'Sentence': test['Sentence'], 'Predicted_Score': predicted_scores})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(predictions_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ws7CU4XHcn",
        "outputId": "e3039090-f3b8-430e-97c0-723137e4aab3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.5, -0.5,  1. ,  1. , -1. , -1. , -1. ,  0.5, -0.5,  1. ,  1. ,\n",
              "       -1. , -1. ,  0.5,  0. ,  0.5, -1. ,  0. ,  1. ,  0. ,  1. ,  1. ,\n",
              "       -0.5, -0.5, -1. ,  0.5,  0. ,  0. ,  1. , -1. ,  1. ,  0.5, -0.5,\n",
              "       -1. ,  1. ,  1. , -1. ,  0.5])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxmpzXpDdJzt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaForSequenceClassification, RobertaTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dhFgICQidK5u"
      },
      "source": [
        "# RoBERTA Augmented with MC Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FduWGtHOdJxT",
        "outputId": "9ec9ac2b-0e8d-4071-c8f5-89b0300369d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing MCRoberta: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing MCRoberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MCRoberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of MCRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and if not, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Function to ensure the input is formatted correctly such as tokenization, length, padding and attention\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        if encoded_dict['input_ids'].size()[1] > max_length:\n",
        "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :max_length]\n",
        "            encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :max_length]\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_augmented['Sentence'], train_augmented['Score'], test_size=0.2, random_state=23)\n",
        "\n",
        "score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "n_classes = len(score_mapping)\n",
        "\n",
        "y_train = y_train.map(score_mapping)\n",
        "y_test = y_test.map(score_mapping)\n",
        "\n",
        "# Tokenize both training and validation sentences\n",
        "train_input_ids, train_attention_mask = tokenize(x_train.values.tolist(), tokenizer, max_length=100)\n",
        "dev_input_ids, dev_attention_mask = tokenize(x_test.values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "dev_labels = torch.tensor(y_test.values)\n",
        "\n",
        "# Create the DataLoader for training set\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data, replacement=False, generator=torch.Generator().manual_seed(23))\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "# Create the DataLoader for dev set\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=32)\n",
        "\n",
        "# RoBERTa model with Monte Carlo Dropout\n",
        "class MCRoberta(RobertaForSequenceClassification):\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "    def eval(self):\n",
        "        self.training = True\n",
        "\n",
        "# Load MCRobertaForSequenceClassification\n",
        "model = MCRoberta.from_pretrained(\n",
        "    \"roberta-base\", \n",
        "    num_labels = n_classes,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # learning rate\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 12 # number of epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nZMvsICdJuz",
        "outputId": "be3c72ab-d5d6-452e-a7e4-ab81b13ab8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.85\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.84\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.89\n",
            "Accuracy for class 3: 0.85\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.60\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.87\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.89\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.58\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.84\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 0.85\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.84\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.81\n",
            "Accuracy for class 3: 0.92\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.87\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.89\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.55\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.87\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.88\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 1.00\n",
            "  Validation Loss: 0.47\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.87\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.89\n",
            "Accuracy for class 3: 0.92\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 12 ========\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.86\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.51\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.86\n",
            "Accuracy for class 0: 0.85\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.85\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.59\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 12 ========\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:00:06\n",
            "\n",
            "  Accuracy: 0.86\n",
            "Accuracy for class 0: 0.90\n",
            "Accuracy for class 1: 0.69\n",
            "Accuracy for class 2: 0.81\n",
            "Accuracy for class 3: 0.92\n",
            "Accuracy for class 4: 0.94\n",
            "  Validation Loss: 0.61\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def flat_accuracy_per_class(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    correct_per_class = defaultdict(int)\n",
        "    total_per_class = defaultdict(int)\n",
        "\n",
        "    for pred, label in zip(pred_flat, labels_flat):\n",
        "        if pred == label:\n",
        "            correct_per_class[label] += 1\n",
        "        total_per_class[label] += 1\n",
        "\n",
        "    accuracies_per_class = {label: correct / total for label, correct, total in zip(correct_per_class.keys(), correct_per_class.values(), total_per_class.values())}\n",
        "    return accuracies_per_class\n",
        "\n",
        "# Function to format time\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # Validation\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    correct_preds_per_class = defaultdict(int)\n",
        "    total_preds_per_class = defaultdict(int)\n",
        "\n",
        "    for batch in dev_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "        for label in np.unique(label_ids):\n",
        "            correct_preds_per_class[label] += np.sum((pred_flat == label_ids) & (label_ids == label))\n",
        "            total_preds_per_class[label] += np.sum(label_ids == label)\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "\n",
        "    for label in total_preds_per_class.keys():\n",
        "        accuracy = correct_preds_per_class[label] / total_preds_per_class[label]\n",
        "        print(f\"Accuracy for class {label}: {accuracy:.2f}\")\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "i1bin-itdpvk",
        "outputId": "e6a191fc-b5d5-4778-f79e-dd6ff0741806"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-d7ae4fddb137>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_inputs = torch.tensor(test_input_ids)\n",
            "<ipython-input-29-d7ae4fddb137>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_masks = torch.tensor(test_attention_mask)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 52.63%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ec629e23-334d-4d0f-8e6e-78677fadeb10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Ground Truth</th>\n",
              "      <th>Predicted_Score</th>\n",
              "      <th>Uncertainty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>\\nThe Federal Reserve is committed to using it...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.798450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>In a related action, the Board of Governors ap...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.203834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>However, a sustained moderation in inflation p...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.265498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Currently, the unemployment rate remains eleva...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.904383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>In determining how long to maintain this targe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.190304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Moreover, the high level of resource utilizati...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.566761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>The Committee continues to closely monitor inf...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.813860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>The Committee expects that economic conditions...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.315373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>\\n\\n_x000D_\\nThe Federal Open Market Committee...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.724857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Increases in the prices of energy and other co...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.217486</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec629e23-334d-4d0f-8e6e-78677fadeb10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ec629e23-334d-4d0f-8e6e-78677fadeb10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ec629e23-334d-4d0f-8e6e-78677fadeb10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              Sentence Ground Truth  \\\n",
              "43   \\nThe Federal Reserve is committed to using it...           -1   \n",
              "120  In a related action, the Board of Governors ap...           -1   \n",
              "163  However, a sustained moderation in inflation p...          0.5   \n",
              "65   Currently, the unemployment rate remains eleva...           -1   \n",
              "178  In determining how long to maintain this targe...            0   \n",
              "8    Moreover, the high level of resource utilizati...            1   \n",
              "148  The Committee continues to closely monitor inf...            0   \n",
              "167  The Committee expects that economic conditions...          0.5   \n",
              "23   \\n\\n_x000D_\\nThe Federal Open Market Committee...            0   \n",
              "47   Increases in the prices of energy and other co...            1   \n",
              "\n",
              "     Predicted_Score  Uncertainty  \n",
              "43              -0.5     0.798450  \n",
              "120             -1.0     0.203834  \n",
              "163              0.5     0.265498  \n",
              "65              -0.5     0.904383  \n",
              "178              0.0     0.190304  \n",
              "8               -1.0     1.566761  \n",
              "148             -0.5     0.813860  \n",
              "167             -1.0     0.315373  \n",
              "23               1.0     0.724857  \n",
              "47               1.0     0.217486  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_input_ids, test_attention_mask = tokenize(test['Sentence'].values.tolist(), tokenizer, max_length=100)\n",
        "\n",
        "# Convert inputs to tensors\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_masks = torch.tensor(test_attention_mask)\n",
        "\n",
        "# Create DataLoader for the test data\n",
        "batch_size = 32\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# For the MCRoberta model, we keep it in train mode to enable dropout\n",
        "model.train()\n",
        "\n",
        "predicted_scores = []\n",
        "uncertainties = []\n",
        "\n",
        "n_mc_samples = 30\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask = batch\n",
        "    \n",
        "    mc_samples = []\n",
        "\n",
        "    for _ in range(n_mc_samples):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            \n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        mc_samples.append(logits)\n",
        "        \n",
        "    mc_samples = np.array(mc_samples)\n",
        "    predicted_score = mc_samples.mean(axis=0)\n",
        "    uncertainty = mc_samples.std(axis=0)\n",
        "\n",
        "    predicted_scores.extend(np.argmax(predicted_score, axis=1).flatten())\n",
        "    uncertainties.extend(uncertainty.max(axis=1).flatten())\n",
        "\n",
        "inverse_class_mapping = {v: k for k, v in score_mapping.items()}\n",
        "\n",
        "# Inverse map the classes to their original values\n",
        "predicted_scores = np.vectorize(inverse_class_mapping.get)(predicted_scores)\n",
        "\n",
        "accuracy = np.mean(predicted_scores == test['Score'].values)\n",
        "print(f'Test Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "# Add the original sentences, their predicted scores and uncertainties to a DataFrame\n",
        "predictions_df = pd.DataFrame({'Sentence': test['Sentence'], 'Ground Truth': test['Score'], 'Predicted_Score': predicted_scores, 'Uncertainty': uncertainties})\n",
        "\n",
        "# Print the DataFrame\n",
        "predictions_df[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRRsbihq70u_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix, precision_score, recall_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhDiTWx_fDaW",
        "outputId": "1d2e5481-3f57-4b47-b553-54ebe54848c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['-1.0' '-1.0' '0.5' '-1.0' '0.0' '1.0' '0.0' '0.5' '0.0' '1.0' '0.0'\n",
            " '-1.0' '0.5' '0.0' '0.0' '0.0' '0.5' '-0.5' '0.0' '1.0' '-1.0' '0.5'\n",
            " '-1.0' '-1.0' '-0.5' '-0.5' '1.0' '-1.0' '0.0' '0.0' '0.0' '-1.0' '0.0'\n",
            " '-0.5' '1.0' '0.5' '1.0' '0.5']\n",
            "['-0.5' '-1.0' '0.5' '-0.5' '0.0' '-1.0' '-0.5' '-1.0' '1.0' '1.0' '0.0'\n",
            " '-0.5' '0.5' '0.0' '0.0' '0.0' '0.5' '-0.5' '0.0' '0.5' '-1.0' '-0.5'\n",
            " '-1.0' '1.0' '1.0' '-0.5' '0.5' '-0.5' '-0.5' '0.5' '-0.5' '-1.0' '0.0'\n",
            " '0.5' '-0.5' '0.5' '1.0' '0.5']\n",
            "F1 Score: 55.96%\n",
            "Balanced Accuracy: 51.51%\n",
            "Precision: 65.83%\n",
            "Recall: 52.63%\n"
          ]
        }
      ],
      "source": [
        "# Convert arrays to string type\n",
        "y_true_str = test['Score'].values.astype(float).astype(str)\n",
        "predicted_scores_str = predicted_scores.astype(float).astype(str)\n",
        "\n",
        "print(y_true_str)\n",
        "print(predicted_scores_str)\n",
        "\n",
        "# Compute F1 score and Balanced Accuracy\n",
        "f1 = f1_score(y_true_str, predicted_scores_str, average='weighted')\n",
        "balanced_accuracy = balanced_accuracy_score(y_true_str, predicted_scores_str)\n",
        "\n",
        "# Compute Precision and Recall\n",
        "precision = precision_score(y_true_str, predicted_scores_str, average='weighted')\n",
        "recall = recall_score(y_true_str, predicted_scores_str, average='weighted')\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "confusion_mat = confusion_matrix(y_true_str, predicted_scores_str)\n",
        "\n",
        "# Print the scores\n",
        "print(f'F1 Score: {f1*100:.2f}%')\n",
        "print(f'Balanced Accuracy: {balanced_accuracy*100:.2f}%')\n",
        "print(f'Precision: {precision*100:.2f}%')\n",
        "print(f'Recall: {recall*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLISU6MRFQS2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score\n",
            "0       45\n",
            "-1      33\n",
            "0.5     29\n",
            "1       26\n",
            "-0.5    17\n",
            "Name: count, dtype: int64\n",
            "Score\n",
            "0       12\n",
            "-1       9\n",
            "0.5      7\n",
            "1        6\n",
            "-0.5     4\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# leave out 20% of the data for testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# only keep sentence and score columns\n",
        "\n",
        "\n",
        "labelled_sentences = labelled_sentences[['Sentence', 'Score']]\n",
        "\n",
        "train, test = train_test_split(labelled_sentences, test_size=0.2, random_state=23, stratify=labelled_sentences['Score'])\n",
        "\n",
        "class_counts_train = train['Score'].value_counts()\n",
        "class_counts_test = test['Score'].value_counts()\n",
        "# Print the counts\n",
        "print(class_counts_train)\n",
        "print(class_counts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In addition, the Committee intends to purchase $600 billion of longer-term Treasury securities by the end of the second quarter of 2011, a pace of about $75 billion per month.'"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['Sentence'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "    new_df = pd.DataFrame(columns=['prompt', 'completion'])\n",
        "    new_df['prompt'] = df['Sentence'].apply(lambda x: 'In the following sentence, calculate a sentiment score of -1 to 1 in steps of 0.5. ' + x + ' ->')\n",
        "    score_mapping = {-1: 0, -0.5: 1, 0: 2, 0.5: 3, 1: 4}\n",
        "    # map the scores to labels\n",
        "    new_df['completion'] = df['Score'].map(score_mapping)\n",
        "    # append space to the beginning of the completion\n",
        "    new_df['completion'] = new_df['completion'].apply(lambda x: ' ' + str(x))    \n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_new = preprocess(train)\n",
        "test_new = preprocess(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In the following sentence, calculate a sentiment score of -1 to 1 in steps of 0.5. In addition, the Committee intends to purchase $600 billion of longer-term Treasury securities by the end of the second quarter of 2011, a pace of about $75 billion per month. ->'"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_new['prompt'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' 0'"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_new['completion'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>In the following sentence, calculate a sentime...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                prompt completion\n",
              "189  In the following sentence, calculate a sentime...          4\n",
              "182  In the following sentence, calculate a sentime...          0\n",
              "134  In the following sentence, calculate a sentime...          3\n",
              "49   In the following sentence, calculate a sentime...          0\n",
              "24   In the following sentence, calculate a sentime...          2\n",
              "..                                                 ...        ...\n",
              "30   In the following sentence, calculate a sentime...          0\n",
              "20   In the following sentence, calculate a sentime...          4\n",
              "17   In the following sentence, calculate a sentime...          0\n",
              "153  In the following sentence, calculate a sentime...          3\n",
              "56   In the following sentence, calculate a sentime...          1\n",
              "\n",
              "[150 rows x 2 columns]"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In addition, the Committee intends to purchase $600 billion of longer-term Treasury securities by the end of the second quarter of 2011, a pace of about $75 billion per month.'"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['Sentence'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_gpt, test_gpt = train_test_split(train_new, test_size=0.2, random_state=23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save train and test data to jsonl files\n",
        "train_gpt.to_json('train_gpt.jsonl', orient='records', lines=True)\n",
        "test_gpt.to_json('test_gpt.jsonl', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 120 prompt-completion pairs\n",
            "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
            "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
            "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
            "- There are 10 duplicated prompt-completion sets. These are rows: [14, 44, 52, 78, 82, 91, 96, 111, 115, 117]\n",
            "- All prompts end with suffix ` ->`\n",
            "- All prompts start with prefix `In the following sentence, calculate a sentiment score of -1 to 1 in steps of 0.5. `. Fine-tuning doesn't require the instruction specifying the task, or a few-shot example scenario. Most of the time you should only add the input data into the prompt, and the desired output into the completion\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Remove 10 duplicate rows [Y/n]: Y\n",
            "- [Recommended] Remove prefix `In the following sentence, calculate a sentiment score of -1 to 1 in steps of 0.5. ` from all prompts [Y/n]: Y\n",
            "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified files to `train_gpt_prepared_train.jsonl` and `train_gpt_prepared_valid.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"train_gpt_prepared_train.jsonl\" -v \"train_gpt_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 5\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt.\n",
            "Once your model starts training, it'll approximately take 4.97 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Owner\\anaconda3\\envs\\nlp_project_3.8\\lib\\site-packages\\openai\\validators.py:283: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x[\"prompt\"] = x[\"prompt\"].str[len(prefix) :]\n"
          ]
        }
      ],
      "source": [
        "# !openai tools fine_tunes.prepare_data -f train_gpt.jsonl -q\n",
        "# # run this cell if you need the openai to prepare the data for you, the output will be in 2 files, train and validation\n",
        "# # example of the output and below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file from train_gpt.jsonl: file-IsuyJVVAXaEZj8igPycsep8D\n",
            "Uploaded file from test_gpt.jsonl: file-P6Ml8dNMM0yZyXxoxeTRDZfc\n",
            "Created fine-tune: ft-4Vl7xEmUju2pDjpV3veybmlk\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-06-02 01:16:29] Created fine-tune: ft-4Vl7xEmUju2pDjpV3veybmlk\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-4Vl7xEmUju2pDjpV3veybmlk\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Upload progress:   0%|          | 0.00/37.4k [00:00<?, ?it/s]\n",
            "Upload progress: 100%|██████████| 37.4k/37.4k [00:00<00:00, 37.4Mit/s]\n",
            "\n",
            "Upload progress:   0%|          | 0.00/10.6k [00:00<?, ?it/s]\n",
            "Upload progress: 100%|██████████| 10.6k/10.6k [00:00<00:00, 10.7Mit/s]\n"
          ]
        }
      ],
      "source": [
        "# create fine tunes model\n",
        "!openai api fine_tunes.create -t \"train_gpt.jsonl\" -v \"test_gpt.jsonl\" --compute_classification_metrics --classification_n_classes 5 -m ada --n_epochs 8 --suffix \"nlp_prod_ada\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-06-02 01:16:29] Created fine-tune: ft-4Vl7xEmUju2pDjpV3veybmlk\n",
            "[2023-06-02 01:17:33] Fine-tune costs $0.02\n",
            "[2023-06-02 01:17:33] Fine-tune enqueued. Queue number: 0\n",
            "[2023-06-02 01:17:37] Fine-tune started\n",
            "[2023-06-02 01:18:11] Completed epoch 1/8\n",
            "[2023-06-02 01:18:32] Completed epoch 2/8\n",
            "[2023-06-02 01:18:51] Completed epoch 3/8\n",
            "[2023-06-02 01:19:10] Completed epoch 4/8\n",
            "[2023-06-02 01:19:29] Completed epoch 5/8\n",
            "[2023-06-02 01:19:48] Completed epoch 6/8\n",
            "[2023-06-02 01:20:08] Completed epoch 7/8\n",
            "[2023-06-02 01:20:27] Completed epoch 8/8\n",
            "[2023-06-02 01:20:51] Uploaded model: ada:ft-personal:nlp-prod-ada-2023-06-01-17-20-50\n",
            "[2023-06-02 01:20:52] Uploaded result file: file-ovSNoj25FWbLt3lwXFLMkQ2W\n",
            "[2023-06-02 01:20:52] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal:nlp-prod-ada-2023-06-01-17-20-50 -p <YOUR_PROMPT>\n"
          ]
        }
      ],
      "source": [
        "!openai api fine_tunes.follow -i ft-4Vl7xEmUju2pDjpV3veybmlk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save results to csv\n",
        "!openai api fine_tunes.results -i ft-4Vl7xEmUju2pDjpV3veybmlk > result.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>step</th>\n",
              "      <th>elapsed_tokens</th>\n",
              "      <th>elapsed_examples</th>\n",
              "      <th>training_loss</th>\n",
              "      <th>training_sequence_accuracy</th>\n",
              "      <th>training_token_accuracy</th>\n",
              "      <th>validation_loss</th>\n",
              "      <th>validation_sequence_accuracy</th>\n",
              "      <th>validation_token_accuracy</th>\n",
              "      <th>classification/accuracy</th>\n",
              "      <th>classification/weighted_f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>122</td>\n",
              "      <td>7650</td>\n",
              "      <td>122</td>\n",
              "      <td>0.041212</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.108333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>241</td>\n",
              "      <td>15281</td>\n",
              "      <td>241</td>\n",
              "      <td>0.026447</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.042067</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.072727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>365</td>\n",
              "      <td>22877</td>\n",
              "      <td>365</td>\n",
              "      <td>0.011655</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.430590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>483</td>\n",
              "      <td>30443</td>\n",
              "      <td>483</td>\n",
              "      <td>0.016794</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.350256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>602</th>\n",
              "      <td>603</td>\n",
              "      <td>38003</td>\n",
              "      <td>603</td>\n",
              "      <td>0.024068</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.456818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>726</th>\n",
              "      <td>727</td>\n",
              "      <td>45631</td>\n",
              "      <td>727</td>\n",
              "      <td>0.016764</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.433056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>848</td>\n",
              "      <td>53176</td>\n",
              "      <td>848</td>\n",
              "      <td>0.002265</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.432586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>961</th>\n",
              "      <td>962</td>\n",
              "      <td>60330</td>\n",
              "      <td>962</td>\n",
              "      <td>0.005568</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.432586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     step  elapsed_tokens  elapsed_examples  training_loss   \n",
              "121   122            7650               122       0.041212  \\\n",
              "240   241           15281               241       0.026447   \n",
              "364   365           22877               365       0.011655   \n",
              "482   483           30443               483       0.016794   \n",
              "602   603           38003               603       0.024068   \n",
              "726   727           45631               727       0.016764   \n",
              "847   848           53176               848       0.002265   \n",
              "961   962           60330               962       0.005568   \n",
              "\n",
              "     training_sequence_accuracy  training_token_accuracy  validation_loss   \n",
              "121                         0.0                      0.0              NaN  \\\n",
              "240                         0.0                      0.0         0.042067   \n",
              "364                         1.0                      1.0              NaN   \n",
              "482                         1.0                      1.0              NaN   \n",
              "602                         1.0                      1.0              NaN   \n",
              "726                         1.0                      1.0              NaN   \n",
              "847                         1.0                      1.0              NaN   \n",
              "961                         1.0                      1.0              NaN   \n",
              "\n",
              "     validation_sequence_accuracy  validation_token_accuracy   \n",
              "121                           NaN                        NaN  \\\n",
              "240                           0.0                        0.0   \n",
              "364                           NaN                        NaN   \n",
              "482                           NaN                        NaN   \n",
              "602                           NaN                        NaN   \n",
              "726                           NaN                        NaN   \n",
              "847                           NaN                        NaN   \n",
              "961                           NaN                        NaN   \n",
              "\n",
              "     classification/accuracy  classification/weighted_f1_score  \n",
              "121                 0.233333                          0.108333  \n",
              "240                 0.200000                          0.072727  \n",
              "364                 0.500000                          0.430590  \n",
              "482                 0.400000                          0.350256  \n",
              "602                 0.466667                          0.456818  \n",
              "726                 0.466667                          0.433056  \n",
              "847                 0.466667                          0.432586  \n",
              "961                 0.466667                          0.432586  "
            ]
          },
          "execution_count": 180,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = pd.read_csv('result.csv')\n",
        "results[results['classification/accuracy'].notnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "results[results['classification/accuracy'].notnull()]['classification/accuracy'].plot()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
